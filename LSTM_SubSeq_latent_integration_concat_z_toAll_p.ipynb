{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:44.080201Z",
     "start_time": "2020-08-21T21:14:42.739357Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "# --- Must haves ---\n",
    "import os, sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from surrogates4sims.datasets import MantaFlowDataset, getSingleSim, createMantaFlowTrainTest\n",
    "\n",
    "from surrogates4sims.utils import create_opt, create_one_cycle, find_lr, printNumModelParams, \\\n",
    "                                    rmse, writeMessage, plotSampleWprediction, plotSampleWpredictionByChannel, \\\n",
    "                                    plotSample, curl, jacobian, stream2uv, create_movie, convertSimToImage\n",
    "\n",
    "#from surrogates4sims.models import Generator, Encoder, AE_no_P, AE_xhat_z, AE_xhat_zV2\n",
    "\n",
    "from surrogates4sims.train import trainEpoch, validEpoch\n",
    "\n",
    "from surrogates4sims.svd import MantaFlowSVDDataset\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:44.091693Z",
     "start_time": "2020-08-21T21:14:44.082602Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "# model name, for tensorboard recording and checkpointing purposes.\n",
    "versionName = \"LIN_SVD_only_z_and_p_LSTM_bidirec\"\n",
    "\n",
    "# GPU Numbers to use. Comma seprate them for multi-GPUs.\n",
    "gpu_ids = \"2\"#,1,2,3\"\n",
    "versionName = versionName + '_GPUs{}'.format(gpu_ids.replace(',',''))\n",
    "# path to load model weights.\n",
    "pretrained_path = None\n",
    "\n",
    "# rate at which to record metrics. (number of batches to average over when recording metrics, e.g. \"every 5 batches\")\n",
    "tensorboard_rate = 5\n",
    "\n",
    "# number of epochs to train. This is defined here so we can use the OneCycle LR Scheduler.\n",
    "epochs = 1000\n",
    "\n",
    "# Data Directory\n",
    "dataDirec = '/data/mantaFlowSim/data/smoke_pos21_size5_f200/v'\n",
    "reverseXY = False \n",
    "\n",
    "# checkpoint directory\n",
    "cps = 'cps'\n",
    "tensorboard_direc = \"tb\"\n",
    "\n",
    "findLRs = True  \n",
    "patience = 10\n",
    "\n",
    "# hyper-params\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "testSplit = .1\n",
    "bz = 4\n",
    "numSamplesToKeep = np.infty #if not debugging\n",
    "latentDim = 512\n",
    "simLen = 200\n",
    "simVizIndex = 0 # sim in the test set to visualize\n",
    "numComponents = latentDim\n",
    "transform = True\n",
    "if DEBUG:\n",
    "    epochs = 5000\n",
    "    numSamplesToKeep = 1000\n",
    "    \n",
    "versionName = versionName + '_latentDim{}_bz{}_transform{}_epochs{}'.format(latentDim,bz,transform,epochs)\n",
    "versionName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Select Personal GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:44.273420Z",
     "start_time": "2020-08-21T21:14:44.093455Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:44.281138Z",
     "start_time": "2020-08-21T21:14:44.277498Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=gpu_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:44.313533Z",
     "start_time": "2020-08-21T21:14:44.283233Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:44.320383Z",
     "start_time": "2020-08-21T21:14:44.314871Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if device.type == 'cuda':\n",
    "    print(cuda.is_available())\n",
    "    print(cuda.device_count())\n",
    "    print(cuda.current_device())\n",
    "    print(cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:46.749563Z",
     "start_time": "2020-08-21T21:14:44.321732Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.zeros(5, device=device.type)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Datasets & Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:46.756155Z",
     "start_time": "2020-08-21T21:14:46.752263Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# A = np.load('svd_out.npz')\n",
    "# vh = A['arr_2']\n",
    "# vh.shape\n",
    "# data = []\n",
    "# numComp = 512\n",
    "# for idx in range(105):\n",
    "#     D = getSingleSim(idx)\n",
    "#     simDataset = MantaFlowDataset(D)\n",
    "#     Z = []\n",
    "#     P = []\n",
    "#     X0, p = simDataset[0]\n",
    "#     for sample in simDataset:\n",
    "#         f, p = sample\n",
    "#         coeffs = vh[:numComp]@f.flatten()\n",
    "#         Z.append(coeffs)\n",
    "#         P.append(p)\n",
    "#     y = np.array(Z)\n",
    "#     P = np.array(P)\n",
    "#     X = (X0,P,y[0])\n",
    "#     data.append((X,y[1:]))\n",
    "# with open('simLatentVectors.pkl','wb') as fid:\n",
    "#     pickle.dump(data,fid)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:46.800565Z",
     "start_time": "2020-08-21T21:14:46.759128Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('simLatentVectors.pkl','rb') as fid:\n",
    "    data = pickle.load(fid)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:46.806113Z",
     "start_time": "2020-08-21T21:14:46.802139Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data[0][0][0].shape, data[0][0][1].shape, data[0][0][2].shape, data[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:46.892998Z",
     "start_time": "2020-08-21T21:14:46.807535Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainData, testData = createMantaFlowTrainTest(dataDirec,simLen,testSplit,seed)\n",
    "print((len(trainData),len(testData)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:46.949340Z",
     "start_time": "2020-08-21T21:14:46.894421Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "d = glob(os.path.join(dataDirec,'*.npz'))\n",
    "d = sorted(d)\n",
    "simLen = 200\n",
    "numSims = len(d)//simLen\n",
    "numTestSamples = int(np.round(testSplit*numSims))\n",
    "np.random.seed(seed)\n",
    "perm = np.random.permutation(numSims)\n",
    "testSims = perm[:numTestSamples]\n",
    "trainSims = perm[numTestSamples:]\n",
    "testSims, trainSims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:46.954503Z",
     "start_time": "2020-08-21T21:14:46.950758Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "testDataset = [data[i] for i in testSims]\n",
    "trainDataset = [data[i] for i in trainSims]\n",
    "len(testDataset), len(trainDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:47.012557Z",
     "start_time": "2020-08-21T21:14:46.956242Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = []\n",
    "E = []\n",
    "for X,y in trainDataset:\n",
    "    z = X[2]\n",
    "    z = z.reshape(1,len(z))\n",
    "    y = np.concatenate([z,y])\n",
    "    D.append(y.max(axis=0))\n",
    "    E.append(y.min(axis=0))\n",
    "D = np.array(D)\n",
    "E = np.array(E)\n",
    "ymx = D.max(axis=0)\n",
    "ymn = E.min(axis=0)\n",
    "ymx, ymn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:47.016713Z",
     "start_time": "2020-08-21T21:14:47.013990Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def transformY(y,ymx,ymn):\n",
    "    return (ymx - y)/(ymx - ymn)\n",
    "\n",
    "def inverseTransformY(y,ymx,ymn):\n",
    "    x = ymx - y*(ymx - ymn)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:47.024405Z",
     "start_time": "2020-08-21T21:14:47.017907Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = transformY(y,ymx,ymn)\n",
    "a.max(), a.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:47.030596Z",
     "start_time": "2020-08-21T21:14:47.025940Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "b = inverseTransformY(a,ymx,ymn)\n",
    "np.abs(y - b).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T20:57:15.847625Z",
     "start_time": "2020-08-21T20:57:15.841529Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LatentSVD(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "                 \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx, return_norm_stats=False):\n",
    "        X, y  = self.data[idx]\n",
    "        p = X[1]\n",
    "        z = X[2]\n",
    "\n",
    "        if self.transform:\n",
    "            y = torch.tensor(y)\n",
    "            norm_stats = y.norm(dim=1).unsqueeze(1)\n",
    "            y = y / norm_stats\n",
    "            \n",
    "        D = []\n",
    "        for pp in p[1:]:\n",
    "            zz = np.concatenate([pp,z])#.reshape(1,515)\n",
    "            D.append(zz)\n",
    "        X = np.array(D)\n",
    "        \n",
    "        if return_norm_stats:\n",
    "            return X, y, norm_stats\n",
    "        else:\n",
    "            return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T20:57:15.851949Z",
     "start_time": "2020-08-21T20:57:15.848860Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if transform:\n",
    "    testDataset = LatentSVD(testDataset, transform=transformY)\n",
    "    trainDataset = LatentSVD(trainDataset, transform=transformY)\n",
    "else:\n",
    "    testDataset = LatentSVD(testDataset)\n",
    "    trainDataset = LatentSVD(trainDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:47.039505Z",
     "start_time": "2020-08-21T21:14:47.032477Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class LatentSVDSub(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "                 \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx, return_norm_stats=False):\n",
    "        X, y  = self.data[idx]\n",
    "        p = X[1]\n",
    "        z = X[2]\n",
    "\n",
    "        if self.transform:\n",
    "            y = torch.tensor(y)\n",
    "            norm_stats = y.norm(dim=1).unsqueeze(1)\n",
    "            y = y / norm_stats\n",
    "            \n",
    "        D = []\n",
    "        for pp in p[1:]:\n",
    "            zz = np.concatenate([pp,z])#.reshape(1,515)\n",
    "            D.append(zz)\n",
    "        X = np.array(D)\n",
    "        \n",
    "        rand_i = random.randint(1, 189)\n",
    "        X = X[rand_i:rand_i+10]\n",
    "        X[:, 3:] = y[rand_i-1]\n",
    "        y = y[rand_i:rand_i+10]\n",
    "        \n",
    "        if return_norm_stats:\n",
    "            norm_stats = norm_stats[rand_i:rand_i+10]\n",
    "            return X, y, norm_stats\n",
    "        else:\n",
    "            return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:47.153048Z",
     "start_time": "2020-08-21T21:14:47.147515Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if transform:\n",
    "    testDataset = LatentSVDSub(testDataset, transform=transformY)\n",
    "    trainDataset = LatentSVDSub(trainDataset, transform=transformY)\n",
    "else:\n",
    "    testDataset = LatentSVDSub(testDataset)\n",
    "    trainDataset = LatentSVDSub(trainDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Making sure the transform works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:48.183016Z",
     "start_time": "2020-08-21T21:14:48.177799Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(dataset=trainDataset, batch_size=1, shuffle=True, drop_last=True)\n",
    "testDataLoader = DataLoader(dataset=testDataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:48.238086Z",
     "start_time": "2020-08-21T21:14:48.212103Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X,y = next(iter(trainDataLoader))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:48.469475Z",
     "start_time": "2020-08-21T21:14:48.239886Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y[0].squeeze().T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:48.475854Z",
     "start_time": "2020-08-21T21:14:48.471160Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y.max(), y.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:48.479833Z",
     "start_time": "2020-08-21T21:14:48.477152Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:48.488743Z",
     "start_time": "2020-08-21T21:14:48.480882Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X[:,:,3:].max(),X[:,:,3:].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:48.492544Z",
     "start_time": "2020-08-21T21:14:48.490107Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(dataset=trainDataset, batch_size=bz, shuffle=True, drop_last=True)\n",
    "testDataLoader = DataLoader(dataset=testDataset, batch_size=bz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:48.529095Z",
     "start_time": "2020-08-21T21:14:48.523964Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(trainDataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:49.205354Z",
     "start_time": "2020-08-21T21:14:49.200187Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inputSize = X.shape[2]\n",
    "hiddenSize = y.shape[2]\n",
    "numLayers = 1\n",
    "bidirectional = False\n",
    "batch_first = True\n",
    "#model = nn.LSTM(inputSize, hiddenSize, numLayers, batch_first=True, bidirectional=bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:49.244358Z",
     "start_time": "2020-08-21T21:14:49.233874Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,inputSize=3, hiddenSize=512, numLayers=1, batch_first=True, bidirectional=False):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm1 = nn.LSTM(3, hiddenSize, numLayers, batch_first=batch_first, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self,x0):\n",
    "        h1_0 = x0[:, 0, 3:].unsqueeze(0).clone()\n",
    "        c1_0 = torch.zeros_like(h1_0)\n",
    "        x1,_ = self.lstm1(x0[:, :100, :3], (h1_0, c1_0))\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:49.451310Z",
     "start_time": "2020-08-21T21:14:49.246022Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = LSTM(inputSize, hiddenSize, numLayers, batch_first, bidirectional).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:49.455707Z",
     "start_time": "2020-08-21T21:14:49.453084Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "printNumModelParams(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:49.463430Z",
     "start_time": "2020-08-21T21:14:49.457048Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output = model(X.to(device))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:49.467333Z",
     "start_time": "2020-08-21T21:14:49.464852Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if len(gpu_ids.split(',')) > 1:\n",
    "    model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Orig Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:50.106143Z",
     "start_time": "2020-08-21T21:14:50.100299Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:50.129965Z",
     "start_time": "2020-08-21T21:14:50.127098Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "L = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:50.141035Z",
     "start_time": "2020-08-21T21:14:50.132744Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    loss = L(output, y.to(device))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Set LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:51.147492Z",
     "start_time": "2020-08-21T21:14:51.144256Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# if findLRs and (len(gpu_ids.split(','))==1): # doesn't work for multigpu???\n",
    "#     opt = create_opt(1e-7,model)\n",
    "#     find_lr(model,opt,L,device,trainDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:51.178683Z",
     "start_time": "2020-08-21T21:14:51.172154Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_lr = .001\n",
    "versionName = versionName + '_lr{}'.format(str(max_lr))\n",
    "\n",
    "versionName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:51.678307Z",
     "start_time": "2020-08-21T21:14:51.664481Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def trainEpoch(myDataLoader, tensorboard_writer, model, opt, loss,\n",
    "               metric, lr_scheduler, tensorboard_rate, device,\n",
    "               tensorboard_recorder_step, total_steps):\n",
    "    running_loss = 0.0\n",
    "    running_rmse = 0.0\n",
    "    total_loss = 0.0\n",
    "    for i, sampleBatch in enumerate(myDataLoader, start=1):\n",
    "\n",
    "        # --- Main Training ---\n",
    "        \n",
    "        # gpu\n",
    "        X,y = sampleBatch[0],sampleBatch[1]\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        y_hat = model(X)\n",
    "        combined_loss = loss(y_hat,y)\n",
    "        combined_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # loss\n",
    "        batch_loss = combined_loss.item()\n",
    "        running_loss += batch_loss\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        # --- Metrics Recording ---\n",
    "\n",
    "        # metrics\n",
    "        r = metric(y_hat, y)\n",
    "        running_rmse += r\n",
    "\n",
    "        # record lr change\n",
    "        total_steps += 1\n",
    "        tensorboard_writer.add_scalar(tag=\"LR\", scalar_value=opt.param_groups[0]['lr'], global_step=total_steps)\n",
    "\n",
    "        # tensorboard writes\n",
    "        if (i % tensorboard_rate == 0):\n",
    "            tensorboard_recorder_step += 1\n",
    "            avg_running_loss = running_loss/tensorboard_rate\n",
    "            avg_running_rmse = running_rmse/tensorboard_rate\n",
    "            tensorboard_writer.add_scalar(tag=\"Loss\", scalar_value=avg_running_loss, global_step=tensorboard_recorder_step)\n",
    "            tensorboard_writer.add_scalar(tag=metric.__name__, scalar_value=avg_running_rmse, global_step=tensorboard_recorder_step)\n",
    "            # reset running_loss for the next set of batches. (tensorboard_rate number of batches)\n",
    "            running_loss = 0.0\n",
    "            running_rmse = 0.0\n",
    "\n",
    "    return total_loss/len(myDataLoader), tensorboard_recorder_step, total_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:51.707122Z",
     "start_time": "2020-08-21T21:14:51.698546Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def validEpoch(myDataLoader, tensorboard_writer, model, loss, metric,\n",
    "               device, tensorboard_recorder_step):\n",
    "    running_loss = 0.0\n",
    "    running_rmse = 0.0\n",
    "    for i, sampleBatch in enumerate(myDataLoader, start=1):\n",
    "\n",
    "        # --- Metrics Recording ---\n",
    "\n",
    "        # gpu\n",
    "        X,y = sampleBatch[0],sampleBatch[1]\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        #perc = len(X)/len(myDataLoader.dataset)\n",
    "        perc = 1./len(myDataLoader.dataset)\n",
    "        # forward, no gradient calculations\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(X)\n",
    "\n",
    "        # loss\n",
    "        combined_loss = loss(y_hat,y)\n",
    "        \n",
    "        running_loss += perc*(combined_loss.item())\n",
    "\n",
    "        # metrics\n",
    "        r = metric(y_hat, y)\n",
    "        running_rmse += perc*r\n",
    "\n",
    "    avg_running_loss = running_loss\n",
    "    avg_running_rmse = running_rmse\n",
    "    tensorboard_writer.add_scalar(tag=\"Loss\", scalar_value=avg_running_loss, global_step=tensorboard_recorder_step)\n",
    "    tensorboard_writer.add_scalar(tag=metric.__name__, scalar_value=avg_running_rmse, global_step=tensorboard_recorder_step)\n",
    "\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:51.711976Z",
     "start_time": "2020-08-21T21:14:51.708858Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(cps)\n",
    "except:\n",
    "    print(\"checkpoints directory already exists :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:51.786669Z",
     "start_time": "2020-08-21T21:14:51.713796Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create a summary writer.\n",
    "train_writer = SummaryWriter(os.path.join(tensorboard_direc, versionName,'train'))\n",
    "test_writer = SummaryWriter(os.path.join(tensorboard_direc, versionName,'valid'))\n",
    "tensorboard_recorder_step = 0\n",
    "total_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:14:51.802367Z",
     "start_time": "2020-08-21T21:14:51.789583Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fit_up_to_trim_idx():\n",
    "    \n",
    "    global tensorboard_recorder_step\n",
    "    global total_steps\n",
    "    \n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,patience=patience)\n",
    "\n",
    "    writeMessage('---------- Started Training ----------', versionName)\n",
    "    bestLoss = np.infty\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs+1)):  # loop over the dataset multiple times\n",
    "\n",
    "        writeMessage(\"--- Epoch {0}/{1} ---\".format(epoch, epochs), versionName)\n",
    "\n",
    "        model.train()\n",
    "        trainLoss, tensorboard_recorder_step, total_steps = trainEpoch(trainDataLoader, \n",
    "                                                                       train_writer, model, opt, L,\n",
    "                                                                       rmse, lr_scheduler, \n",
    "                                                                       tensorboard_rate, device,\n",
    "                                                                       tensorboard_recorder_step, total_steps)\n",
    "\n",
    "        writeMessage(\"trainLoss: {:.4e}\".format(trainLoss),versionName)\n",
    "        writeMessage(\"LR: {:.4e}\".format(opt.param_groups[0]['lr']),versionName)\n",
    "    #     if trainLoss < bestLoss:\n",
    "    #         bestLoss = trainLoss\n",
    "    #         writeMessage(\"Better trainLoss: {:.4e}, Saving models...\".format(bestLoss),versionName)\n",
    "    #         torch.save(model.state_dict(), os.path.join(cps,versionName))\n",
    "\n",
    "        model.eval()\n",
    "        valLoss = validEpoch(testDataLoader, test_writer, model, L, rmse, device, tensorboard_recorder_step)\n",
    "        writeMessage(\"valLoss: {:.4e}\".format(valLoss),versionName)\n",
    "\n",
    "        #checkpoint progress\n",
    "        if valLoss < bestLoss:\n",
    "            bestLoss = valLoss\n",
    "            writeMessage(\"Better valLoss: {:.4e}, Saving models...\".format(bestLoss),versionName)\n",
    "            torch.save(model.state_dict(), os.path.join(cps,versionName))\n",
    "\n",
    "        #lr_scheduler.step(trainLoss)\n",
    "\n",
    "        if opt.param_groups[0]['lr'] < 5e-8:\n",
    "            break\n",
    "    writeMessage('---------- Finished Training ----------', versionName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:15:53.910277Z",
     "start_time": "2020-08-21T21:14:51.803953Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_up_to_trim_idx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Compare: Generated vs. Simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:15:57.580322Z",
     "start_time": "2020-08-21T21:15:57.563569Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(cps,versionName)))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:15:57.770839Z",
     "start_time": "2020-08-21T21:15:57.765280Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:16:05.883226Z",
     "start_time": "2020-08-21T21:16:05.871410Z"
    },
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "X, y, norm_stats = testDataset.__getitem__(0, True)\n",
    "X = torch.tensor(X).unsqueeze(0)\n",
    "y = y.to(device)\n",
    "norm_stats = norm_stats.to(device)\n",
    "X.shape, y.shape, norm_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:16:25.893735Z",
     "start_time": "2020-08-21T21:16:25.880169Z"
    },
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "X, y = testDataset[0]\n",
    "X = torch.tensor(X).unsqueeze(0)\n",
    "y = torch.tensor(y).to(device)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:16:26.121046Z",
     "start_time": "2020-08-21T21:16:26.115373Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_out = model(X.to(device))\n",
    "batch_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:16:26.447818Z",
     "start_time": "2020-08-21T21:16:26.429039Z"
    },
    "hidden": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_out = batch_out.squeeze()\n",
    "y = y.squeeze()\n",
    "err = []\n",
    "for i in range(y.shape[0]):\n",
    "    err.append(torch.norm(y[i] - batch_out[i]))\n",
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:16:27.002070Z",
     "start_time": "2020-08-21T21:16:26.981131Z"
    },
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "batch_out = batch_out.squeeze()*norm_stats\n",
    "y = y.squeeze()*norm_stats\n",
    "err = []\n",
    "for i in range(y.shape[0]):\n",
    "    err.append(torch.norm(y[i] - batch_out[i]))\n",
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:16:27.737884Z",
     "start_time": "2020-08-21T21:16:27.731278Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d = batch_out - y\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:16:28.063832Z",
     "start_time": "2020-08-21T21:16:27.938420Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T20:04:16.009744Z",
     "start_time": "2020-08-21T20:04:14.887970Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = np.arange(512)\n",
    "for dd,yy in zip(batch_out,y):\n",
    "    plt.plot(t, dd.detach().cpu().numpy(),t, yy.detach().cpu().numpy(),'--')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T20:04:17.010364Z",
     "start_time": "2020-08-21T20:04:17.005409Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = y.detach().cpu().numpy()\n",
    "tmp.max(), tmp.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T20:04:18.589303Z",
     "start_time": "2020-08-21T20:04:18.584116Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bb = batch_out.detach().cpu().numpy()\n",
    "bb.max(), bb.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.889Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(tmp.max(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.890Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(d[-2].detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.892Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X = y.detach().cpu().numpy()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.893Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2,verbose=3).fit_transform(X)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.894Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_embedded[:,0],X_embedded[:,1],s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.895Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = batch_out.detach().cpu().numpy()\n",
    "X_embedded = TSNE(n_components=2,verbose=3).fit_transform(X)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.896Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_embedded[:,0],X_embedded[:,1],s=2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
