{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:03.871499Z",
     "start_time": "2020-10-30T03:01:01.725856Z"
    }
   },
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "# --- Must haves ---\n",
    "import os, sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "\n",
    "from surrogates4sims.mantaflowDatasets import MantaFlowDataset, getSingleSim, createMantaFlowTrainTest\n",
    "\"\"\n",
    "from surrogates4sims.utils import create_opt, create_one_cycle, find_lr, printNumModelParams, reconFrame, \\\n",
    "                                    rmse, writeMessage, plotSampleWprediction, plotSampleWpredictionByChannel, \\\n",
    "                                    plotSample, pkl_load, curl, jacobian, stream2uv, create_movie, convertSimToImage\n",
    "\n",
    "from surrogates4sims.models import Generator, Encoder, AE_no_P, AE_xhat_z, AE_xhat_zV2\n",
    "\n",
    "from surrogates4sims.train import trainEpoch, validEpoch\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T16:10:43.083472Z",
     "start_time": "2020-10-30T16:10:43.071675Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'end_to_end_plateau_train_GPUs13_latentDim4003_window_size10_bz20_SVDTrue_streamFalse_jacobianTrue_epochs1000_stackTrue'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data \n",
    "eval_only=False\n",
    "DEBUG = False\n",
    "# model name, for tensorboard recording and checkpointing purposes.\n",
    "versionName = \"end_to_end_plateau_train\"\n",
    "\n",
    "# GPU Numbers to use. Comma seprate them for multi-GPUs.\n",
    "gpu_ids = \"1,3\"\n",
    "versionName = versionName + '_GPUs{}'.format(gpu_ids.replace(',',''))\n",
    "# path to load model weights.\n",
    "pretrained_path = None\n",
    "\n",
    "# rate at which to record metrics. (number of batches to average over when recording metrics, e.g. \"every 5 batches\")\n",
    "tensorboard_rate = 5\n",
    "\n",
    "# number of epochs to train. This is defined here so we can use the OneCycle LR Scheduler.\n",
    "epochs = 1000\n",
    "\n",
    "# Data Directory\n",
    "dataDirec = '/data/mantaFlowSim/data/smoke_pos21_size5_f200/v'\n",
    "reverseXY = False \n",
    "\n",
    "# checkpoint directory\n",
    "cps = 'cps'\n",
    "tensorboard_direc = \"tb\"\n",
    "\n",
    "findLRs = False  \n",
    "\n",
    "# hyper-params\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "testSplit = .1\n",
    "bz = 20\n",
    "numSamplesToKeep = np.infty #if not debugging\n",
    "latentDim = 4003\n",
    "window_size = 10 # to-do: have an increasing window size as training proceeds?\n",
    "filters = 128\n",
    "num_conv = 4 # breaks when less than 2\n",
    "simLen = 200\n",
    "stack = True\n",
    "simVizIndex = 0 # sim in the test set to visualize\n",
    "createStreamFcn = False\n",
    "doJacobian = True\n",
    "repeat = 0\n",
    "skip_connection = False\n",
    "patience = 2\n",
    "if DEBUG:\n",
    "    epochs = 10000\n",
    "    numSamplesToKeep = bz\n",
    "    \n",
    "    \n",
    "versionName = versionName + '_latentDim{}_window_size{}_bz{}_SVD{}_stream{}_jacobian{}_epochs{}_stack{}'.format(latentDim,window_size,bz,True,createStreamFcn,doJacobian,epochs,stack)\n",
    "versionName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:04.113471Z",
     "start_time": "2020-10-30T03:01:03.894634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "(19000, 2000)\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=gpu_ids\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "trainData, testData = createMantaFlowTrainTest(dataDirec,simLen,testSplit,seed)\n",
    "print((len(trainData),len(testData)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:04.132947Z",
     "start_time": "2020-10-30T03:01:04.115255Z"
    }
   },
   "outputs": [],
   "source": [
    "class MantaFlowDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 dataDirec='/data/mantaFlowSim/data/smoke_pos21_size5_f200/v',\n",
    "                 numToKeep=np.infty,transform=None, reverseXY=False, preprocess=True, AE=False,\n",
    "                 w = 1, simLen = 200): \n",
    "        if type(dataDirec) == list:\n",
    "            self.files = dataDirec\n",
    "        else:\n",
    "            self.files = glob(os.path.join(dataDirec,'*.npz'))\n",
    "        self.dataDirec = dataDirec\n",
    "        self.numToKeep = numToKeep\n",
    "        self.transform = transform\n",
    "        self.reverseXY = reverseXY\n",
    "        self.AE = AE\n",
    "        self.w = w\n",
    "        self.simLen = simLen\n",
    "        self.data = []\n",
    " \n",
    "        if numToKeep < len(self.files):\n",
    "            self.files = self.files[:numToKeep]\n",
    "        for f in tqdm(self.files):\n",
    "            X,y = self.loadfile(f)\n",
    "            \n",
    "            if preprocess:\n",
    "                X,y = self.preprocessFcn(X,y)\n",
    "                \n",
    "            if reverseXY:\n",
    "                self.data.append((y,X))\n",
    "            else:\n",
    "                self.data.append((X,y))\n",
    "\n",
    "    def loadfile(self,fn):\n",
    "        A = np.load(fn)\n",
    "        X = A['x'].astype('float32')\n",
    "        X = np.rollaxis(X,-1)\n",
    "        y = A['y'].astype('float32')\n",
    "        return X,y\n",
    "\n",
    "    def preprocessFcn(self,X,y):\n",
    "        x_range = 11.953\n",
    "        X /= x_range\n",
    "        y_range = [[0.2, 0.8], [0.04, 0.12], [0.0, 199.0]]\n",
    "        for i, ri in enumerate(y_range):\n",
    "            y[i] = (y[i]-ri[0]) / (ri[1]-ri[0]) * 2 - 1\n",
    "        return X,y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def plot(self,idx,savefig=False):\n",
    "        X, label  = self.data[idx]\n",
    "        if self.reverseXY:\n",
    "            X = label\n",
    "            \n",
    "        plt.figure(figsize=(20,10))\n",
    "        \n",
    "        plt.subplot(211)\n",
    "        fn = self.files[idx].replace('.npz','')\n",
    "        title = '{} channel 0'.format(fn)\n",
    "        plt.title(title)\n",
    "        plt.imshow(X[0][::-1])\n",
    "        plt.colorbar()\n",
    "        \n",
    "        plt.subplot(212)\n",
    "        title = '{} channel 1'.format(fn)\n",
    "        plt.title(title)\n",
    "        plt.imshow(X[1][::-1])\n",
    "        plt.colorbar()\n",
    "        \n",
    "        if savefig:\n",
    "            title = title.replace(' ','_') + '.png'\n",
    "            plt.savefig(title, dpi=300)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        q = idx // self.simLen\n",
    "        r_idx = np.random.randint(0,self.simLen-self.w)\n",
    "        x = self.data[q*simLen + r_idx : q*simLen + r_idx + 1]\n",
    "        y = self.data[q*simLen + r_idx + 1 : q*simLen + r_idx + 1 + self.w]\n",
    "        # to unpack this data into X (image) and p (cfg settings) arrays, use the following code\n",
    "        U_x, p_x = zip(*x)\n",
    "        U_y, p_y = zip(*y)\n",
    "        return np.array(U_x), np.array(U_y), np.array(p_x), np.array(p_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:49.057285Z",
     "start_time": "2020-10-30T03:01:04.134603Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:03<00:00, 519.00it/s]\n",
      "100%|██████████| 19000/19000 [00:41<00:00, 462.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19000, 2000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasets may be smaller because: numSamplesToKeep \n",
    "testDataset = MantaFlowDataset(testData, reverseXY=reverseXY, numToKeep=numSamplesToKeep, AE=False,\n",
    "                               w=simLen-1, simLen=simLen)\n",
    "trainDataset = MantaFlowDataset(trainData, reverseXY=reverseXY,numToKeep=numSamplesToKeep, AE=False,\n",
    "                                w=window_size, simLen=simLen)\n",
    "len(trainDataset), len(testDataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:50.824767Z",
     "start_time": "2020-10-30T03:01:49.059804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 2, 128, 96]) torch.Size([20, 1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 2, 128, 96]),\n",
       " torch.Size([20, 10, 2, 128, 96]),\n",
       " torch.Size([20, 3]),\n",
       " torch.Size([20, 10, 3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataLoader = DataLoader(dataset=trainDataset, batch_size=bz, shuffle=True, drop_last=True, pin_memory = True)\n",
    "testDataLoader = DataLoader(dataset=testDataset, batch_size=bz, pin_memory=True)\n",
    "# only use the first frame of each simulation as the input (U_x) for the full simulation test dataloader, \n",
    "# the next 199 frames are stored in the targets (U_y).\n",
    "# added this on 10/27/2020\n",
    "first_frame_testDataset = torch.utils.data.Subset(testDataset, range(0, len(testDataset), simLen))\n",
    "simulation_testDataLoader = DataLoader(dataset= first_frame_testDataset, batch_size=2)\n",
    "\n",
    "U_x, U_y, p_x, p_y = next(iter(trainDataLoader))\n",
    "print(U_x.shape, p_x.shape)\n",
    "U_x = U_x.squeeze(1) # squeeze away the window dimension for inputs, not targets (e.g., not U_y)\n",
    "p_x = p_x.squeeze(1) # squeeze away the window dimension for inputs, not targets (e.g., not U_y)\n",
    "U_x.shape, U_y.shape, p_x.shape, p_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:50.839298Z",
     "start_time": "2020-10-30T03:01:50.826792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 65.0000,  66.0000,  67.0000,  68.0000,  69.0000,  70.0000,  71.0000,\n",
       "          72.0000,  73.0000,  74.0000],\n",
       "        [151.0000, 152.0000, 153.0000, 154.0000, 155.0000, 156.0000, 157.0000,\n",
       "         158.0000, 159.0000, 160.0000],\n",
       "        [185.0000, 186.0000, 187.0000, 188.0000, 189.0000, 190.0000, 191.0000,\n",
       "         192.0000, 193.0000, 194.0000],\n",
       "        [137.0000, 138.0000, 139.0000, 140.0000, 141.0000, 142.0000, 143.0000,\n",
       "         144.0000, 145.0000, 146.0000],\n",
       "        [173.0000, 174.0000, 175.0000, 176.0000, 177.0000, 178.0000, 179.0000,\n",
       "         180.0000, 181.0000, 182.0000],\n",
       "        [153.0000, 154.0000, 155.0000, 156.0000, 157.0000, 158.0000, 159.0000,\n",
       "         160.0000, 161.0000, 162.0000],\n",
       "        [ 16.0000,  17.0000,  18.0000,  19.0000,  20.0000,  21.0000,  22.0000,\n",
       "          23.0000,  24.0000,  25.0000],\n",
       "        [ 73.0000,  74.0000,  75.0000,  76.0000,  77.0000,  78.0000,  79.0000,\n",
       "          80.0000,  81.0000,  82.0000],\n",
       "        [131.0000, 132.0000, 133.0000, 134.0000, 135.0000, 136.0000, 137.0000,\n",
       "         138.0000, 139.0000, 140.0000],\n",
       "        [145.0000, 146.0000, 147.0000, 148.0000, 149.0000, 150.0000, 151.0000,\n",
       "         152.0000, 153.0000, 154.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm that timestep number in p_y is contiguous within a window\n",
    "((p_y[:,:,2]+1)/2 * 199)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:50.987452Z",
     "start_time": "2020-10-30T03:01:50.840890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 10\n",
      "tensor([  1.0000,   2.0000,   3.0000,   4.0000,   5.0000,   6.0000,   7.0000,\n",
      "          8.0000,   9.0000,  10.0000,  11.0000,  12.0000,  13.0000,  14.0000,\n",
      "         15.0000,  16.0000,  17.0000,  18.0000,  19.0000,  20.0000,  21.0000,\n",
      "         22.0000,  23.0000,  24.0000,  25.0000,  26.0000,  27.0000,  28.0000,\n",
      "         29.0000,  30.0000,  31.0000,  32.0000,  33.0000,  34.0000,  35.0000,\n",
      "         36.0000,  37.0000,  38.0000,  39.0000,  40.0000,  41.0000,  42.0000,\n",
      "         43.0000,  44.0000,  45.0000,  46.0000,  47.0000,  48.0000,  49.0000,\n",
      "         50.0000,  51.0000,  52.0000,  53.0000,  54.0000,  55.0000,  56.0000,\n",
      "         57.0000,  58.0000,  59.0000,  60.0000,  61.0000,  62.0000,  63.0000,\n",
      "         64.0000,  65.0000,  66.0000,  67.0000,  68.0000,  69.0000,  70.0000,\n",
      "         71.0000,  72.0000,  73.0000,  74.0000,  75.0000,  76.0000,  77.0000,\n",
      "         78.0000,  79.0000,  80.0000,  81.0000,  82.0000,  83.0000,  84.0000,\n",
      "         85.0000,  86.0000,  87.0000,  88.0000,  89.0000,  90.0000,  91.0000,\n",
      "         92.0000,  93.0000,  94.0000,  95.0000,  96.0000,  97.0000,  98.0000,\n",
      "         99.0000, 100.0000, 101.0000, 102.0000, 103.0000, 104.0000, 105.0000,\n",
      "        106.0000, 107.0000, 108.0000, 109.0000, 110.0000, 111.0000, 112.0000,\n",
      "        113.0000, 114.0000, 115.0000, 116.0000, 117.0000, 118.0000, 119.0000,\n",
      "        120.0000, 121.0000, 122.0000, 123.0000, 124.0000, 125.0000, 126.0000,\n",
      "        127.0000, 128.0000, 129.0000, 130.0000, 131.0000, 132.0000, 133.0000,\n",
      "        134.0000, 135.0000, 136.0000, 137.0000, 138.0000, 139.0000, 140.0000,\n",
      "        141.0000, 142.0000, 143.0000, 144.0000, 145.0000, 146.0000, 147.0000,\n",
      "        148.0000, 149.0000, 150.0000, 151.0000, 152.0000, 153.0000, 154.0000,\n",
      "        155.0000, 156.0000, 157.0000, 158.0000, 159.0000, 160.0000, 161.0000,\n",
      "        162.0000, 163.0000, 164.0000, 165.0000, 166.0000, 167.0000, 168.0000,\n",
      "        169.0000, 170.0000, 171.0000, 172.0000, 173.0000, 174.0000, 175.0000,\n",
      "        176.0000, 177.0000, 178.0000, 179.0000, 180.0000, 181.0000, 182.0000,\n",
      "        183.0000, 184.0000, 185.0000, 186.0000, 187.0000, 188.0000, 189.0000,\n",
      "        190.0000, 191.0000, 192.0000, 193.0000, 194.0000, 195.0000, 196.0000,\n",
      "        197.0000, 198.0000, 199.0000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 2, 128, 96]),\n",
       " torch.Size([2, 199, 2, 128, 96]),\n",
       " torch.Size([2, 1, 3]),\n",
       " torch.Size([2, 199, 3]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are now subsetting this data to only include the first frame in each sim, p_y and U_y include next 199 frames\n",
    "print(len(simulation_testDataLoader), len(simulation_testDataLoader.dataset))\n",
    "for batch in simulation_testDataLoader:\n",
    "    test_U_x, test_U_y, test_p_x, test_p_y = batch\n",
    "    assert (test_p_x[0,:,2]+1)/2 * 199 == 0\n",
    "    assert (test_p_x[1,:,2]+1)/2 * 199 == 0\n",
    "\n",
    "print(((test_p_y[0,:,2]+1)/2 * 199))\n",
    "test_U_x.shape, test_U_y.shape, test_p_x.shape, test_p_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:51.679262Z",
     "start_time": "2020-10-30T03:01:50.988940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['spatialVecs', 'S', 'timeVecs_transpose'])\n"
     ]
    }
   ],
   "source": [
    "# SVD vectors\n",
    "SVDFn = '/data/mantaFlowSim/data/smoke_pos21_size5_f200/svd/svd.pkl'\n",
    "svd_vec_file = '/data/mantaFlowSim/data/smoke_pos21_size5_f200/svd/mantaSVDvecs.pkl'\n",
    "\n",
    "svd_data = pkl_load(SVDFn)\n",
    "print(svd_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:51.683910Z",
     "start_time": "2020-10-30T03:01:51.680391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24576, 19000) (200, 200)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_vecs = svd_data['spatialVecs'][:,:19000]\n",
    "time_vecs = svd_data['timeVecs_transpose']\n",
    "print(svd_vecs.shape, time_vecs.shape)\n",
    "svd_data['S'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:52.108663Z",
     "start_time": "2020-10-30T03:01:51.684923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see top of page 5 here https://www.cs.ox.ac.uk/people/james.worrell/SVD-thin.pdf\n",
    "# we need about 4000 SVD vectors to get reconstruction relative error down to O(1e-2)\n",
    "max_numComponents = 4000\n",
    "matrix_norm = np.sum(svd_data['S']**2)\n",
    "error_norm = lambda numComponents: np.sum(svd_data['S'][numComponents:]**2)\n",
    "plt.semilogy(range(max_numComponents), \n",
    "         np.sqrt([error_norm(numComponents)/matrix_norm for numComponents in range(max_numComponents)]))\n",
    "plt.plot(range(max_numComponents),[1e-2]*max_numComponents, linestyle='--')\n",
    "plt.xlabel(\"number of SVD vecs used\")\n",
    "plt.ylabel(\"relative error (frobenius norm)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:54.353891Z",
     "start_time": "2020-10-30T03:01:52.109837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5485571693056548e-06\n",
      "^ this should have zero reconstruction error\n",
      "\n",
      "0.007698855317373547\n",
      "^ this error from using 4000 num components should be O(10^-2) and be exactly equal to...\n",
      "0.0076988577\n"
     ]
    }
   ],
   "source": [
    "# grab a frame and find its error\n",
    "frame_num = 300\n",
    "recon, all_coeffs = reconFrame(svd_vecs, trainDataset.data[frame_num][0], 19000)\n",
    "print(np.linalg.norm(recon-trainDataset.data[frame_num][0]) /\n",
    "              np.linalg.norm(trainDataset.data[frame_num][0]))\n",
    "print('^ this should have zero reconstruction error\\n')\n",
    "\n",
    "numComponents = 4000\n",
    "recon, coeffs = reconFrame(svd_vecs, trainDataset.data[frame_num][0], numComponents)\n",
    "print(np.linalg.norm(recon-trainDataset.data[frame_num][0]) / np.linalg.norm(trainDataset.data[frame_num][0]))\n",
    "print('^ this error from using 4000 num components should be O(10^-2) and be exactly equal to...')\n",
    "print(np.sqrt(np.sum(all_coeffs[numComponents:]**2)/np.sum(all_coeffs**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:56.667385Z",
     "start_time": "2020-10-30T03:01:54.355148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.007459851923217e-06\n",
      "^ this should have zero reconstruction error\n",
      "\n",
      "0.01238280828782689\n",
      "^ this error from using 4000 num components should be O(10^-2) and be exactly equal to...\n",
      "0.012382787\n"
     ]
    }
   ],
   "source": [
    "# try for another frame:\n",
    "frame_num = 50\n",
    "recon, all_coeffs = reconFrame(svd_vecs, trainDataset.data[frame_num][0], 19000)\n",
    "print(np.linalg.norm(recon-trainDataset.data[frame_num][0]) /\n",
    "              np.linalg.norm(trainDataset.data[frame_num][0]))\n",
    "print('^ this should have zero reconstruction error\\n')\n",
    "recon, coeffs = reconFrame(svd_vecs, trainDataset.data[frame_num][0], numComponents)\n",
    "print(np.linalg.norm(recon-trainDataset.data[frame_num][0]) / np.linalg.norm(trainDataset.data[frame_num][0]))\n",
    "print('^ this error from using 4000 num components should be O(10^-2) and be exactly equal to...')\n",
    "print(np.sqrt(np.sum(all_coeffs[numComponents:]**2)/np.sum(all_coeffs**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:56.775977Z",
     "start_time": "2020-10-30T03:01:56.668651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012382808\n",
      "0.0076988568\n"
     ]
    }
   ],
   "source": [
    "def reconBatchOfFrames(u,frames, numComp=4000):\n",
    "    # u is from u,s,vh = svd(data)\n",
    "    # frames = batch_size x channels x height x width\n",
    "    assert len(frames.shape)==4\n",
    "    x = frames.reshape(len(frames), -1)\n",
    "    coeffs = x@u[:,:numComp]\n",
    "    # coeffs is now batch_size x numComp\n",
    "    R = u[:,:numComp]@coeffs.T\n",
    "    R = R.T.reshape(frames.shape)\n",
    "    return R, coeffs\n",
    "\n",
    "recons, coeffs = reconBatchOfFrames(svd_vecs, \n",
    "                np.stack((trainDataset.data[50][0],trainDataset.data[300][0])), 4000)\n",
    "print(np.linalg.norm(recons[0]-trainDataset.data[50][0]) / np.linalg.norm(trainDataset.data[50][0]))\n",
    "print(np.linalg.norm(recons[1]-trainDataset.data[300][0]) / np.linalg.norm(trainDataset.data[300][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:56.822024Z",
     "start_time": "2020-10-30T03:01:56.777268Z"
    }
   },
   "outputs": [],
   "source": [
    "class SVD_Encoder(nn.Module):\n",
    "    def __init__(self, U):\n",
    "        super(SVD_Encoder,self).__init__()\n",
    "        self.U = U\n",
    "\n",
    "    def forward(self, frames):\n",
    "        # u is from u,s,vh = svd(data)\n",
    "        # frames = batch_size x channels x height x width\n",
    "        assert len(frames.shape)==4\n",
    "        x = torch.tensor(frames.reshape(len(frames), -1))\n",
    "        coeffs = x.matmul(self.U)\n",
    "        # coeffs is now batch_size x numComp\n",
    "        return coeffs    \n",
    "    \n",
    "class SVD_Decoder(nn.Module):\n",
    "    def __init__(self, U):\n",
    "        super(SVD_Decoder,self).__init__()\n",
    "        self.U = U\n",
    "\n",
    "    def forward(self, coeffs, orig_shape):\n",
    "        # coeffs is now batch_size x numComp\n",
    "        R = self.U.matmul(coeffs.T)\n",
    "        R = R.T.reshape(orig_shape)\n",
    "        return R\n",
    "    \n",
    "class SVD_Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self, svd_vectors, latentDim, allow_updates_to_U):\n",
    "        super(SVD_Autoencoder,self).__init__()\n",
    "        self.U = nn.Parameter(torch.tensor(svd_vectors[:,:latentDim]), requires_grad = allow_updates_to_U)\n",
    "        self.encoder=SVD_Encoder(self.U)\n",
    "        self.decoder=SVD_Decoder(self.U)\n",
    "        # initialize these last few vectors to 0, they will connect to the p dimensions of the latent space\n",
    "        self.U[:,-p_x.size(1):] = 0\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        return self.decoder(self.encoder(frames))\n",
    "\n",
    "SVD_autoencoder = SVD_Autoencoder(svd_vecs, latentDim, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:56.866647Z",
     "start_time": "2020-10-30T03:01:56.823241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012382806\n",
      "0.0076988563\n"
     ]
    }
   ],
   "source": [
    "z = SVD_autoencoder.encoder(np.stack((trainDataset.data[50][0],trainDataset.data[300][0])))\n",
    "recons = SVD_autoencoder.decoder(z, \n",
    "                np.stack((trainDataset.data[50][0],trainDataset.data[300][0])).shape)\n",
    "print(np.linalg.norm(recons[0]-trainDataset.data[50][0]) / np.linalg.norm(trainDataset.data[50][0]))\n",
    "print(np.linalg.norm(recons[1]-trainDataset.data[300][0]) / np.linalg.norm(trainDataset.data[300][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:56.870360Z",
     "start_time": "2020-10-30T03:01:56.867772Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'end_to_end_plateau_train_GPUs13_latentDim4003_window_size10_bz20_SVDTrue_streamFalse_jacobianTrue_epochs1000_stackTrue'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "versionName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:56.887148Z",
     "start_time": "2020-10-30T03:01:56.871397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nif len(gpu_ids.split(',')) > 1:\\n    LIN_model = nn.DataParallel(LIN_model)\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LIN Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, X, hiddenLayerSizes = [1024], activation=nn.ELU()):\n",
    "        super(MLP,self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.inputSize = X.shape[1:]\n",
    "        self.modules = []\n",
    "        self.modules.append(nn.Linear(np.prod(self.inputSize),hiddenLayerSizes[0], bias=False))\n",
    "        self.modules.append(nn.BatchNorm1d(hiddenLayerSizes[0]))\n",
    "        self.modules.append(self.activation)\n",
    "        self.modules.append(nn.Dropout(p=0.1))\n",
    "        for idx,sz in enumerate(hiddenLayerSizes[:-1]):\n",
    "            self.modules.append(nn.Linear(hiddenLayerSizes[idx],hiddenLayerSizes[idx+1], bias=False))\n",
    "            self.modules.append(nn.BatchNorm1d(hiddenLayerSizes[0]))\n",
    "            self.modules.append(self.activation)\n",
    "            self.modules.append(nn.Dropout(p=0.1))\n",
    "                               \n",
    "        self.modules.append(nn.Linear(hiddenLayerSizes[-1],np.prod(self.inputSize)))\n",
    "        self.layers = nn.Sequential(*self.modules)\n",
    "                                \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "    \n",
    "hiddenLayers = [128,128]\n",
    "LIN_model = MLP(z, hiddenLayerSizes=hiddenLayers, activation=nn.ELU())\n",
    "'''\n",
    "if len(gpu_ids.split(',')) > 1:\n",
    "    LIN_model = nn.DataParallel(LIN_model)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:56.895266Z",
     "start_time": "2020-10-30T03:01:56.888210Z"
    }
   },
   "outputs": [],
   "source": [
    "# surrogate class\n",
    "\n",
    "class Surrogate(nn.Module):\n",
    "    \n",
    "    def __init__(self, window,\n",
    "                 z_size, p_size,\n",
    "                LIN, encoder, decoder):\n",
    "        super(Surrogate, self).__init__()\n",
    "        self.window = window\n",
    "        self.z_size = z_size # this does not include the size of p\n",
    "        self.p_size = p_size\n",
    "        self.c_size = z_size + p_size # this does include the size of p\n",
    "        self.LIN = LIN\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def encode(self, U):\n",
    "        \n",
    "        self.shape_of_last_frames_encoded = U.shape\n",
    "        \n",
    "        return self.encoder(U)\n",
    "        \n",
    "    def decode(self, encoding):\n",
    "        \n",
    "        return self.decoder(encoding, self.shape_of_last_frames_encoded)\n",
    "        \n",
    "    def predict_next_w_encodings(self, encoding, p_y, window):\n",
    "        '''\n",
    "        use the LIN to predict the next w encodings for each \n",
    "        encoded U in the batch\n",
    "        '''\n",
    "            \n",
    "        predicted_encodings = []\n",
    "            \n",
    "        # given a batch of encodings, advance each encoding window time steps.\n",
    "        # save the result at each time step\n",
    "        for i in range(window):\n",
    "            encoding = self.LIN(encoding) + encoding # use LIN to predict delta in encoding\n",
    "            # this was encoding[:,:,-self.p_size:] in 09_manta..., why the extra dimension?\n",
    "            encoding[:,-self.p_size:] = p_y[:, i]\n",
    "            predicted_encodings.append(encoding)\n",
    "            \n",
    "            \n",
    "        return torch.stack(predicted_encodings)\n",
    "    \n",
    "    def forward(self, U, p_x, p_y, window = None):\n",
    "        \n",
    "        if window == None:\n",
    "            window = self.window\n",
    "        assert p_y.size(1) == window\n",
    "            \n",
    "        encoding = self.encode(U)\n",
    "        encoding[:,-self.p_size:] = p_x # added this on 10/27/2020\n",
    "        encoding_w = self.predict_next_w_encodings(encoding, p_y, window)\n",
    "        # want to have this agree with U_y, which is [batch_size, window_size, channels, nx, ny]\n",
    "        # right now, it's [window_size, batch_size, c_size], so transpose dimensions 0 and 1\n",
    "        # print(encoding_w.shape)\n",
    "        U = torch.stack([self.decode(encoding_i) for encoding_i in encoding_w])\n",
    "        \n",
    "        return U.transpose(0,1)\n",
    "    \n",
    "    \n",
    "surrogate = Surrogate(window_size, latentDim - 3, 3, LIN_model, SVD_autoencoder.encoder, \n",
    "                      SVD_autoencoder.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:56.901153Z",
     "start_time": "2020-10-30T03:01:56.896323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1],\n",
       "         [2, 2],\n",
       "         [3, 3]]),\n",
       " tensor([[1, 1, 2],\n",
       "         [2, 3, 3]]),\n",
       " tensor([[1, 2, 3],\n",
       "         [1, 2, 3]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note the important difference here\n",
    "foo = torch.tensor([[1,1],[2,2],[3,3]])\n",
    "foo, foo.reshape(2,3), foo.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:56.954479Z",
     "start_time": "2020-10-30T03:01:56.902244Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-ef994b2ccc56>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(frames.reshape(len(frames), -1))\n"
     ]
    }
   ],
   "source": [
    "encoding = surrogate.encode(U_x)\n",
    "decoding = surrogate.decode(encoding)\n",
    "assert surrogate.c_size == latentDim\n",
    "assert surrogate.p_size == len(p_x[0])\n",
    "assert encoding.shape[-1] == surrogate.c_size\n",
    "assert decoding.shape == U_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:57.327142Z",
     "start_time": "2020-10-30T03:01:56.955605Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-ef994b2ccc56>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(frames.reshape(len(frames), -1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 10, 2, 128, 96]),\n",
       " torch.Size([20, 10, 2, 128, 96]),\n",
       " tensor(9832.3887, grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_hat = surrogate.forward(U_x, p_x, p_y)\n",
    "U_hat.shape, U_y.shape, torch.norm(U_hat-U_y,p=1)/torch.norm(U_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:57.330139Z",
     "start_time": "2020-10-30T03:01:57.328222Z"
    }
   },
   "outputs": [],
   "source": [
    "del surrogate, encoding, decoding, U_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:57.497755Z",
     "start_time": "2020-10-30T03:01:57.331243Z"
    }
   },
   "outputs": [],
   "source": [
    "SVD_autoencoder = SVD_Autoencoder(svd_vecs, latentDim, False)\n",
    "surrogate = Surrogate(window_size, latentDim - 3, 3, LIN_model, SVD_autoencoder.encoder, \n",
    "                      SVD_autoencoder.decoder).to(device)\n",
    "\n",
    "if len(gpu_ids.split(',')) > 1:\n",
    "    surrogate = nn.DataParallel(surrogate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:57.504009Z",
     "start_time": "2020-10-30T03:01:57.498907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 4.1633e-17, -2.0817e-17,  2.0817e-17,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [-1.1102e-16,  2.7756e-17,  1.0408e-16,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  3.4694e-17,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        ...,\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVD_autoencoder.encoder.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:57.507568Z",
     "start_time": "2020-10-30T03:01:57.504998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVD_autoencoder.encoder.U is SVD_autoencoder.decoder.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:57.511412Z",
     "start_time": "2020-10-30T03:01:57.508601Z"
    }
   },
   "outputs": [],
   "source": [
    "max_lr = .0001\n",
    "start_lr = 5*max_lr/10\n",
    "#opt = create_opt(max_lr,model)\n",
    "#lr_scheduler = create_one_cycle(opt,max_lr,epochs,trainDataLoader)\n",
    "opt = torch.optim.Adam(surrogate.parameters(),lr=max_lr,betas=(.5,.999))\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:57.517941Z",
     "start_time": "2020-10-30T03:01:57.512462Z"
    }
   },
   "outputs": [],
   "source": [
    "def L1_loss(pred, target):\n",
    "    return torch.mean(torch.abs(pred - target))\n",
    "\n",
    "\n",
    "def jacobian_loss(pred, target, device='cpu'):\n",
    "    return L1_loss(\n",
    "        torch.stack([jacobian(pred_window_i, device) for pred_window_i in pred.transpose(0,1)]), \n",
    "        torch.stack([jacobian(target_window_i, device) for target_window_i in target.transpose(0,1)])\n",
    "        )\n",
    "\n",
    "\n",
    "def curl_loss(pred, target, device):\n",
    "    return L1_loss(curl(pred, device), curl(target, device))\n",
    "\n",
    "\n",
    "L = nn.MSELoss()\n",
    "\n",
    "\n",
    "def p_loss(pred, target):\n",
    "    return L(pred[:, -target.shape[1]:], target)\n",
    "\n",
    "\n",
    "def loss(pred, target, device):\n",
    "    \n",
    "    if createStreamFcn:\n",
    "        pred = stream2uv(pred, device)\n",
    "        \n",
    "    L1 = L1_loss(pred, target)\n",
    "    Lj = 0\n",
    "    if doJacobian:\n",
    "        Lj = jacobian_loss(pred, target, device)\n",
    "        \n",
    "    return L1 + Lj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:57.525841Z",
     "start_time": "2020-10-30T03:01:57.519019Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainEpoch(myDataLoader, tensorboard_writer, model, opt, p_loss, loss,\n",
    "               metric, lr_scheduler, tensorboard_rate, device,\n",
    "               tensorboard_recorder_step, total_steps):\n",
    "    running_loss = 0.0\n",
    "    running_rmse = 0.0\n",
    "    total_loss = 0.0\n",
    "    running_ploss = 0.0\n",
    "    for i, sampleBatch in enumerate(myDataLoader, start=1):\n",
    "\n",
    "        # --- Main Training ---\n",
    "        \n",
    "        # gpu\n",
    "        U_x, U_y, p_x, p_y = sampleBatch\n",
    "        U_x = U_x.squeeze(1).to(device)\n",
    "        p_x = p_x.squeeze(1).to(device)\n",
    "        U_y = U_y.to(device)\n",
    "        p_y = p_y.to(device)\n",
    "            \n",
    "\n",
    "        # zero the parameter gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        U_hat = model(U_x, p_x, p_y)\n",
    "        pl = 0\n",
    "        ll = loss(U_hat, U_y, device)\n",
    "        combined_loss = pl + ll\n",
    "        combined_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # loss\n",
    "        batch_loss = combined_loss.item()\n",
    "        running_loss += batch_loss\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        batch_ploss = pl\n",
    "        running_ploss += batch_ploss\n",
    "\n",
    "        # --- Metrics Recording ---\n",
    "\n",
    "        # metrics\n",
    "        r = metric(U_hat, U_y)\n",
    "        running_rmse += r\n",
    "\n",
    "        total_steps += 1\n",
    "\n",
    "        # tensorboard writes\n",
    "        if (i % tensorboard_rate == 0):\n",
    "            tensorboard_recorder_step += 1\n",
    "            avg_running_loss = running_loss/tensorboard_rate\n",
    "            avg_running_rmse = running_rmse/tensorboard_rate\n",
    "            avg_running_ploss = running_ploss/tensorboard_rate\n",
    "            tensorboard_writer.add_scalar(tag=\"Loss\", scalar_value=avg_running_loss, global_step=tensorboard_recorder_step)\n",
    "            tensorboard_writer.add_scalar(tag=\"p_loss\", scalar_value=avg_running_ploss, global_step=tensorboard_recorder_step)\n",
    "            tensorboard_writer.add_scalar(tag=metric.__name__, scalar_value=avg_running_rmse, global_step=tensorboard_recorder_step)\n",
    "            # reset running_loss for the next set of batches. (tensorboard_rate number of batches)\n",
    "            running_loss = 0.0\n",
    "            running_rmse = 0.0\n",
    "            running_ploss = 0.0\n",
    "            tensorboard_writer.flush()\n",
    "\n",
    "    return total_loss/len(myDataLoader), tensorboard_recorder_step, total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:57.533746Z",
     "start_time": "2020-10-30T03:01:57.526907Z"
    }
   },
   "outputs": [],
   "source": [
    "def relative_error_info(model, data, tensorboard_writer, tensorboard_recorder_step, quiet=False):\n",
    "    U_hats = []\n",
    "    Us = []\n",
    "    for i, sampleBatch in enumerate(data, start=1):\n",
    "        U_x, U_y, p_x, p_y = sampleBatch\n",
    "        U_x = U_x.squeeze(1).to(device)\n",
    "        p_x = p_x.squeeze(1).to(device)\n",
    "        U_y = U_y.to(device)\n",
    "        p_y = p_y.to(device)\n",
    "        with torch.no_grad():\n",
    "            Us.append(U_y.detach().cpu())\n",
    "            U_hat = model(U_x, p_x, p_y, window=simLen-1)\n",
    "            U_hats.append(U_hat.detach().cpu())\n",
    "            \n",
    "    Real_U = torch.stack(Us)\n",
    "    Surr_U = torch.stack(U_hats)\n",
    "    rel_error = torch.norm(Real_U - Surr_U)/torch.norm(Real_U)\n",
    "    writeMessage(\"Relative_Error: {:.4e}\".format(rel_error),versionName)\n",
    "    if not quiet:\n",
    "        tensorboard_writer.add_scalar(tag=\"Relative_Error\", scalar_value=rel_error,\n",
    "                                  global_step=tensorboard_recorder_step)\n",
    "\n",
    "    \n",
    "    sample_sim = U_hat[0].cpu()\n",
    "    sample_GT = U_y[0].cpu()\n",
    "    fig = plt.figure()\n",
    "    frame_error = torch.norm((sample_GT - sample_sim).view(simLen-1,-1), \n",
    "                                 dim=1)/torch.norm(sample_GT.view(simLen-1,-1), dim=1)\n",
    "    plt.plot(range(1,200),frame_error)\n",
    "    plt.title(\"Relative Error over Time for One Test Simulation\")\n",
    "    if not quiet:\n",
    "        tensorboard_writer.add_figure('test_rel_error_by_frame', fig, global_step=tensorboard_recorder_step,\n",
    "                                  close=True, walltime=None)\n",
    "    \n",
    "    return sample_sim, sample_GT, rel_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:01:57.540423Z",
     "start_time": "2020-10-30T03:01:57.534813Z"
    }
   },
   "outputs": [],
   "source": [
    "def validEpoch(myDataLoader, tensorboard_writer, model, p_loss, loss, metric,\n",
    "               device, tensorboard_recorder_step, lr_scheduler):\n",
    "    running_loss = 0.0\n",
    "    running_rmse = 0.0\n",
    "    for i, sampleBatch in enumerate(myDataLoader, start=1):\n",
    "\n",
    "        # --- Metrics Recording ---\n",
    "\n",
    "        # gpu\n",
    "        U_x, U_y, p_x, p_y = sampleBatch\n",
    "        U_x = U_x.squeeze(1).to(device) # only squeeze away the window dimension (because batch size = 1)\n",
    "        p_x = p_x.squeeze(1).to(device) # only squeeze away the window dimension (because batch size = 1)\n",
    "        U_y = U_y.to(device)\n",
    "        p_y = p_y.to(device)\n",
    "        \n",
    "        perc = len(U_x)/len(myDataLoader.dataset)\n",
    "\n",
    "        # forward, no gradient calculations\n",
    "        with torch.no_grad():\n",
    "            U_hat = model(U_x, p_x, p_y, window = simLen-1)\n",
    "\n",
    "        # loss\n",
    "        combined_loss = loss(U_hat, U_y, device)\n",
    "        \n",
    "        running_loss += perc*(combined_loss.item())\n",
    "\n",
    "        # metrics\n",
    "        r = metric(U_hat, U_y)\n",
    "        running_rmse += perc*r\n",
    "\n",
    "    avg_running_loss = running_loss\n",
    "    avg_running_rmse = running_rmse\n",
    "    tensorboard_writer.add_scalar(tag=\"Loss\", scalar_value=avg_running_loss, global_step=tensorboard_recorder_step)\n",
    "    tensorboard_writer.add_scalar(tag=metric.__name__, scalar_value=avg_running_rmse, global_step=tensorboard_recorder_step)\n",
    "    \n",
    "    _, _, rel_error = relative_error_info(model, simulation_testDataLoader, \n",
    "                                          tensorboard_writer, tensorboard_recorder_step)\n",
    "    tensorboard_writer.flush()\n",
    "    lr_scheduler.step(rel_error)\n",
    "    \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T03:02:01.835605Z",
     "start_time": "2020-10-30T03:01:57.541461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints directory already exists :)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(cps)\n",
    "except:\n",
    "    print(\"checkpoints directory already exists :)\")\n",
    "    \n",
    "# create a summary writer.\n",
    "train_writer = SummaryWriter(os.path.join(tensorboard_direc, versionName,'train'))\n",
    "test_writer = SummaryWriter(os.path.join(tensorboard_direc, versionName,'valid'))\n",
    "tensorboard_recorder_step = 0\n",
    "total_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-30T03:01:02.810Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Started Training ----------\n",
      "--- Epoch 1/1000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-ef994b2ccc56>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(frames.reshape(len(frames), -1))\n"
     ]
    }
   ],
   "source": [
    "writeMessage('---------- Started Training ----------', versionName)\n",
    "bestLoss = np.infty\n",
    "\n",
    "if not eval_only:\n",
    "    for epoch in tqdm(range(1, epochs+1)):  # loop over the dataset multiple times\n",
    "\n",
    "        writeMessage(\"--- Epoch {0}/{1} ---\".format(epoch, epochs), versionName)\n",
    "\n",
    "        surrogate.train()\n",
    "        trainLoss, tensorboard_recorder_step, total_steps = trainEpoch(trainDataLoader, \n",
    "                                                                       train_writer, surrogate,\n",
    "                                                                       opt, p_loss, loss,\n",
    "                                                                       rmse, lr_scheduler, \n",
    "                                                                       tensorboard_rate, device,\n",
    "                                                                       tensorboard_recorder_step, total_steps)\n",
    "\n",
    "        writeMessage(\"trainLoss: {:.4e}\".format(trainLoss),versionName)\n",
    "        writeMessage(\"LR: {:.4e}\".format(opt.param_groups[0]['lr']),versionName)\n",
    "#         if trainLoss < bestLoss:\n",
    "#             bestLoss = trainLoss\n",
    "#             writeMessage(\"Better trainLoss: {:.4e}, Saving models...\".format(bestLoss),versionName)\n",
    "#             torch.save(surrogate.state_dict(), os.path.join(cps,versionName))\n",
    "\n",
    "        surrogate.eval()\n",
    "        valLoss = validEpoch(testDataLoader, test_writer, surrogate, p_loss, loss, rmse, device,\n",
    "                             tensorboard_recorder_step, lr_scheduler)\n",
    "        writeMessage(\"valLoss: {:.4e}\".format(valLoss),versionName)\n",
    "\n",
    "        # checkpoint progress\n",
    "        if valLoss < bestLoss:\n",
    "            bestLoss = valLoss\n",
    "            writeMessage(\"Better valLoss: {:.4e}, Saving models...\".format(bestLoss),versionName)\n",
    "            torch.save(surrogate.state_dict(), os.path.join(cps,versionName))\n",
    "\n",
    "        # record lr change\n",
    "        train_writer.add_scalar(tag=\"LR\", scalar_value=opt.param_groups[0]['lr'], global_step=total_steps)\n",
    "\n",
    "        if opt.param_groups[0]['lr'] < 5e-8:\n",
    "            break\n",
    "    torch.save(surrogate.state_dict(), os.path.join(cps,versionName+'_final_model'))\n",
    "    writeMessage('---------- Finished Training ----------', versionName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-30T03:01:02.812Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.animation as manimati\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import Video\n",
    "def create_1_channel_movie(im,outfile='sim.mp4',title='surrogate            simulation'):\n",
    "    ti = 0\n",
    "    u_mx = 255 #np.max(np.abs(Xrgb))\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.title(title)\n",
    "    #cmap = plt.cm.ocean\n",
    "    img = ax.imshow(im[0].squeeze(),  vmin=0, vmax=u_mx)\n",
    "    #plt.show()\n",
    "    \n",
    "    # initialization function: plot the background of each frame\n",
    "    def init():\n",
    "        img = ax.imshow(im[0].squeeze(),  vmin=0, vmax=u_mx)\n",
    "        return (fig,)\n",
    "\n",
    "    # animation function. This is called sequentially\n",
    "    def animate(i):\n",
    "        img = ax.imshow(im[i].squeeze(), vmin=0, vmax=u_mx)\n",
    "        return (fig,)\n",
    "\n",
    "\n",
    "    # call the animator. blit=True means only re-draw the parts that have changed.\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                                   frames=len(im), interval=20, blit=True)\n",
    "    anim.save(outfile, fps=30, extra_args=['-vcodec', 'libx264'])\n",
    "    \n",
    "def get_img(X):\n",
    "    M = 255\n",
    "    mx = X.max()\n",
    "    mn = X.min()\n",
    "    X = (X - mn)/(mx - mn)\n",
    "    C = (M*X).type(torch.uint8)\n",
    "    return C\n",
    "\n",
    "def make_movie(sample_sim, sample_GT, ID = ''):\n",
    "    # get sample_sim and sample_GT from the relative_error_info function above\n",
    "    Xrgb = torch.cat([get_img(sample_sim), get_img(sample_GT)], dim=3)[:,0].detach().cpu().numpy()\n",
    "    Xrgb.shape\n",
    "    \n",
    "    video_name = versionName + ID\n",
    "    \n",
    "    # to-do: dave says we should flip vertically before making the mp4\n",
    "    outGif = '{}.mp4'.format(video_name)\n",
    "    create_1_channel_movie(Xrgb,outfile=outGif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-30T03:01:02.815Z"
    }
   },
   "outputs": [],
   "source": [
    "# load surrogate\n",
    "surrogate.load_state_dict(torch.load(os.path.join(cps,versionName))), surrogate.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-30T03:01:02.816Z"
    }
   },
   "outputs": [],
   "source": [
    "# bring in sample_sim and sample_GT \n",
    "sample_sim, sample_GT, rel_error = relative_error_info(surrogate, simulation_testDataLoader, \n",
    "                                                       None, None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-30T03:01:02.818Z"
    }
   },
   "outputs": [],
   "source": [
    "# make movie\n",
    "make_movie(sample_sim, sample_GT)\n",
    "Video('{}.mp4'.format(versionName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-30T03:01:02.820Z"
    }
   },
   "outputs": [],
   "source": [
    "train_w_199 = deepcopy(trainDataset)\n",
    "train_w_199.w = 199\n",
    "first_frame_trainDataset = torch.utils.data.Subset(train_w_199, range(4000, 4400, simLen))\n",
    "simulation_trainDataLoader = DataLoader(dataset= first_frame_trainDataset, batch_size=2)\n",
    "# bring in sample_sim and sample_GT from train data\n",
    "sample_sim, sample_GT, rel_error = relative_error_info(surrogate, simulation_trainDataLoader, \n",
    "                                                       None, None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-30T03:01:02.822Z"
    }
   },
   "outputs": [],
   "source": [
    "make_movie(sample_sim, sample_GT, ID='_train')\n",
    "Video('{}.mp4'.format(versionName+'_train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T16:11:02.018677Z",
     "start_time": "2020-10-30T16:11:02.012788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"end_to_end_plateau_train_GPUs13_latentDim4003_window_size10_bz20_SVDTrue_streamFalse_jacobianTrue_epochs1000_stackTrue.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video('{}.mp4'.format(versionName))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
