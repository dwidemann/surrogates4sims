{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Integration Network (LIN)\n",
    "This notebook gives an example of how to train a LIN on SVD vectors. Note: 05_experiment_SVD.ipynb created and saved the SVD Decomposition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:03.630287Z",
     "start_time": "2020-11-16T05:26:01.797114Z"
    }
   },
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "# --- Must haves ---\n",
    "import os, sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from surrogates4sims.pnnlDatasets import CCSI_2D\n",
    "\n",
    "from surrogates4sims.utils import create_opt, create_one_cycle, find_lr, printNumModelParams, \\\n",
    "                                    rmse, writeMessage, plotSampleWprediction, plotSampleWpredictionByChannel, \\\n",
    "                                    plotSample, curl, jacobian, stream2uv, convertSimToImage, pkl_save, pkl_load, \\\n",
    "                                    create_1_channel_movie\n",
    "\n",
    "from surrogates4sims.models import Generator, Encoder, ConvDeconvFactor2\n",
    "\n",
    "from surrogates4sims.train import trainEpoch, validEpoch\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:03.759879Z",
     "start_time": "2020-11-16T05:26:03.632305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gridsize_128  gridsize_512  svd_channel1_gridsize128.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls /work/pnnl_liquid_inlet/channel_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:03.787996Z",
     "start_time": "2020-11-16T05:26:03.764538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LIN_SVD_PNNL_GPUs1_w10_latentDim1024_hd128_128_128_bz16_epochs1000'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEBUG = False\n",
    "# model name, for tensorboard recording and checkpointing purposes.\n",
    "versionName = \"LIN_SVD_PNNL\"\n",
    "\n",
    "# GPU Numbers to use. Comma seprate them for multi-GPUs.\n",
    "gpu_ids = \"1\"#,1,2,3\"\n",
    "versionName = versionName + '_GPUs{}'.format(gpu_ids.replace(',',''))\n",
    "# path to load model weights.\n",
    "pretrained_path = None\n",
    "\n",
    "# rate at which to record metrics. (number of batches to average over when recording metrics, e.g. \"every 5 batches\")\n",
    "tensorboard_rate = 5\n",
    "\n",
    "# number of epochs to train. This is defined here so we can use the OneCycle LR Scheduler.\n",
    "epochs = 1000\n",
    "\n",
    "# Data Directory\n",
    "channel = 1\n",
    "gridsize = 128\n",
    "dataDirec = '/work/pnnl_liquid_inlet/channel_{}/gridsize_{}'.format(channel,gridsize)\n",
    "build_vecs = True \n",
    "SVDFn = '/work/pnnl_liquid_inlet/channel_1/svd_channel1_gridsize128.pkl'\n",
    "preprocess = False\n",
    "AE = True\n",
    "\n",
    "# checkpoint directory\n",
    "cps = 'cps'\n",
    "tensorboard_direc = \"tb\"\n",
    "\n",
    "findLRs = True  \n",
    "patience = 1\n",
    "\n",
    "# hyper-params\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "testSplit = .2\n",
    "bz = 16\n",
    "numSamplesToKeep = np.infty #if not debugging\n",
    "latentDim = 64\n",
    "simLen = 500\n",
    "\n",
    "w = 10\n",
    "numComponents = 1024 # this does not include p. so the vectors will be of size numComponents + len(p)\n",
    "hiddenLayers =  [128, 128, 128]\n",
    "hd ='_'.join(map(str,hiddenLayers))\n",
    "activation = nn.ELU()\n",
    "\n",
    "if DEBUG:\n",
    "    epochs = 2\n",
    "    numSamplesToKeep = 200\n",
    "    createDebugData = True\n",
    "    \n",
    "\n",
    "versionName = versionName + '_w{}_latentDim{}_hd{}_bz{}_epochs{}'.format(w,numComponents,hd,bz,epochs)\n",
    "versionName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Personal GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:03.977486Z",
     "start_time": "2020-11-16T05:26:03.791362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 15 21:26:03 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN Xp            On   | 00000000:02:00.0 Off |                  N/A |\n",
      "| 23%   19C    P8     8W / 250W |   4274MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN Xp            On   | 00000000:03:00.0 Off |                  N/A |\n",
      "| 23%   20C    P8     8W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN Xp            On   | 00000000:81:00.0 Off |                  N/A |\n",
      "| 23%   20C    P8     8W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  TITAN Xp            On   | 00000000:82:00.0 Off |                  N/A |\n",
      "| 23%   20C    P8     9W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     16705      C   ...iverasoto1/repos/HP1/HP1_env/bin/python  2085MiB |\n",
      "|    0     26377      C   ...iverasoto1/repos/HP1/HP1_env/bin/python  2177MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:03.985517Z",
     "start_time": "2020-11-16T05:26:03.980796Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=gpu_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:04.046855Z",
     "start_time": "2020-11-16T05:26:03.988478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:04.058333Z",
     "start_time": "2020-11-16T05:26:04.049870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "if device.type == 'cuda':\n",
    "    print(cuda.is_available())\n",
    "    print(cuda.device_count())\n",
    "    print(cuda.current_device())\n",
    "    print(cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:06.130447Z",
     "start_time": "2020-11-16T05:26:04.061374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 15 21:26:05 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN Xp            On   | 00000000:02:00.0 Off |                  N/A |\n",
      "| 23%   19C    P8     8W / 250W |   4274MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN Xp            On   | 00000000:03:00.0 Off |                  N/A |\n",
      "| 23%   22C    P2    59W / 250W |    573MiB / 12196MiB |      3%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN Xp            On   | 00000000:81:00.0 Off |                  N/A |\n",
      "| 23%   20C    P8    10W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  TITAN Xp            On   | 00000000:82:00.0 Off |                  N/A |\n",
      "| 23%   20C    P8    11W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     16705      C   ...iverasoto1/repos/HP1/HP1_env/bin/python  2085MiB |\n",
      "|    0     26377      C   ...iverasoto1/repos/HP1/HP1_env/bin/python  2177MiB |\n",
      "|    1     10446      C   /home/bartoldson1/anaconda3/bin/python       561MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(5, device=device.type)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Latent Vectors (Warning....)\n",
    "The computation of building the latent vectors takes a loooong time. \n",
    "This codes checks to see if svd_vec_file has been saved. If it has, \n",
    "it will reload them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:06.155478Z",
     "start_time": "2020-11-16T05:26:06.135577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 40)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = glob(os.path.join(dataDirec,'*.pkl'))\n",
    "numSims = len(sims)\n",
    "idx = int(testSplit*numSims)\n",
    "testInds = np.linspace(1,numSims-2,idx).astype('int')\n",
    "trainInds = list(set(np.arange(0,numSims)).difference(set(testInds)))\n",
    "# perm = np.random.permutation(numSims)\n",
    "# testInds = perm[:idx]\n",
    "# trainInds = perm[idx:]\n",
    "testSimFiles = [sims[idx] for idx in testInds]\n",
    "trainSimFiles = [sims[idx] for idx in trainInds]\n",
    "len(testSimFiles), len(trainSimFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:06.177084Z",
     "start_time": "2020-11-16T05:26:06.157995Z"
    }
   },
   "outputs": [],
   "source": [
    "class CCSI_2D_one_of_each_getitem(Dataset):\n",
    "    def __init__(self, \n",
    "                 dataFiles,\n",
    "                 txtFile = '/data/ccsi/pnnl_liquid_inlet/liquid_inlet_velocity.txt',\n",
    "                 channel=1,\n",
    "                 gridSize=128,\n",
    "                 simLen = 500,\n",
    "                 w = 10, # this is the length of the Y output to predict\n",
    "                 AE = False, # this only return x,x, i.e. no y.\n",
    "                 numToKeep=np.infty,doPreprocess=False): \n",
    "        \n",
    "        self.dataFiles = dataFiles\n",
    "        if numToKeep < len(self.dataFiles):\n",
    "            self.dataFiles = self.dataFiles[:numToKeep]\n",
    "\n",
    "        self.channel = channel\n",
    "        self.gridSize = gridSize\n",
    "        self.numToKeep = numToKeep\n",
    "        self.simLen = simLen\n",
    "        self.t = np.linspace(0,1,simLen).astype('float32')\n",
    "        self.w = w\n",
    "        self.AE = AE\n",
    "        self.doPreprocess = doPreprocess\n",
    "        \n",
    "        # Get the inlet velocity\n",
    "        with open(txtFile) as fid:\n",
    "            txt = fid.read().splitlines()\n",
    "        inletVelocity = np.array(list(map(float,txt[1:]))).astype('float32')\n",
    "        self.inletMx = np.max(inletVelocity)\n",
    "        self.inletMn = np.min(inletVelocity)\n",
    "        \n",
    "        data = []\n",
    "        for fn in self.dataFiles:\n",
    "            idx = int(fn.split('/')[-1].replace('.pkl','')) - 1\n",
    "            D = pkl_load(fn)\n",
    "            data.append((D,inletVelocity[idx]))\n",
    "               \n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.simLen*self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q,r = np.divmod(idx,self.simLen)\n",
    "        r_idx = r\n",
    "            \n",
    "        X,p = self.data[q]\n",
    "        x = X[r_idx:r_idx+1]\n",
    "        #print(x.shape)\n",
    "        y = X[r_idx+1:r_idx+self.w+1]\n",
    "        #print(y.shape)\n",
    "        if self.doPreprocess:\n",
    "            x = self.preprocessFcn(x)\n",
    "            y = self.preprocessFcn(y)\n",
    "        \n",
    "        y = np.expand_dims(y,1)\n",
    "        p_x = np.hstack([p,self.t[r_idx]])\n",
    "        p_y = np.vstack([p*np.ones((self.w,)),self.t[r_idx+1:r_idx+self.w+1]]).T\n",
    "        X = x.astype('float32')\n",
    "        Y = y.astype('float32')\n",
    "        if self.AE:\n",
    "            return X,X # this allows LR_finder to work\n",
    "        else:\n",
    "            return X, Y, p_x, p_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:07.091922Z",
     "start_time": "2020-11-16T05:26:06.180728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 5000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataset = CCSI_2D_one_of_each_getitem(testSimFiles,doPreprocess=preprocess,\n",
    "                                          numToKeep=numSamplesToKeep,channel=channel,w=0,AE=False)\n",
    "trainDataset = CCSI_2D_one_of_each_getitem(trainSimFiles,doPreprocess=preprocess,\n",
    "                                           numToKeep=numSamplesToKeep,channel=channel,w=0,AE=False)\n",
    "len(trainDataset),len(testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:07.096580Z",
     "start_time": "2020-11-16T05:26:07.093123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0031,\n",
       " 0.00585,\n",
       " 0.00268,\n",
       " 0.00416,\n",
       " 0.00295,\n",
       " 0.00531,\n",
       " 0.00243,\n",
       " 0.002,\n",
       " 0.0021,\n",
       " 0.00255,\n",
       " 0.00281,\n",
       " 0.00342,\n",
       " 0.00221,\n",
       " 0.00557,\n",
       " 0.00481,\n",
       " 0.00614,\n",
       " 0.00437,\n",
       " 0.00505,\n",
       " 0.00359,\n",
       " 0.00645,\n",
       " 0.00677,\n",
       " 0.00711,\n",
       " 0.00908,\n",
       " 0.00864,\n",
       " 0.00747,\n",
       " 0.00823,\n",
       " 0.011,\n",
       " 0.0128,\n",
       " 0.0116,\n",
       " 0.0122,\n",
       " 0.0163,\n",
       " 0.0155,\n",
       " 0.0189,\n",
       " 0.0171,\n",
       " 0.0208,\n",
       " 0.0198,\n",
       " 0.0218,\n",
       " 0.018,\n",
       " 0.00953,\n",
       " 0.0141]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is this inlet velocity supposed to be monotonic?\n",
    "[i for x,i in trainDataset.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:07.681380Z",
     "start_time": "2020-11-16T05:26:07.097747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['spatialVecs', 'S', 'timeVecs_transpose'])\n",
      "(16384, 1024)\n"
     ]
    }
   ],
   "source": [
    "svd_data = pkl_load(SVDFn)\n",
    "print(svd_data.keys())\n",
    "\n",
    "svd_vecs = svd_data['spatialVecs'][:,:numComponents]\n",
    "print(svd_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:07.694096Z",
     "start_time": "2020-11-16T05:26:07.682584Z"
    }
   },
   "outputs": [],
   "source": [
    "class SVD_Encoder(nn.Module):\n",
    "    def __init__(self, U):\n",
    "        super(SVD_Encoder,self).__init__()\n",
    "        self.U = U\n",
    "\n",
    "    def forward(self, frames):\n",
    "        # u is from u,s,vh = svd(data)\n",
    "        # frames = batch_size x channels x height x width\n",
    "        assert len(frames.shape)==4\n",
    "        x = frames.reshape(len(frames), -1)\n",
    "        coeffs = x.matmul(self.U)\n",
    "        # coeffs is now batch_size x numComp\n",
    "        return coeffs    \n",
    "    \n",
    "class SVD_Decoder(nn.Module):\n",
    "    def __init__(self, U):\n",
    "        super(SVD_Decoder,self).__init__()\n",
    "        self.U = U\n",
    "\n",
    "    def forward(self, coeffs, orig_shape):\n",
    "        # coeffs is now batch_size x numComp\n",
    "        R = self.U.matmul(coeffs.T)\n",
    "        R = R.T.reshape(orig_shape)\n",
    "        return R\n",
    "    \n",
    "class SVD_Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self, svd_vectors, latentDim, allow_updates_to_U):\n",
    "        super(SVD_Autoencoder,self).__init__()\n",
    "        self.U = nn.Parameter(torch.tensor(svd_vectors[:,:latentDim]), requires_grad = allow_updates_to_U)\n",
    "        self.encoder=SVD_Encoder(self.U)\n",
    "        self.decoder=SVD_Decoder(self.U)\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        return self.decoder(self.encoder(frames))\n",
    "\n",
    "SVD_autoencoder = SVD_Autoencoder(svd_vecs, latentDim, False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.368328Z",
     "start_time": "2020-11-16T05:26:07.695198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 128, 128]) torch.Size([500, 0, 1, 128, 128]) torch.Size([500, 2]) torch.Size([500, 0, 2])\n",
      "num_sims 40\n",
      "torch.Size([500, 1, 128, 128]) torch.Size([500, 0, 1, 128, 128]) torch.Size([500, 2]) torch.Size([500, 0, 2])\n",
      "num_sims 10\n"
     ]
    }
   ],
   "source": [
    "def createSVDdataset(Dataset, latentDim):\n",
    "\n",
    "    # datasets may be smaller because: numSamplesToKeep \n",
    "    # Be careful the default is for the data to be preprocessed. Therefore, we have to invPrecprocess if \n",
    "    # we are looking at relative errors. \n",
    "    loader = DataLoader(dataset=Dataset, batch_size=simLen, shuffle=False, num_workers=4)\n",
    "    X, Y, p_x, p_y = next(iter(loader))\n",
    "    print(X.shape, Y.shape, p_x.shape, p_y.shape)\n",
    "    z = []\n",
    "    p = []\n",
    "    for batch in loader:\n",
    "        X, Y, p_x, p_y = batch\n",
    "        z.append(SVD_autoencoder.encoder(X.to(device)).cpu())\n",
    "        p.append(p_x.cpu())\n",
    "        \n",
    "    z = torch.stack(z).reshape(-1, latentDim)\n",
    "    p = torch.stack(p).reshape(-1, p_x.size(1))\n",
    "    \n",
    "\n",
    "    v = np.arange(0, len(z), simLen)\n",
    "\n",
    "    sims = []\n",
    "    for idx in v:\n",
    "        sims.append((z[idx:idx+simLen],p[idx:idx+simLen]))\n",
    "    print('num_sims {}'.format(len(sims)))\n",
    "    return sims\n",
    "\n",
    "train_data = createSVDdataset(trainDataset, latentDim)\n",
    "test_data = createSVDdataset(testDataset, latentDim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.373846Z",
     "start_time": "2020-11-16T05:26:11.370027Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([500, 64]), torch.Size([500, 2]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].shape, train_data[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.380983Z",
     "start_time": "2020-11-16T05:26:11.375187Z"
    }
   },
   "outputs": [],
   "source": [
    "# reduce the dimensions of z down to the latentDim \n",
    "for idx,d in enumerate(train_data):\n",
    "    X = d[0][:,:latentDim]\n",
    "    p = d[1]\n",
    "    train_data[idx] = (X,p)\n",
    "    \n",
    "for idx,d in enumerate(test_data):\n",
    "    X = d[0][:,:latentDim]\n",
    "    p = d[1]\n",
    "    test_data[idx] = (X,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.398353Z",
     "start_time": "2020-11-16T05:26:11.382292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0031, 0.0000],\n",
      "        [0.0031, 0.0020],\n",
      "        [0.0031, 0.0040],\n",
      "        [0.0031, 0.0060],\n",
      "        [0.0031, 0.0080],\n",
      "        [0.0031, 0.0100],\n",
      "        [0.0031, 0.0120],\n",
      "        [0.0031, 0.0140],\n",
      "        [0.0031, 0.0160],\n",
      "        [0.0031, 0.0180],\n",
      "        [0.0031, 0.0200],\n",
      "        [0.0031, 0.0220],\n",
      "        [0.0031, 0.0240],\n",
      "        [0.0031, 0.0261],\n",
      "        [0.0031, 0.0281],\n",
      "        [0.0031, 0.0301],\n",
      "        [0.0031, 0.0321],\n",
      "        [0.0031, 0.0341],\n",
      "        [0.0031, 0.0361],\n",
      "        [0.0031, 0.0381],\n",
      "        [0.0031, 0.0401],\n",
      "        [0.0031, 0.0421],\n",
      "        [0.0031, 0.0441],\n",
      "        [0.0031, 0.0461],\n",
      "        [0.0031, 0.0481],\n",
      "        [0.0031, 0.0501],\n",
      "        [0.0031, 0.0521],\n",
      "        [0.0031, 0.0541],\n",
      "        [0.0031, 0.0561],\n",
      "        [0.0031, 0.0581],\n",
      "        [0.0031, 0.0601],\n",
      "        [0.0031, 0.0621],\n",
      "        [0.0031, 0.0641],\n",
      "        [0.0031, 0.0661],\n",
      "        [0.0031, 0.0681],\n",
      "        [0.0031, 0.0701],\n",
      "        [0.0031, 0.0721],\n",
      "        [0.0031, 0.0741],\n",
      "        [0.0031, 0.0762],\n",
      "        [0.0031, 0.0782],\n",
      "        [0.0031, 0.0802],\n",
      "        [0.0031, 0.0822],\n",
      "        [0.0031, 0.0842],\n",
      "        [0.0031, 0.0862],\n",
      "        [0.0031, 0.0882],\n",
      "        [0.0031, 0.0902],\n",
      "        [0.0031, 0.0922],\n",
      "        [0.0031, 0.0942],\n",
      "        [0.0031, 0.0962],\n",
      "        [0.0031, 0.0982],\n",
      "        [0.0031, 0.1002],\n",
      "        [0.0031, 0.1022],\n",
      "        [0.0031, 0.1042],\n",
      "        [0.0031, 0.1062],\n",
      "        [0.0031, 0.1082],\n",
      "        [0.0031, 0.1102],\n",
      "        [0.0031, 0.1122],\n",
      "        [0.0031, 0.1142],\n",
      "        [0.0031, 0.1162],\n",
      "        [0.0031, 0.1182],\n",
      "        [0.0031, 0.1202],\n",
      "        [0.0031, 0.1222],\n",
      "        [0.0031, 0.1242],\n",
      "        [0.0031, 0.1263],\n",
      "        [0.0031, 0.1283],\n",
      "        [0.0031, 0.1303],\n",
      "        [0.0031, 0.1323],\n",
      "        [0.0031, 0.1343],\n",
      "        [0.0031, 0.1363],\n",
      "        [0.0031, 0.1383],\n",
      "        [0.0031, 0.1403],\n",
      "        [0.0031, 0.1423],\n",
      "        [0.0031, 0.1443],\n",
      "        [0.0031, 0.1463],\n",
      "        [0.0031, 0.1483],\n",
      "        [0.0031, 0.1503],\n",
      "        [0.0031, 0.1523],\n",
      "        [0.0031, 0.1543],\n",
      "        [0.0031, 0.1563],\n",
      "        [0.0031, 0.1583],\n",
      "        [0.0031, 0.1603],\n",
      "        [0.0031, 0.1623],\n",
      "        [0.0031, 0.1643],\n",
      "        [0.0031, 0.1663],\n",
      "        [0.0031, 0.1683],\n",
      "        [0.0031, 0.1703],\n",
      "        [0.0031, 0.1723],\n",
      "        [0.0031, 0.1743],\n",
      "        [0.0031, 0.1764],\n",
      "        [0.0031, 0.1784],\n",
      "        [0.0031, 0.1804],\n",
      "        [0.0031, 0.1824],\n",
      "        [0.0031, 0.1844],\n",
      "        [0.0031, 0.1864],\n",
      "        [0.0031, 0.1884],\n",
      "        [0.0031, 0.1904],\n",
      "        [0.0031, 0.1924],\n",
      "        [0.0031, 0.1944],\n",
      "        [0.0031, 0.1964],\n",
      "        [0.0031, 0.1984],\n",
      "        [0.0031, 0.2004],\n",
      "        [0.0031, 0.2024],\n",
      "        [0.0031, 0.2044],\n",
      "        [0.0031, 0.2064],\n",
      "        [0.0031, 0.2084],\n",
      "        [0.0031, 0.2104],\n",
      "        [0.0031, 0.2124],\n",
      "        [0.0031, 0.2144],\n",
      "        [0.0031, 0.2164],\n",
      "        [0.0031, 0.2184],\n",
      "        [0.0031, 0.2204],\n",
      "        [0.0031, 0.2224],\n",
      "        [0.0031, 0.2244],\n",
      "        [0.0031, 0.2265],\n",
      "        [0.0031, 0.2285],\n",
      "        [0.0031, 0.2305],\n",
      "        [0.0031, 0.2325],\n",
      "        [0.0031, 0.2345],\n",
      "        [0.0031, 0.2365],\n",
      "        [0.0031, 0.2385],\n",
      "        [0.0031, 0.2405],\n",
      "        [0.0031, 0.2425],\n",
      "        [0.0031, 0.2445],\n",
      "        [0.0031, 0.2465],\n",
      "        [0.0031, 0.2485],\n",
      "        [0.0031, 0.2505],\n",
      "        [0.0031, 0.2525],\n",
      "        [0.0031, 0.2545],\n",
      "        [0.0031, 0.2565],\n",
      "        [0.0031, 0.2585],\n",
      "        [0.0031, 0.2605],\n",
      "        [0.0031, 0.2625],\n",
      "        [0.0031, 0.2645],\n",
      "        [0.0031, 0.2665],\n",
      "        [0.0031, 0.2685],\n",
      "        [0.0031, 0.2705],\n",
      "        [0.0031, 0.2725],\n",
      "        [0.0031, 0.2745],\n",
      "        [0.0031, 0.2766],\n",
      "        [0.0031, 0.2786],\n",
      "        [0.0031, 0.2806],\n",
      "        [0.0031, 0.2826],\n",
      "        [0.0031, 0.2846],\n",
      "        [0.0031, 0.2866],\n",
      "        [0.0031, 0.2886],\n",
      "        [0.0031, 0.2906],\n",
      "        [0.0031, 0.2926],\n",
      "        [0.0031, 0.2946],\n",
      "        [0.0031, 0.2966],\n",
      "        [0.0031, 0.2986],\n",
      "        [0.0031, 0.3006],\n",
      "        [0.0031, 0.3026],\n",
      "        [0.0031, 0.3046],\n",
      "        [0.0031, 0.3066],\n",
      "        [0.0031, 0.3086],\n",
      "        [0.0031, 0.3106],\n",
      "        [0.0031, 0.3126],\n",
      "        [0.0031, 0.3146],\n",
      "        [0.0031, 0.3166],\n",
      "        [0.0031, 0.3186],\n",
      "        [0.0031, 0.3206],\n",
      "        [0.0031, 0.3226],\n",
      "        [0.0031, 0.3246],\n",
      "        [0.0031, 0.3267],\n",
      "        [0.0031, 0.3287],\n",
      "        [0.0031, 0.3307],\n",
      "        [0.0031, 0.3327],\n",
      "        [0.0031, 0.3347],\n",
      "        [0.0031, 0.3367],\n",
      "        [0.0031, 0.3387],\n",
      "        [0.0031, 0.3407],\n",
      "        [0.0031, 0.3427],\n",
      "        [0.0031, 0.3447],\n",
      "        [0.0031, 0.3467],\n",
      "        [0.0031, 0.3487],\n",
      "        [0.0031, 0.3507],\n",
      "        [0.0031, 0.3527],\n",
      "        [0.0031, 0.3547],\n",
      "        [0.0031, 0.3567],\n",
      "        [0.0031, 0.3587],\n",
      "        [0.0031, 0.3607],\n",
      "        [0.0031, 0.3627],\n",
      "        [0.0031, 0.3647],\n",
      "        [0.0031, 0.3667],\n",
      "        [0.0031, 0.3687],\n",
      "        [0.0031, 0.3707],\n",
      "        [0.0031, 0.3727],\n",
      "        [0.0031, 0.3747],\n",
      "        [0.0031, 0.3768],\n",
      "        [0.0031, 0.3788],\n",
      "        [0.0031, 0.3808],\n",
      "        [0.0031, 0.3828],\n",
      "        [0.0031, 0.3848],\n",
      "        [0.0031, 0.3868],\n",
      "        [0.0031, 0.3888],\n",
      "        [0.0031, 0.3908],\n",
      "        [0.0031, 0.3928],\n",
      "        [0.0031, 0.3948],\n",
      "        [0.0031, 0.3968],\n",
      "        [0.0031, 0.3988],\n",
      "        [0.0031, 0.4008],\n",
      "        [0.0031, 0.4028],\n",
      "        [0.0031, 0.4048],\n",
      "        [0.0031, 0.4068],\n",
      "        [0.0031, 0.4088],\n",
      "        [0.0031, 0.4108],\n",
      "        [0.0031, 0.4128],\n",
      "        [0.0031, 0.4148],\n",
      "        [0.0031, 0.4168],\n",
      "        [0.0031, 0.4188],\n",
      "        [0.0031, 0.4208],\n",
      "        [0.0031, 0.4228],\n",
      "        [0.0031, 0.4248],\n",
      "        [0.0031, 0.4269],\n",
      "        [0.0031, 0.4289],\n",
      "        [0.0031, 0.4309],\n",
      "        [0.0031, 0.4329],\n",
      "        [0.0031, 0.4349],\n",
      "        [0.0031, 0.4369],\n",
      "        [0.0031, 0.4389],\n",
      "        [0.0031, 0.4409],\n",
      "        [0.0031, 0.4429],\n",
      "        [0.0031, 0.4449],\n",
      "        [0.0031, 0.4469],\n",
      "        [0.0031, 0.4489],\n",
      "        [0.0031, 0.4509],\n",
      "        [0.0031, 0.4529],\n",
      "        [0.0031, 0.4549],\n",
      "        [0.0031, 0.4569],\n",
      "        [0.0031, 0.4589],\n",
      "        [0.0031, 0.4609],\n",
      "        [0.0031, 0.4629],\n",
      "        [0.0031, 0.4649],\n",
      "        [0.0031, 0.4669],\n",
      "        [0.0031, 0.4689],\n",
      "        [0.0031, 0.4709],\n",
      "        [0.0031, 0.4729],\n",
      "        [0.0031, 0.4749],\n",
      "        [0.0031, 0.4770],\n",
      "        [0.0031, 0.4790],\n",
      "        [0.0031, 0.4810],\n",
      "        [0.0031, 0.4830],\n",
      "        [0.0031, 0.4850],\n",
      "        [0.0031, 0.4870],\n",
      "        [0.0031, 0.4890],\n",
      "        [0.0031, 0.4910],\n",
      "        [0.0031, 0.4930],\n",
      "        [0.0031, 0.4950],\n",
      "        [0.0031, 0.4970],\n",
      "        [0.0031, 0.4990],\n",
      "        [0.0031, 0.5010],\n",
      "        [0.0031, 0.5030],\n",
      "        [0.0031, 0.5050],\n",
      "        [0.0031, 0.5070],\n",
      "        [0.0031, 0.5090],\n",
      "        [0.0031, 0.5110],\n",
      "        [0.0031, 0.5130],\n",
      "        [0.0031, 0.5150],\n",
      "        [0.0031, 0.5170],\n",
      "        [0.0031, 0.5190],\n",
      "        [0.0031, 0.5210],\n",
      "        [0.0031, 0.5230],\n",
      "        [0.0031, 0.5251],\n",
      "        [0.0031, 0.5271],\n",
      "        [0.0031, 0.5291],\n",
      "        [0.0031, 0.5311],\n",
      "        [0.0031, 0.5331],\n",
      "        [0.0031, 0.5351],\n",
      "        [0.0031, 0.5371],\n",
      "        [0.0031, 0.5391],\n",
      "        [0.0031, 0.5411],\n",
      "        [0.0031, 0.5431],\n",
      "        [0.0031, 0.5451],\n",
      "        [0.0031, 0.5471],\n",
      "        [0.0031, 0.5491],\n",
      "        [0.0031, 0.5511],\n",
      "        [0.0031, 0.5531],\n",
      "        [0.0031, 0.5551],\n",
      "        [0.0031, 0.5571],\n",
      "        [0.0031, 0.5591],\n",
      "        [0.0031, 0.5611],\n",
      "        [0.0031, 0.5631],\n",
      "        [0.0031, 0.5651],\n",
      "        [0.0031, 0.5671],\n",
      "        [0.0031, 0.5691],\n",
      "        [0.0031, 0.5711],\n",
      "        [0.0031, 0.5731],\n",
      "        [0.0031, 0.5752],\n",
      "        [0.0031, 0.5772],\n",
      "        [0.0031, 0.5792],\n",
      "        [0.0031, 0.5812],\n",
      "        [0.0031, 0.5832],\n",
      "        [0.0031, 0.5852],\n",
      "        [0.0031, 0.5872],\n",
      "        [0.0031, 0.5892],\n",
      "        [0.0031, 0.5912],\n",
      "        [0.0031, 0.5932],\n",
      "        [0.0031, 0.5952],\n",
      "        [0.0031, 0.5972],\n",
      "        [0.0031, 0.5992],\n",
      "        [0.0031, 0.6012],\n",
      "        [0.0031, 0.6032],\n",
      "        [0.0031, 0.6052],\n",
      "        [0.0031, 0.6072],\n",
      "        [0.0031, 0.6092],\n",
      "        [0.0031, 0.6112],\n",
      "        [0.0031, 0.6132],\n",
      "        [0.0031, 0.6152],\n",
      "        [0.0031, 0.6172],\n",
      "        [0.0031, 0.6192],\n",
      "        [0.0031, 0.6212],\n",
      "        [0.0031, 0.6232],\n",
      "        [0.0031, 0.6253],\n",
      "        [0.0031, 0.6273],\n",
      "        [0.0031, 0.6293],\n",
      "        [0.0031, 0.6313],\n",
      "        [0.0031, 0.6333],\n",
      "        [0.0031, 0.6353],\n",
      "        [0.0031, 0.6373],\n",
      "        [0.0031, 0.6393],\n",
      "        [0.0031, 0.6413],\n",
      "        [0.0031, 0.6433],\n",
      "        [0.0031, 0.6453],\n",
      "        [0.0031, 0.6473],\n",
      "        [0.0031, 0.6493],\n",
      "        [0.0031, 0.6513],\n",
      "        [0.0031, 0.6533],\n",
      "        [0.0031, 0.6553],\n",
      "        [0.0031, 0.6573],\n",
      "        [0.0031, 0.6593],\n",
      "        [0.0031, 0.6613],\n",
      "        [0.0031, 0.6633],\n",
      "        [0.0031, 0.6653],\n",
      "        [0.0031, 0.6673],\n",
      "        [0.0031, 0.6693],\n",
      "        [0.0031, 0.6713],\n",
      "        [0.0031, 0.6733],\n",
      "        [0.0031, 0.6754],\n",
      "        [0.0031, 0.6774],\n",
      "        [0.0031, 0.6794],\n",
      "        [0.0031, 0.6814],\n",
      "        [0.0031, 0.6834],\n",
      "        [0.0031, 0.6854],\n",
      "        [0.0031, 0.6874],\n",
      "        [0.0031, 0.6894],\n",
      "        [0.0031, 0.6914],\n",
      "        [0.0031, 0.6934],\n",
      "        [0.0031, 0.6954],\n",
      "        [0.0031, 0.6974],\n",
      "        [0.0031, 0.6994],\n",
      "        [0.0031, 0.7014],\n",
      "        [0.0031, 0.7034],\n",
      "        [0.0031, 0.7054],\n",
      "        [0.0031, 0.7074],\n",
      "        [0.0031, 0.7094],\n",
      "        [0.0031, 0.7114],\n",
      "        [0.0031, 0.7134],\n",
      "        [0.0031, 0.7154],\n",
      "        [0.0031, 0.7174],\n",
      "        [0.0031, 0.7194],\n",
      "        [0.0031, 0.7214],\n",
      "        [0.0031, 0.7234],\n",
      "        [0.0031, 0.7255],\n",
      "        [0.0031, 0.7275],\n",
      "        [0.0031, 0.7295],\n",
      "        [0.0031, 0.7315],\n",
      "        [0.0031, 0.7335],\n",
      "        [0.0031, 0.7355],\n",
      "        [0.0031, 0.7375],\n",
      "        [0.0031, 0.7395],\n",
      "        [0.0031, 0.7415],\n",
      "        [0.0031, 0.7435],\n",
      "        [0.0031, 0.7455],\n",
      "        [0.0031, 0.7475],\n",
      "        [0.0031, 0.7495],\n",
      "        [0.0031, 0.7515],\n",
      "        [0.0031, 0.7535],\n",
      "        [0.0031, 0.7555],\n",
      "        [0.0031, 0.7575],\n",
      "        [0.0031, 0.7595],\n",
      "        [0.0031, 0.7615],\n",
      "        [0.0031, 0.7635],\n",
      "        [0.0031, 0.7655],\n",
      "        [0.0031, 0.7675],\n",
      "        [0.0031, 0.7695],\n",
      "        [0.0031, 0.7715],\n",
      "        [0.0031, 0.7735],\n",
      "        [0.0031, 0.7756],\n",
      "        [0.0031, 0.7776],\n",
      "        [0.0031, 0.7796],\n",
      "        [0.0031, 0.7816],\n",
      "        [0.0031, 0.7836],\n",
      "        [0.0031, 0.7856],\n",
      "        [0.0031, 0.7876],\n",
      "        [0.0031, 0.7896],\n",
      "        [0.0031, 0.7916],\n",
      "        [0.0031, 0.7936],\n",
      "        [0.0031, 0.7956],\n",
      "        [0.0031, 0.7976],\n",
      "        [0.0031, 0.7996],\n",
      "        [0.0031, 0.8016],\n",
      "        [0.0031, 0.8036],\n",
      "        [0.0031, 0.8056],\n",
      "        [0.0031, 0.8076],\n",
      "        [0.0031, 0.8096],\n",
      "        [0.0031, 0.8116],\n",
      "        [0.0031, 0.8136],\n",
      "        [0.0031, 0.8156],\n",
      "        [0.0031, 0.8176],\n",
      "        [0.0031, 0.8196],\n",
      "        [0.0031, 0.8216],\n",
      "        [0.0031, 0.8236],\n",
      "        [0.0031, 0.8257],\n",
      "        [0.0031, 0.8277],\n",
      "        [0.0031, 0.8297],\n",
      "        [0.0031, 0.8317],\n",
      "        [0.0031, 0.8337],\n",
      "        [0.0031, 0.8357],\n",
      "        [0.0031, 0.8377],\n",
      "        [0.0031, 0.8397],\n",
      "        [0.0031, 0.8417],\n",
      "        [0.0031, 0.8437],\n",
      "        [0.0031, 0.8457],\n",
      "        [0.0031, 0.8477],\n",
      "        [0.0031, 0.8497],\n",
      "        [0.0031, 0.8517],\n",
      "        [0.0031, 0.8537],\n",
      "        [0.0031, 0.8557],\n",
      "        [0.0031, 0.8577],\n",
      "        [0.0031, 0.8597],\n",
      "        [0.0031, 0.8617],\n",
      "        [0.0031, 0.8637],\n",
      "        [0.0031, 0.8657],\n",
      "        [0.0031, 0.8677],\n",
      "        [0.0031, 0.8697],\n",
      "        [0.0031, 0.8717],\n",
      "        [0.0031, 0.8737],\n",
      "        [0.0031, 0.8758],\n",
      "        [0.0031, 0.8778],\n",
      "        [0.0031, 0.8798],\n",
      "        [0.0031, 0.8818],\n",
      "        [0.0031, 0.8838],\n",
      "        [0.0031, 0.8858],\n",
      "        [0.0031, 0.8878],\n",
      "        [0.0031, 0.8898],\n",
      "        [0.0031, 0.8918],\n",
      "        [0.0031, 0.8938],\n",
      "        [0.0031, 0.8958],\n",
      "        [0.0031, 0.8978],\n",
      "        [0.0031, 0.8998],\n",
      "        [0.0031, 0.9018],\n",
      "        [0.0031, 0.9038],\n",
      "        [0.0031, 0.9058],\n",
      "        [0.0031, 0.9078],\n",
      "        [0.0031, 0.9098],\n",
      "        [0.0031, 0.9118],\n",
      "        [0.0031, 0.9138],\n",
      "        [0.0031, 0.9158],\n",
      "        [0.0031, 0.9178],\n",
      "        [0.0031, 0.9198],\n",
      "        [0.0031, 0.9218],\n",
      "        [0.0031, 0.9238],\n",
      "        [0.0031, 0.9259],\n",
      "        [0.0031, 0.9279],\n",
      "        [0.0031, 0.9299],\n",
      "        [0.0031, 0.9319],\n",
      "        [0.0031, 0.9339],\n",
      "        [0.0031, 0.9359],\n",
      "        [0.0031, 0.9379],\n",
      "        [0.0031, 0.9399],\n",
      "        [0.0031, 0.9419],\n",
      "        [0.0031, 0.9439],\n",
      "        [0.0031, 0.9459],\n",
      "        [0.0031, 0.9479],\n",
      "        [0.0031, 0.9499],\n",
      "        [0.0031, 0.9519],\n",
      "        [0.0031, 0.9539],\n",
      "        [0.0031, 0.9559],\n",
      "        [0.0031, 0.9579],\n",
      "        [0.0031, 0.9599],\n",
      "        [0.0031, 0.9619],\n",
      "        [0.0031, 0.9639],\n",
      "        [0.0031, 0.9659],\n",
      "        [0.0031, 0.9679],\n",
      "        [0.0031, 0.9699],\n",
      "        [0.0031, 0.9719],\n",
      "        [0.0031, 0.9739],\n",
      "        [0.0031, 0.9760],\n",
      "        [0.0031, 0.9780],\n",
      "        [0.0031, 0.9800],\n",
      "        [0.0031, 0.9820],\n",
      "        [0.0031, 0.9840],\n",
      "        [0.0031, 0.9860],\n",
      "        [0.0031, 0.9880],\n",
      "        [0.0031, 0.9900],\n",
      "        [0.0031, 0.9920],\n",
      "        [0.0031, 0.9940],\n",
      "        [0.0031, 0.9960],\n",
      "        [0.0031, 0.9980],\n",
      "        [0.0031, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.412862Z",
     "start_time": "2020-11-16T05:26:11.399480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0100, 0.0000],\n",
      "        [0.0100, 0.0020],\n",
      "        [0.0100, 0.0040],\n",
      "        [0.0100, 0.0060],\n",
      "        [0.0100, 0.0080],\n",
      "        [0.0100, 0.0100],\n",
      "        [0.0100, 0.0120],\n",
      "        [0.0100, 0.0140],\n",
      "        [0.0100, 0.0160],\n",
      "        [0.0100, 0.0180],\n",
      "        [0.0100, 0.0200],\n",
      "        [0.0100, 0.0220],\n",
      "        [0.0100, 0.0240],\n",
      "        [0.0100, 0.0261],\n",
      "        [0.0100, 0.0281],\n",
      "        [0.0100, 0.0301],\n",
      "        [0.0100, 0.0321],\n",
      "        [0.0100, 0.0341],\n",
      "        [0.0100, 0.0361],\n",
      "        [0.0100, 0.0381],\n",
      "        [0.0100, 0.0401],\n",
      "        [0.0100, 0.0421],\n",
      "        [0.0100, 0.0441],\n",
      "        [0.0100, 0.0461],\n",
      "        [0.0100, 0.0481],\n",
      "        [0.0100, 0.0501],\n",
      "        [0.0100, 0.0521],\n",
      "        [0.0100, 0.0541],\n",
      "        [0.0100, 0.0561],\n",
      "        [0.0100, 0.0581],\n",
      "        [0.0100, 0.0601],\n",
      "        [0.0100, 0.0621],\n",
      "        [0.0100, 0.0641],\n",
      "        [0.0100, 0.0661],\n",
      "        [0.0100, 0.0681],\n",
      "        [0.0100, 0.0701],\n",
      "        [0.0100, 0.0721],\n",
      "        [0.0100, 0.0741],\n",
      "        [0.0100, 0.0762],\n",
      "        [0.0100, 0.0782],\n",
      "        [0.0100, 0.0802],\n",
      "        [0.0100, 0.0822],\n",
      "        [0.0100, 0.0842],\n",
      "        [0.0100, 0.0862],\n",
      "        [0.0100, 0.0882],\n",
      "        [0.0100, 0.0902],\n",
      "        [0.0100, 0.0922],\n",
      "        [0.0100, 0.0942],\n",
      "        [0.0100, 0.0962],\n",
      "        [0.0100, 0.0982],\n",
      "        [0.0100, 0.1002],\n",
      "        [0.0100, 0.1022],\n",
      "        [0.0100, 0.1042],\n",
      "        [0.0100, 0.1062],\n",
      "        [0.0100, 0.1082],\n",
      "        [0.0100, 0.1102],\n",
      "        [0.0100, 0.1122],\n",
      "        [0.0100, 0.1142],\n",
      "        [0.0100, 0.1162],\n",
      "        [0.0100, 0.1182],\n",
      "        [0.0100, 0.1202],\n",
      "        [0.0100, 0.1222],\n",
      "        [0.0100, 0.1242],\n",
      "        [0.0100, 0.1263],\n",
      "        [0.0100, 0.1283],\n",
      "        [0.0100, 0.1303],\n",
      "        [0.0100, 0.1323],\n",
      "        [0.0100, 0.1343],\n",
      "        [0.0100, 0.1363],\n",
      "        [0.0100, 0.1383],\n",
      "        [0.0100, 0.1403],\n",
      "        [0.0100, 0.1423],\n",
      "        [0.0100, 0.1443],\n",
      "        [0.0100, 0.1463],\n",
      "        [0.0100, 0.1483],\n",
      "        [0.0100, 0.1503],\n",
      "        [0.0100, 0.1523],\n",
      "        [0.0100, 0.1543],\n",
      "        [0.0100, 0.1563],\n",
      "        [0.0100, 0.1583],\n",
      "        [0.0100, 0.1603],\n",
      "        [0.0100, 0.1623],\n",
      "        [0.0100, 0.1643],\n",
      "        [0.0100, 0.1663],\n",
      "        [0.0100, 0.1683],\n",
      "        [0.0100, 0.1703],\n",
      "        [0.0100, 0.1723],\n",
      "        [0.0100, 0.1743],\n",
      "        [0.0100, 0.1764],\n",
      "        [0.0100, 0.1784],\n",
      "        [0.0100, 0.1804],\n",
      "        [0.0100, 0.1824],\n",
      "        [0.0100, 0.1844],\n",
      "        [0.0100, 0.1864],\n",
      "        [0.0100, 0.1884],\n",
      "        [0.0100, 0.1904],\n",
      "        [0.0100, 0.1924],\n",
      "        [0.0100, 0.1944],\n",
      "        [0.0100, 0.1964],\n",
      "        [0.0100, 0.1984],\n",
      "        [0.0100, 0.2004],\n",
      "        [0.0100, 0.2024],\n",
      "        [0.0100, 0.2044],\n",
      "        [0.0100, 0.2064],\n",
      "        [0.0100, 0.2084],\n",
      "        [0.0100, 0.2104],\n",
      "        [0.0100, 0.2124],\n",
      "        [0.0100, 0.2144],\n",
      "        [0.0100, 0.2164],\n",
      "        [0.0100, 0.2184],\n",
      "        [0.0100, 0.2204],\n",
      "        [0.0100, 0.2224],\n",
      "        [0.0100, 0.2244],\n",
      "        [0.0100, 0.2265],\n",
      "        [0.0100, 0.2285],\n",
      "        [0.0100, 0.2305],\n",
      "        [0.0100, 0.2325],\n",
      "        [0.0100, 0.2345],\n",
      "        [0.0100, 0.2365],\n",
      "        [0.0100, 0.2385],\n",
      "        [0.0100, 0.2405],\n",
      "        [0.0100, 0.2425],\n",
      "        [0.0100, 0.2445],\n",
      "        [0.0100, 0.2465],\n",
      "        [0.0100, 0.2485],\n",
      "        [0.0100, 0.2505],\n",
      "        [0.0100, 0.2525],\n",
      "        [0.0100, 0.2545],\n",
      "        [0.0100, 0.2565],\n",
      "        [0.0100, 0.2585],\n",
      "        [0.0100, 0.2605],\n",
      "        [0.0100, 0.2625],\n",
      "        [0.0100, 0.2645],\n",
      "        [0.0100, 0.2665],\n",
      "        [0.0100, 0.2685],\n",
      "        [0.0100, 0.2705],\n",
      "        [0.0100, 0.2725],\n",
      "        [0.0100, 0.2745],\n",
      "        [0.0100, 0.2766],\n",
      "        [0.0100, 0.2786],\n",
      "        [0.0100, 0.2806],\n",
      "        [0.0100, 0.2826],\n",
      "        [0.0100, 0.2846],\n",
      "        [0.0100, 0.2866],\n",
      "        [0.0100, 0.2886],\n",
      "        [0.0100, 0.2906],\n",
      "        [0.0100, 0.2926],\n",
      "        [0.0100, 0.2946],\n",
      "        [0.0100, 0.2966],\n",
      "        [0.0100, 0.2986],\n",
      "        [0.0100, 0.3006],\n",
      "        [0.0100, 0.3026],\n",
      "        [0.0100, 0.3046],\n",
      "        [0.0100, 0.3066],\n",
      "        [0.0100, 0.3086],\n",
      "        [0.0100, 0.3106],\n",
      "        [0.0100, 0.3126],\n",
      "        [0.0100, 0.3146],\n",
      "        [0.0100, 0.3166],\n",
      "        [0.0100, 0.3186],\n",
      "        [0.0100, 0.3206],\n",
      "        [0.0100, 0.3226],\n",
      "        [0.0100, 0.3246],\n",
      "        [0.0100, 0.3267],\n",
      "        [0.0100, 0.3287],\n",
      "        [0.0100, 0.3307],\n",
      "        [0.0100, 0.3327],\n",
      "        [0.0100, 0.3347],\n",
      "        [0.0100, 0.3367],\n",
      "        [0.0100, 0.3387],\n",
      "        [0.0100, 0.3407],\n",
      "        [0.0100, 0.3427],\n",
      "        [0.0100, 0.3447],\n",
      "        [0.0100, 0.3467],\n",
      "        [0.0100, 0.3487],\n",
      "        [0.0100, 0.3507],\n",
      "        [0.0100, 0.3527],\n",
      "        [0.0100, 0.3547],\n",
      "        [0.0100, 0.3567],\n",
      "        [0.0100, 0.3587],\n",
      "        [0.0100, 0.3607],\n",
      "        [0.0100, 0.3627],\n",
      "        [0.0100, 0.3647],\n",
      "        [0.0100, 0.3667],\n",
      "        [0.0100, 0.3687],\n",
      "        [0.0100, 0.3707],\n",
      "        [0.0100, 0.3727],\n",
      "        [0.0100, 0.3747],\n",
      "        [0.0100, 0.3768],\n",
      "        [0.0100, 0.3788],\n",
      "        [0.0100, 0.3808],\n",
      "        [0.0100, 0.3828],\n",
      "        [0.0100, 0.3848],\n",
      "        [0.0100, 0.3868],\n",
      "        [0.0100, 0.3888],\n",
      "        [0.0100, 0.3908],\n",
      "        [0.0100, 0.3928],\n",
      "        [0.0100, 0.3948],\n",
      "        [0.0100, 0.3968],\n",
      "        [0.0100, 0.3988],\n",
      "        [0.0100, 0.4008],\n",
      "        [0.0100, 0.4028],\n",
      "        [0.0100, 0.4048],\n",
      "        [0.0100, 0.4068],\n",
      "        [0.0100, 0.4088],\n",
      "        [0.0100, 0.4108],\n",
      "        [0.0100, 0.4128],\n",
      "        [0.0100, 0.4148],\n",
      "        [0.0100, 0.4168],\n",
      "        [0.0100, 0.4188],\n",
      "        [0.0100, 0.4208],\n",
      "        [0.0100, 0.4228],\n",
      "        [0.0100, 0.4248],\n",
      "        [0.0100, 0.4269],\n",
      "        [0.0100, 0.4289],\n",
      "        [0.0100, 0.4309],\n",
      "        [0.0100, 0.4329],\n",
      "        [0.0100, 0.4349],\n",
      "        [0.0100, 0.4369],\n",
      "        [0.0100, 0.4389],\n",
      "        [0.0100, 0.4409],\n",
      "        [0.0100, 0.4429],\n",
      "        [0.0100, 0.4449],\n",
      "        [0.0100, 0.4469],\n",
      "        [0.0100, 0.4489],\n",
      "        [0.0100, 0.4509],\n",
      "        [0.0100, 0.4529],\n",
      "        [0.0100, 0.4549],\n",
      "        [0.0100, 0.4569],\n",
      "        [0.0100, 0.4589],\n",
      "        [0.0100, 0.4609],\n",
      "        [0.0100, 0.4629],\n",
      "        [0.0100, 0.4649],\n",
      "        [0.0100, 0.4669],\n",
      "        [0.0100, 0.4689],\n",
      "        [0.0100, 0.4709],\n",
      "        [0.0100, 0.4729],\n",
      "        [0.0100, 0.4749],\n",
      "        [0.0100, 0.4770],\n",
      "        [0.0100, 0.4790],\n",
      "        [0.0100, 0.4810],\n",
      "        [0.0100, 0.4830],\n",
      "        [0.0100, 0.4850],\n",
      "        [0.0100, 0.4870],\n",
      "        [0.0100, 0.4890],\n",
      "        [0.0100, 0.4910],\n",
      "        [0.0100, 0.4930],\n",
      "        [0.0100, 0.4950],\n",
      "        [0.0100, 0.4970],\n",
      "        [0.0100, 0.4990],\n",
      "        [0.0100, 0.5010],\n",
      "        [0.0100, 0.5030],\n",
      "        [0.0100, 0.5050],\n",
      "        [0.0100, 0.5070],\n",
      "        [0.0100, 0.5090],\n",
      "        [0.0100, 0.5110],\n",
      "        [0.0100, 0.5130],\n",
      "        [0.0100, 0.5150],\n",
      "        [0.0100, 0.5170],\n",
      "        [0.0100, 0.5190],\n",
      "        [0.0100, 0.5210],\n",
      "        [0.0100, 0.5230],\n",
      "        [0.0100, 0.5251],\n",
      "        [0.0100, 0.5271],\n",
      "        [0.0100, 0.5291],\n",
      "        [0.0100, 0.5311],\n",
      "        [0.0100, 0.5331],\n",
      "        [0.0100, 0.5351],\n",
      "        [0.0100, 0.5371],\n",
      "        [0.0100, 0.5391],\n",
      "        [0.0100, 0.5411],\n",
      "        [0.0100, 0.5431],\n",
      "        [0.0100, 0.5451],\n",
      "        [0.0100, 0.5471],\n",
      "        [0.0100, 0.5491],\n",
      "        [0.0100, 0.5511],\n",
      "        [0.0100, 0.5531],\n",
      "        [0.0100, 0.5551],\n",
      "        [0.0100, 0.5571],\n",
      "        [0.0100, 0.5591],\n",
      "        [0.0100, 0.5611],\n",
      "        [0.0100, 0.5631],\n",
      "        [0.0100, 0.5651],\n",
      "        [0.0100, 0.5671],\n",
      "        [0.0100, 0.5691],\n",
      "        [0.0100, 0.5711],\n",
      "        [0.0100, 0.5731],\n",
      "        [0.0100, 0.5752],\n",
      "        [0.0100, 0.5772],\n",
      "        [0.0100, 0.5792],\n",
      "        [0.0100, 0.5812],\n",
      "        [0.0100, 0.5832],\n",
      "        [0.0100, 0.5852],\n",
      "        [0.0100, 0.5872],\n",
      "        [0.0100, 0.5892],\n",
      "        [0.0100, 0.5912],\n",
      "        [0.0100, 0.5932],\n",
      "        [0.0100, 0.5952],\n",
      "        [0.0100, 0.5972],\n",
      "        [0.0100, 0.5992],\n",
      "        [0.0100, 0.6012],\n",
      "        [0.0100, 0.6032],\n",
      "        [0.0100, 0.6052],\n",
      "        [0.0100, 0.6072],\n",
      "        [0.0100, 0.6092],\n",
      "        [0.0100, 0.6112],\n",
      "        [0.0100, 0.6132],\n",
      "        [0.0100, 0.6152],\n",
      "        [0.0100, 0.6172],\n",
      "        [0.0100, 0.6192],\n",
      "        [0.0100, 0.6212],\n",
      "        [0.0100, 0.6232],\n",
      "        [0.0100, 0.6253],\n",
      "        [0.0100, 0.6273],\n",
      "        [0.0100, 0.6293],\n",
      "        [0.0100, 0.6313],\n",
      "        [0.0100, 0.6333],\n",
      "        [0.0100, 0.6353],\n",
      "        [0.0100, 0.6373],\n",
      "        [0.0100, 0.6393],\n",
      "        [0.0100, 0.6413],\n",
      "        [0.0100, 0.6433],\n",
      "        [0.0100, 0.6453],\n",
      "        [0.0100, 0.6473],\n",
      "        [0.0100, 0.6493],\n",
      "        [0.0100, 0.6513],\n",
      "        [0.0100, 0.6533],\n",
      "        [0.0100, 0.6553],\n",
      "        [0.0100, 0.6573],\n",
      "        [0.0100, 0.6593],\n",
      "        [0.0100, 0.6613],\n",
      "        [0.0100, 0.6633],\n",
      "        [0.0100, 0.6653],\n",
      "        [0.0100, 0.6673],\n",
      "        [0.0100, 0.6693],\n",
      "        [0.0100, 0.6713],\n",
      "        [0.0100, 0.6733],\n",
      "        [0.0100, 0.6754],\n",
      "        [0.0100, 0.6774],\n",
      "        [0.0100, 0.6794],\n",
      "        [0.0100, 0.6814],\n",
      "        [0.0100, 0.6834],\n",
      "        [0.0100, 0.6854],\n",
      "        [0.0100, 0.6874],\n",
      "        [0.0100, 0.6894],\n",
      "        [0.0100, 0.6914],\n",
      "        [0.0100, 0.6934],\n",
      "        [0.0100, 0.6954],\n",
      "        [0.0100, 0.6974],\n",
      "        [0.0100, 0.6994],\n",
      "        [0.0100, 0.7014],\n",
      "        [0.0100, 0.7034],\n",
      "        [0.0100, 0.7054],\n",
      "        [0.0100, 0.7074],\n",
      "        [0.0100, 0.7094],\n",
      "        [0.0100, 0.7114],\n",
      "        [0.0100, 0.7134],\n",
      "        [0.0100, 0.7154],\n",
      "        [0.0100, 0.7174],\n",
      "        [0.0100, 0.7194],\n",
      "        [0.0100, 0.7214],\n",
      "        [0.0100, 0.7234],\n",
      "        [0.0100, 0.7255],\n",
      "        [0.0100, 0.7275],\n",
      "        [0.0100, 0.7295],\n",
      "        [0.0100, 0.7315],\n",
      "        [0.0100, 0.7335],\n",
      "        [0.0100, 0.7355],\n",
      "        [0.0100, 0.7375],\n",
      "        [0.0100, 0.7395],\n",
      "        [0.0100, 0.7415],\n",
      "        [0.0100, 0.7435],\n",
      "        [0.0100, 0.7455],\n",
      "        [0.0100, 0.7475],\n",
      "        [0.0100, 0.7495],\n",
      "        [0.0100, 0.7515],\n",
      "        [0.0100, 0.7535],\n",
      "        [0.0100, 0.7555],\n",
      "        [0.0100, 0.7575],\n",
      "        [0.0100, 0.7595],\n",
      "        [0.0100, 0.7615],\n",
      "        [0.0100, 0.7635],\n",
      "        [0.0100, 0.7655],\n",
      "        [0.0100, 0.7675],\n",
      "        [0.0100, 0.7695],\n",
      "        [0.0100, 0.7715],\n",
      "        [0.0100, 0.7735],\n",
      "        [0.0100, 0.7756],\n",
      "        [0.0100, 0.7776],\n",
      "        [0.0100, 0.7796],\n",
      "        [0.0100, 0.7816],\n",
      "        [0.0100, 0.7836],\n",
      "        [0.0100, 0.7856],\n",
      "        [0.0100, 0.7876],\n",
      "        [0.0100, 0.7896],\n",
      "        [0.0100, 0.7916],\n",
      "        [0.0100, 0.7936],\n",
      "        [0.0100, 0.7956],\n",
      "        [0.0100, 0.7976],\n",
      "        [0.0100, 0.7996],\n",
      "        [0.0100, 0.8016],\n",
      "        [0.0100, 0.8036],\n",
      "        [0.0100, 0.8056],\n",
      "        [0.0100, 0.8076],\n",
      "        [0.0100, 0.8096],\n",
      "        [0.0100, 0.8116],\n",
      "        [0.0100, 0.8136],\n",
      "        [0.0100, 0.8156],\n",
      "        [0.0100, 0.8176],\n",
      "        [0.0100, 0.8196],\n",
      "        [0.0100, 0.8216],\n",
      "        [0.0100, 0.8236],\n",
      "        [0.0100, 0.8257],\n",
      "        [0.0100, 0.8277],\n",
      "        [0.0100, 0.8297],\n",
      "        [0.0100, 0.8317],\n",
      "        [0.0100, 0.8337],\n",
      "        [0.0100, 0.8357],\n",
      "        [0.0100, 0.8377],\n",
      "        [0.0100, 0.8397],\n",
      "        [0.0100, 0.8417],\n",
      "        [0.0100, 0.8437],\n",
      "        [0.0100, 0.8457],\n",
      "        [0.0100, 0.8477],\n",
      "        [0.0100, 0.8497],\n",
      "        [0.0100, 0.8517],\n",
      "        [0.0100, 0.8537],\n",
      "        [0.0100, 0.8557],\n",
      "        [0.0100, 0.8577],\n",
      "        [0.0100, 0.8597],\n",
      "        [0.0100, 0.8617],\n",
      "        [0.0100, 0.8637],\n",
      "        [0.0100, 0.8657],\n",
      "        [0.0100, 0.8677],\n",
      "        [0.0100, 0.8697],\n",
      "        [0.0100, 0.8717],\n",
      "        [0.0100, 0.8737],\n",
      "        [0.0100, 0.8758],\n",
      "        [0.0100, 0.8778],\n",
      "        [0.0100, 0.8798],\n",
      "        [0.0100, 0.8818],\n",
      "        [0.0100, 0.8838],\n",
      "        [0.0100, 0.8858],\n",
      "        [0.0100, 0.8878],\n",
      "        [0.0100, 0.8898],\n",
      "        [0.0100, 0.8918],\n",
      "        [0.0100, 0.8938],\n",
      "        [0.0100, 0.8958],\n",
      "        [0.0100, 0.8978],\n",
      "        [0.0100, 0.8998],\n",
      "        [0.0100, 0.9018],\n",
      "        [0.0100, 0.9038],\n",
      "        [0.0100, 0.9058],\n",
      "        [0.0100, 0.9078],\n",
      "        [0.0100, 0.9098],\n",
      "        [0.0100, 0.9118],\n",
      "        [0.0100, 0.9138],\n",
      "        [0.0100, 0.9158],\n",
      "        [0.0100, 0.9178],\n",
      "        [0.0100, 0.9198],\n",
      "        [0.0100, 0.9218],\n",
      "        [0.0100, 0.9238],\n",
      "        [0.0100, 0.9259],\n",
      "        [0.0100, 0.9279],\n",
      "        [0.0100, 0.9299],\n",
      "        [0.0100, 0.9319],\n",
      "        [0.0100, 0.9339],\n",
      "        [0.0100, 0.9359],\n",
      "        [0.0100, 0.9379],\n",
      "        [0.0100, 0.9399],\n",
      "        [0.0100, 0.9419],\n",
      "        [0.0100, 0.9439],\n",
      "        [0.0100, 0.9459],\n",
      "        [0.0100, 0.9479],\n",
      "        [0.0100, 0.9499],\n",
      "        [0.0100, 0.9519],\n",
      "        [0.0100, 0.9539],\n",
      "        [0.0100, 0.9559],\n",
      "        [0.0100, 0.9579],\n",
      "        [0.0100, 0.9599],\n",
      "        [0.0100, 0.9619],\n",
      "        [0.0100, 0.9639],\n",
      "        [0.0100, 0.9659],\n",
      "        [0.0100, 0.9679],\n",
      "        [0.0100, 0.9699],\n",
      "        [0.0100, 0.9719],\n",
      "        [0.0100, 0.9739],\n",
      "        [0.0100, 0.9760],\n",
      "        [0.0100, 0.9780],\n",
      "        [0.0100, 0.9800],\n",
      "        [0.0100, 0.9820],\n",
      "        [0.0100, 0.9840],\n",
      "        [0.0100, 0.9860],\n",
      "        [0.0100, 0.9880],\n",
      "        [0.0100, 0.9900],\n",
      "        [0.0100, 0.9920],\n",
      "        [0.0100, 0.9940],\n",
      "        [0.0100, 0.9960],\n",
      "        [0.0100, 0.9980],\n",
      "        [0.0100, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(test_data[-1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate how to preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.416661Z",
     "start_time": "2020-11-16T05:26:11.414025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.427508Z",
     "start_time": "2020-11-16T05:26:11.417717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 66)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = []\n",
    "for d in train_data:\n",
    "    D.append(np.hstack(d))\n",
    "D = np.vstack(D)\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.431496Z",
     "start_time": "2020-11-16T05:26:11.428658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-19.950754"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.571093Z",
     "start_time": "2020-11-16T05:26:11.432571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_mx = np.max(D,axis=0)\n",
    "x_mn = np.min(D,axis=0)\n",
    "abs_x_mx = np.max(abs(D),axis=0)\n",
    "plt.plot(x_mx)\n",
    "plt.plot(x_mn)\n",
    "plt.plot(abs_x_mx)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.579977Z",
     "start_time": "2020-11-16T05:26:11.572424Z"
    }
   },
   "outputs": [],
   "source": [
    "class LatentVectors(Dataset):\n",
    "    def __init__(self, data,doPreprocess=False,w=1,simLen=200,abs_x_mx=abs_x_mx):\n",
    "        self.data = data\n",
    "        self.doPreprocess = doPreprocess\n",
    "        self.simLen = simLen\n",
    "        self.w = w\n",
    "        self.abs_x_mx = abs_x_mx[:-2]\n",
    "        self.abs_p_mx = abs_x_mx[-2:]\n",
    "                 \n",
    "    def __len__(self):\n",
    "        return self.simLen*len(self.data)\n",
    "\n",
    "    def preprocess_x(self,x):\n",
    "        xnew = x/self.abs_x_mx\n",
    "        return xnew\n",
    "\n",
    "    def preprocess_p(self,p):\n",
    "        pnew = p/self.abs_p_mx\n",
    "        return pnew\n",
    "\n",
    "    def invPreprocess_x(self,xnew):\n",
    "        x = xnew*self.abs_x_mx\n",
    "        return x\n",
    "\n",
    "    def invPreprocess_x(self,pnew):\n",
    "        p = pnew*self.abs_p_mx\n",
    "        return p\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        q,r = np.divmod(idx,self.simLen)\n",
    "        X, p = self.data[q]\n",
    "        r_idx = np.random.randint(0,self.simLen-self.w)\n",
    "        \n",
    "        x = X[r_idx:r_idx+1]\n",
    "        y = X[r_idx+1:r_idx+self.w+1]\n",
    "        \n",
    "        p_x = p[r_idx:r_idx+1]\n",
    "        p_y = p[r_idx+1:r_idx+self.w+1]\n",
    "        \n",
    "        if self.doPreprocess:\n",
    "            x = self.preprocess_x(x)\n",
    "            y = self.preprocess_x(y)\n",
    "            p_x = self.preprocess_p(p_x)\n",
    "            p_y = self.preprocess_p(p_y)\n",
    "        \n",
    "        return x, y, p_x, p_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.660101Z",
     "start_time": "2020-11-16T05:26:11.581322Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDataset = LatentVectors(train_data,doPreprocess=True,w=w,simLen=simLen,abs_x_mx=abs_x_mx)\n",
    "testDataset = LatentVectors(test_data,doPreprocess=True,w=w,simLen=simLen,abs_x_mx=abs_x_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.666256Z",
     "start_time": "2020-11-16T05:26:11.661937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]),\n",
       " torch.Size([10, 64]),\n",
       " torch.Size([1, 2]),\n",
       " torch.Size([10, 2]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y,p_x,p_y = trainDataset[-2000]\n",
    "X.shape,y.shape,p_x.shape,p_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.671542Z",
     "start_time": "2020-11-16T05:26:11.667507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.6092],\n",
       "        [1.0000, 0.6112],\n",
       "        [1.0000, 0.6132],\n",
       "        [1.0000, 0.6152],\n",
       "        [1.0000, 0.6172],\n",
       "        [1.0000, 0.6192],\n",
       "        [1.0000, 0.6212],\n",
       "        [1.0000, 0.6232],\n",
       "        [1.0000, 0.6253],\n",
       "        [1.0000, 0.6273]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirming that normalization is working (-2000 grabs sims with largest inlet velocity)\n",
    "p_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.676644Z",
     "start_time": "2020-11-16T05:26:11.672728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8211), tensor(-0.8489))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.max(), y.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.681501Z",
     "start_time": "2020-11-16T05:26:11.677940Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 1250, 5000, 313)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataLoader = DataLoader(dataset=trainDataset, batch_size=bz, shuffle=True, drop_last=True)\n",
    "testDataLoader = DataLoader(dataset=testDataset, batch_size=bz)\n",
    "len(trainDataset), len(trainDataLoader), len(testDataset), len(testDataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.692134Z",
     "start_time": "2020-11-16T05:26:11.682692Z"
    }
   },
   "outputs": [],
   "source": [
    "class LatentVectors(Dataset):\n",
    "    def __init__(self, data,doPreprocess=False,w=1,simLen=200,abs_x_mx=abs_x_mx, memory=6):\n",
    "        self.data = data\n",
    "        self.doPreprocess = doPreprocess\n",
    "        self.simLen = simLen\n",
    "        self.w = w\n",
    "        self.abs_x_mx = abs_x_mx[:-2]\n",
    "        self.abs_p_mx = abs_x_mx[-2:]\n",
    "        self.memory= memory\n",
    "                 \n",
    "    def __len__(self):\n",
    "        return self.simLen*len(self.data)\n",
    "\n",
    "    def preprocess_x(self,x):\n",
    "        if x.shape[0] == self.abs_x_mx.shape[0]:\n",
    "            return x/self.abs_x_mx[:,None]\n",
    "        else:\n",
    "            return x/self.abs_x_mx\n",
    "\n",
    "    def preprocess_p(self,p):\n",
    "        if p.shape[0] == self.abs_p_mx.shape[0]:\n",
    "            return p/self.abs_p_mx[:,None]\n",
    "        else:\n",
    "            return p/self.abs_p_mx\n",
    "\n",
    "    def invPreprocess_x(self,xnew):\n",
    "        x = xnew*self.abs_x_mx\n",
    "        return x\n",
    "\n",
    "    def invPreprocess_x(self,pnew):\n",
    "        p = pnew*self.abs_p_mx\n",
    "        return p\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        q,r = np.divmod(idx,self.simLen)\n",
    "        X, p = self.data[q]\n",
    "        r_idx = np.random.randint(0,self.simLen-self.w)\n",
    "        \n",
    "        x = torch.zeros((latentDim, self.memory))\n",
    "        p_x = torch.zeros((2, self.memory))\n",
    "        \n",
    "        x[:,0] = X[r_idx:r_idx+1]\n",
    "        y = X[r_idx+1:r_idx+self.w+1]\n",
    "        \n",
    "        p_x[:,0] = p[r_idx:r_idx+1]\n",
    "        p_y = p[r_idx+1:r_idx+self.w+1]\n",
    "        \n",
    "        if self.doPreprocess:\n",
    "            x = self.preprocess_x(x)\n",
    "            y = self.preprocess_x(y)\n",
    "            p_x = self.preprocess_p(p_x)\n",
    "            p_y = self.preprocess_p(p_y)\n",
    "        \n",
    "        return x, y, p_x, p_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.698513Z",
     "start_time": "2020-11-16T05:26:11.693633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 6]),\n",
       " torch.Size([10, 64]),\n",
       " torch.Size([2, 6]),\n",
       " torch.Size([10, 2]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset = LatentVectors(train_data,doPreprocess=True,w=w,simLen=simLen,abs_x_mx=abs_x_mx)\n",
    "testDataset = LatentVectors(test_data,doPreprocess=True,w=w,simLen=simLen,abs_x_mx=abs_x_mx)\n",
    "X,y,p_x,p_y = trainDataset[-2000]\n",
    "X.shape,y.shape,p_x.shape,p_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.703828Z",
     "start_time": "2020-11-16T05:26:11.699992Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 1250, 5000, 313)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataLoader = DataLoader(dataset=trainDataset, batch_size=bz, shuffle=True, drop_last=True)\n",
    "testDataLoader = DataLoader(dataset=testDataset, batch_size=bz)\n",
    "len(trainDataset), len(trainDataLoader), len(testDataset), len(testDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.715725Z",
     "start_time": "2020-11-16T05:26:11.705079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 64, 6]),\n",
       " torch.Size([16, 10, 64]),\n",
       " torch.Size([16, 2, 6]),\n",
       " torch.Size([16, 10, 2]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y, p_x, p_y = next(iter(trainDataLoader))\n",
    "X = X.squeeze()\n",
    "p_x = p_x.squeeze()\n",
    "X.shape, y.shape, p_x.shape, p_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.720580Z",
     "start_time": "2020-11-16T05:26:11.716906Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8257, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5892, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.3106, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1062, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.2683, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4088, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.1353, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7455, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.2436, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3046, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.2959, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2866, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3547, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.1647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5591, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.6468, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4669, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.7477, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3086, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.1014, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0601, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.3106, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3427, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3166, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.2005, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4729, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.7110, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7615, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.1229, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5651, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.729907Z",
     "start_time": "2020-11-16T05:26:11.721677Z"
    }
   },
   "outputs": [],
   "source": [
    "class Autoregressive_Conv_Net(nn.Module):\n",
    "    def __init__(self, X, hiddenLayerSizes = [128, 128, 128], activation=nn.ELU(), memory = 6):\n",
    "        super(Autoregressive_Conv_Net,self).__init__()\n",
    "        \n",
    "        self.memory = memory\n",
    "        self.activation = activation\n",
    "        if hiddenLayerSizes[-1] != 64:\n",
    "            hiddenLayerSizes.append(64)\n",
    "        self.modules = []\n",
    "        for idx,sz in enumerate(hiddenLayerSizes):\n",
    "            if idx == 0:\n",
    "                in_channels = latentDim\n",
    "            else:\n",
    "                in_channels = hiddenLayerSizes[idx-1]\n",
    "            # flip the channel_in-length order?\n",
    "            self.modules.append(nn.Conv1d(in_channels, hiddenLayerSizes[idx], 3, padding=1)) \n",
    "            self.modules.append(self.activation)\n",
    "            \n",
    "        self.layers = nn.Sequential(*self.modules)\n",
    "\n",
    "        self.output = nn.Conv1d(memory, 1, 1, padding=0)\n",
    "        \n",
    "        #self.output = nn.Linear(hiddenLayerSizes[-1]*memory, latentDim)\n",
    "                                \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.layers(x)\n",
    "        if type(self.output) == nn.Conv1d:\n",
    "            x = self.output(x.transpose(-1,-2)).squeeze()\n",
    "        else:\n",
    "            x = self.output(x.view(x.size(0), -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.736866Z",
     "start_time": "2020-11-16T05:26:11.731376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoregressive_Conv_Net(\n",
       "  (activation): ELU(alpha=1.0)\n",
       "  (layers): Sequential(\n",
       "    (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (3): ELU(alpha=1.0)\n",
       "    (4): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (5): ELU(alpha=1.0)\n",
       "    (6): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (7): ELU(alpha=1.0)\n",
       "  )\n",
       "  (output): Conv1d(6, 1, kernel_size=(1,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Autoregressive_Conv_Net(X, hiddenLayerSizes=hiddenLayers,activation=activation)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.746821Z",
     "start_time": "2020-11-16T05:26:11.738299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(X.float())\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.750177Z",
     "start_time": "2020-11-16T05:26:11.748064Z"
    }
   },
   "outputs": [],
   "source": [
    "if len(gpu_ids.split(',')) > 1:\n",
    "    model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.754287Z",
     "start_time": "2020-11-16T05:26:11.751511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 layers require gradients (unfrozen) out of 10 layers\n",
      "147,911 parameters require gradients (unfrozen) out of 147,911 parameters\n"
     ]
    }
   ],
   "source": [
    "printNumModelParams(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.759333Z",
     "start_time": "2020-11-16T05:26:11.755510Z"
    }
   },
   "outputs": [],
   "source": [
    "del SVD_autoencoder\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.764849Z",
     "start_time": "2020-11-16T05:26:11.760632Z"
    }
   },
   "outputs": [],
   "source": [
    "SVD_autoencoder = SVD_Autoencoder(svd_vecs, latentDim, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.775635Z",
     "start_time": "2020-11-16T05:26:11.766176Z"
    }
   },
   "outputs": [],
   "source": [
    "# surrogate class\n",
    "\n",
    "class Surrogate(nn.Module):\n",
    "    \n",
    "    def __init__(self, window,\n",
    "                 z_size, p_size,\n",
    "                LIN, encoder, decoder):\n",
    "        super(Surrogate, self).__init__()\n",
    "        self.window = window\n",
    "        self.z_size = z_size # this does not include the size of p\n",
    "        self.p_size = p_size\n",
    "        self.c_size = z_size + p_size # this does include the size of p\n",
    "        self.LIN = LIN\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def encode(self, U):\n",
    "        \n",
    "        self.shape_of_last_frames_encoded = U.shape\n",
    "        \n",
    "        return self.encoder(U)\n",
    "        \n",
    "    def decode(self, encoding):\n",
    "        \n",
    "        self.shape_of_last_frames_encoded = torch.Size([encoding.size(0), 1, 128, 128])\n",
    "        \n",
    "        return self.decoder(encoding, self.shape_of_last_frames_encoded)\n",
    "        \n",
    "    def predict_next_w_encodings(self, encoding, p_y, window):\n",
    "        '''\n",
    "        use the LIN to predict the next w encodings for each \n",
    "        encoded U in the batch\n",
    "        '''\n",
    "            \n",
    "        predicted_encodings = []\n",
    "            \n",
    "        # given a batch of encodings, advance each encoding window time steps.\n",
    "        # save the result at each time step\n",
    "        for i in range(window):\n",
    "            memory_to_save = encoding[:,:,0:-1].clone()\n",
    "            encoding[:,:,0] = self.LIN(encoding.clone()) + encoding[:,:,0] # use LIN to predict delta in encoding\n",
    "            encoding[:,:,1:] = memory_to_save\n",
    "            encoding[:,-self.p_size:,0] = p_y[:, i]\n",
    "                    \n",
    "            predicted_encodings.append(encoding[:,:,0].clone())\n",
    "            \n",
    "            \n",
    "        return torch.stack(predicted_encodings)\n",
    "    \n",
    "    def forward(self, U, p_x, p_y, window = None):\n",
    "        \n",
    "        if window == None:\n",
    "            window = self.window\n",
    "        assert p_y.size(1) == window\n",
    "            \n",
    "        #encoding = self.encode(U)\n",
    "        encoding = U\n",
    "        encoding[:,-self.p_size:] = p_x\n",
    "        encoding_w = self.predict_next_w_encodings(encoding, p_y, window)\n",
    "        # want to have this agree with U_y, which is [batch_size, window_size, channels, nx, ny]\n",
    "        # right now, it's [window_size, batch_size, c_size], so transpose dimensions 0 and 1\n",
    "        # print(encoding_w.shape)\n",
    "        U = torch.stack([self.decode(encoding_i) for encoding_i in encoding_w])\n",
    "        \n",
    "        return U.transpose(0,1), encoding_w.transpose(0,1)\n",
    "    \n",
    "    \n",
    "surrogate = Surrogate(w, latentDim - 2, 2, model, SVD_autoencoder.encoder, \n",
    "                      SVD_autoencoder.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.780347Z",
     "start_time": "2020-11-16T05:26:11.777005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.818385Z",
     "start_time": "2020-11-16T05:26:11.781504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 10, 1, 128, 128]), torch.Size([16, 10, 64]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, encoding_hat = surrogate(X.float(), p_x.float(), p_y.float())\n",
    "U.shape, encoding_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.823019Z",
     "start_time": "2020-11-16T05:26:11.819502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(encoding_hat[:,:,-2:].detach().cpu(), p_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.829512Z",
     "start_time": "2020-11-16T05:26:11.824181Z"
    }
   },
   "outputs": [],
   "source": [
    "surrogate = Surrogate(w, latentDim - 2, 2, model, SVD_autoencoder.encoder, \n",
    "                      SVD_autoencoder.decoder).to(device)\n",
    "if len(gpu_ids.split(',')) > 1:\n",
    "    surrogate = nn.DataParallel(surrogate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.835249Z",
     "start_time": "2020-11-16T05:26:11.830598Z"
    }
   },
   "outputs": [],
   "source": [
    "def L2_relative_loss(pred, target):\n",
    "    return torch.norm(pred - target)/torch.norm(target)\n",
    "\n",
    "\n",
    "def L1_loss(pred, target):\n",
    "    return torch.mean(torch.abs(pred - target))\n",
    "\n",
    "\n",
    "def jacobian_loss(pred, target, device='cpu'):\n",
    "    return L1_loss(jacobian(pred, device), jacobian(target, device))\n",
    "\n",
    "\n",
    "def curl_loss(pred, target, device):\n",
    "    return L1_loss(curl(pred, device), curl(target, device))\n",
    "\n",
    "\n",
    "def MSE(pred,target):\n",
    "    return nn.MSELoss()(pred, target)\n",
    "\n",
    "\n",
    "def p_loss(pred, target):\n",
    "    return L(pred[:, -target.shape[1]:], target)\n",
    "\n",
    "\n",
    "def loss(pred, target, device):\n",
    "        \n",
    "    L = MSE(pred, target)\n",
    "        \n",
    "    return L "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.840136Z",
     "start_time": "2020-11-16T05:26:11.836436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 10, 64]), torch.Size([16, 10, 2]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:,:,-2:] = p_y\n",
    "y.shape, p_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.849780Z",
     "start_time": "2020-11-16T05:26:11.841214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1756, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(encoding_hat, y, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.853120Z",
     "start_time": "2020-11-16T05:26:11.850920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndoesn't work with our dataloader\\nif findLRs and (len(gpu_ids.split(','))==1): # doesn't work for multigpu???\\n    opt = create_opt(1e-7,model)\\n    find_lr(model,opt,L,device,trainDataLoader)\\n\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "doesn't work with our dataloader\n",
    "if findLRs and (len(gpu_ids.split(','))==1): # doesn't work for multigpu???\n",
    "    opt = create_opt(1e-7,model)\n",
    "    find_lr(model,opt,L,device,trainDataLoader)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.856776Z",
     "start_time": "2020-11-16T05:26:11.854173Z"
    }
   },
   "outputs": [],
   "source": [
    "max_lr = .005\n",
    "opt = torch.optim.Adam(model.parameters(),lr=max_lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.864453Z",
     "start_time": "2020-11-16T05:26:11.857926Z"
    }
   },
   "outputs": [],
   "source": [
    "versionName = versionName + '_lr{}'.format(str(max_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.868773Z",
     "start_time": "2020-11-16T05:26:11.865548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LIN_SVD_PNNL_GPUs1_w10_latentDim1024_hd128_128_128_bz16_epochs1000_lr0.005'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "versionName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.877983Z",
     "start_time": "2020-11-16T05:26:11.870073Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainEpoch(myDataLoader, tensorboard_writer, model, opt, p_loss, loss,\n",
    "               metric, lr_scheduler, tensorboard_rate, device,\n",
    "               tensorboard_recorder_step, total_steps):\n",
    "    running_loss = 0.0\n",
    "    running_rmse = 0.0\n",
    "    total_loss = 0.0\n",
    "    running_ploss = 0.0\n",
    "    for i, sampleBatch in enumerate(myDataLoader, start=1):\n",
    "\n",
    "        # --- Main Training ---\n",
    "        \n",
    "        # gpu\n",
    "        Encoding_x, Encoding_y, p_x, p_y = sampleBatch\n",
    "        Encoding_x = Encoding_x.to(device)\n",
    "        p_x = p_x.to(device)\n",
    "        Encoding_y = Encoding_y.to(device)\n",
    "        p_y = p_y.to(device)\n",
    "            \n",
    "\n",
    "        # zero the parameter gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        U_hat, encoding_hat = model(Encoding_x, p_x, p_y)\n",
    "        pl = 0\n",
    "        with torch.no_grad():\n",
    "            Encoding_y[:,:,-2:] = p_y \n",
    "        ll = loss(encoding_hat, Encoding_y, device)\n",
    "        combined_loss = pl + ll\n",
    "        combined_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # loss\n",
    "        batch_loss = combined_loss.item()\n",
    "        running_loss += batch_loss\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        batch_ploss = pl\n",
    "        running_ploss += batch_ploss\n",
    "\n",
    "\n",
    "\n",
    "        # record lr change\n",
    "        total_steps += 1\n",
    "        tensorboard_writer.add_scalar(tag=\"LR\", scalar_value=opt.param_groups[0]['lr'], global_step=total_steps)\n",
    "        #lr_scheduler.step()\n",
    "\n",
    "        # tensorboard writes\n",
    "        if (i % tensorboard_rate == 0):\n",
    "            tensorboard_recorder_step += 1\n",
    "            avg_running_loss = running_loss/tensorboard_rate\n",
    "            avg_running_rmse = running_rmse/tensorboard_rate\n",
    "            avg_running_ploss = running_ploss/tensorboard_rate\n",
    "            tensorboard_writer.add_scalar(tag=\"Loss\", scalar_value=avg_running_loss, global_step=tensorboard_recorder_step)\n",
    "            tensorboard_writer.add_scalar(tag=\"p_loss\", scalar_value=avg_running_ploss, global_step=tensorboard_recorder_step)\n",
    "            # reset running_loss for the next set of batches. (tensorboard_rate number of batches)\n",
    "            running_loss = 0.0\n",
    "            running_rmse = 0.0\n",
    "            running_ploss = 0.0\n",
    "            tensorboard_writer.flush()\n",
    "\n",
    "    return total_loss/len(myDataLoader), tensorboard_recorder_step, total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:11.885375Z",
     "start_time": "2020-11-16T05:26:11.879516Z"
    }
   },
   "outputs": [],
   "source": [
    "def validEpoch(myDataLoader, tensorboard_writer, model, p_loss, loss, metric,\n",
    "               device, tensorboard_recorder_step):\n",
    "    running_loss = 0.0\n",
    "    running_rmse = 0.0\n",
    "    for i, sampleBatch in enumerate(myDataLoader, start=1):\n",
    "\n",
    "        # --- Metrics Recording ---\n",
    "\n",
    "        # gpu\n",
    "        Encoding_x, Encoding_y, p_x, p_y = sampleBatch\n",
    "        Encoding_x = Encoding_x.to(device)\n",
    "        p_x = p_x.to(device)\n",
    "        Encoding_y = Encoding_y.to(device)\n",
    "        p_y = p_y.to(device)\n",
    "                \n",
    "        perc = len(Encoding_x)/len(myDataLoader.dataset)\n",
    "\n",
    "        # forward, no gradient calculations\n",
    "        with torch.no_grad():\n",
    "            U_hat, encoding_hat = model(Encoding_x, p_x, p_y)\n",
    "            \n",
    "        Encoding_y[:,:,-2:] = p_y \n",
    "\n",
    "        # loss\n",
    "        combined_loss = loss(encoding_hat, Encoding_y, device)\n",
    "        \n",
    "        running_loss += perc*(combined_loss.item())\n",
    "\n",
    "\n",
    "    avg_running_loss = running_loss\n",
    "    avg_running_rmse = running_rmse\n",
    "    tensorboard_writer.add_scalar(tag=\"Loss\", scalar_value=avg_running_loss, global_step=tensorboard_recorder_step)\n",
    "    tensorboard_writer.flush()\n",
    "    \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T05:26:14.194528Z",
     "start_time": "2020-11-16T05:26:11.886626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints directory already exists :)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(cps)\n",
    "except:\n",
    "    print(\"checkpoints directory already exists :)\")\n",
    "    \n",
    "# create a summary writer.\n",
    "train_writer = SummaryWriter(os.path.join(tensorboard_direc, versionName,'train'))\n",
    "test_writer = SummaryWriter(os.path.join(tensorboard_direc, versionName,'valid'))\n",
    "tensorboard_recorder_step = 0\n",
    "total_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-16T05:26:01.933Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Started Training ----------\n",
      "--- Epoch 1/1000 ---\n"
     ]
    }
   ],
   "source": [
    "writeMessage('---------- Started Training ----------', versionName)\n",
    "bestLoss = np.infty\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in tqdm(range(1, epochs+1)):  # loop over the dataset multiple times\n",
    "\n",
    "    writeMessage(\"--- Epoch {0}/{1} ---\".format(epoch, epochs), versionName)\n",
    "\n",
    "    surrogate.train()\n",
    "    trainLoss, tensorboard_recorder_step, total_steps = trainEpoch(trainDataLoader, \n",
    "                                                                   train_writer, surrogate,\n",
    "                                                                   opt, p_loss, loss,\n",
    "                                                                   rmse, lr_scheduler, \n",
    "                                                                   tensorboard_rate, device,\n",
    "                                                                   tensorboard_recorder_step, total_steps)\n",
    "\n",
    "    writeMessage(\"trainLoss: {:.4e}\".format(trainLoss),versionName)\n",
    "    writeMessage(\"LR: {:.4e}\".format(opt.param_groups[0]['lr']),versionName)\n",
    "#         if trainLoss < bestLoss:\n",
    "#             bestLoss = trainLoss\n",
    "#             writeMessage(\"Better trainLoss: {:.4e}, Saving models...\".format(bestLoss),versionName)\n",
    "#             torch.save(surrogate.state_dict(), os.path.join(cps,versionName))\n",
    "\n",
    "    surrogate.eval()\n",
    "    valLoss = validEpoch(testDataLoader, test_writer, surrogate, p_loss, loss, rmse, device, tensorboard_recorder_step)\n",
    "    writeMessage(\"valLoss: {:.4e}\".format(valLoss),versionName)\n",
    "\n",
    "    # checkpoint progress\n",
    "    if valLoss < bestLoss:\n",
    "        bestLoss = valLoss\n",
    "        writeMessage(\"Better valLoss: {:.4e}, Saving models...\".format(bestLoss),versionName)\n",
    "        torch.save(surrogate.state_dict(), os.path.join(cps,versionName))\n",
    "\n",
    "    lr_scheduler.step(valLoss)\n",
    "\n",
    "    if opt.param_groups[0]['lr'] < 5e-8:\n",
    "        break\n",
    "writeMessage('---------- Finished Training ----------', versionName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
