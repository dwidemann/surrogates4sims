{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.033572Z",
     "start_time": "2020-08-21T22:47:14.021020Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "# --- Must haves ---\n",
    "import os, sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from surrogates4sims.datasets import MantaFlowDataset, getSingleSim, createMantaFlowTrainTest\n",
    "\n",
    "from surrogates4sims.utils import create_opt, create_one_cycle, find_lr, printNumModelParams, \\\n",
    "                                    rmse, writeMessage, plotSampleWprediction, plotSampleWpredictionByChannel, \\\n",
    "                                    plotSample, curl, jacobian, stream2uv, create_movie, convertSimToImage\n",
    "\n",
    "#from surrogates4sims.models import Generator, Encoder, AE_no_P, AE_xhat_z, AE_xhat_zV2\n",
    "\n",
    "from surrogates4sims.train import trainEpoch, validEpoch\n",
    "\n",
    "from surrogates4sims.svd import MantaFlowSVDDataset\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.114297Z",
     "start_time": "2020-08-21T22:47:14.099949Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "# model name, for tensorboard recording and checkpointing purposes.\n",
    "versionName = \"LIN_SVD_only_z_and_p_LSTM_bidirec\"\n",
    "\n",
    "# GPU Numbers to use. Comma seprate them for multi-GPUs.\n",
    "gpu_ids = \"2\"#,1,2,3\"\n",
    "versionName = versionName + '_GPUs{}'.format(gpu_ids.replace(',',''))\n",
    "# path to load model weights.\n",
    "pretrained_path = None\n",
    "\n",
    "# rate at which to record metrics. (number of batches to average over when recording metrics, e.g. \"every 5 batches\")\n",
    "tensorboard_rate = 5\n",
    "\n",
    "# number of epochs to train. This is defined here so we can use the OneCycle LR Scheduler.\n",
    "epochs = 1000\n",
    "\n",
    "# Data Directory\n",
    "dataDirec = '/data/mantaFlowSim/data/smoke_pos21_size5_f200/v'\n",
    "reverseXY = False \n",
    "\n",
    "# checkpoint directory\n",
    "cps = 'cps'\n",
    "tensorboard_direc = \"tb\"\n",
    "\n",
    "findLRs = True  \n",
    "patience = 10\n",
    "\n",
    "# hyper-params\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "testSplit = .1\n",
    "bz = 4\n",
    "numSamplesToKeep = np.infty #if not debugging\n",
    "latentDim = 512\n",
    "simLen = 200\n",
    "simVizIndex = 0 # sim in the test set to visualize\n",
    "numComponents = latentDim\n",
    "transform = True\n",
    "if DEBUG:\n",
    "    epochs = 5000\n",
    "    numSamplesToKeep = 1000\n",
    "    \n",
    "versionName = versionName + '_latentDim{}_bz{}_transform{}_epochs{}'.format(latentDim,bz,transform,epochs)\n",
    "versionName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Select Personal GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.367721Z",
     "start_time": "2020-08-21T22:47:14.116261Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.376518Z",
     "start_time": "2020-08-21T22:47:14.372348Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=gpu_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.383607Z",
     "start_time": "2020-08-21T22:47:14.378916Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.395753Z",
     "start_time": "2020-08-21T22:47:14.385827Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if device.type == 'cuda':\n",
    "    print(cuda.is_available())\n",
    "    print(cuda.device_count())\n",
    "    print(cuda.current_device())\n",
    "    print(cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.655837Z",
     "start_time": "2020-08-21T22:47:14.398055Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.zeros(5, device=device.type)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Datasets & Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.664560Z",
     "start_time": "2020-08-21T22:47:14.659576Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# A = np.load('svd_out.npz')\n",
    "# vh = A['arr_2']\n",
    "# vh.shape\n",
    "# data = []\n",
    "# numComp = 512\n",
    "# for idx in range(105):\n",
    "#     D = getSingleSim(idx)\n",
    "#     simDataset = MantaFlowDataset(D)\n",
    "#     Z = []\n",
    "#     P = []\n",
    "#     X0, p = simDataset[0]\n",
    "#     for sample in simDataset:\n",
    "#         f, p = sample\n",
    "#         coeffs = vh[:numComp]@f.flatten()\n",
    "#         Z.append(coeffs)\n",
    "#         P.append(p)\n",
    "#     y = np.array(Z)\n",
    "#     P = np.array(P)\n",
    "#     X = (X0,P,y[0])\n",
    "#     data.append((X,y[1:]))\n",
    "# with open('simLatentVectors.pkl','wb') as fid:\n",
    "#     pickle.dump(data,fid)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.724937Z",
     "start_time": "2020-08-21T22:47:14.668108Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('simLatentVectors.pkl','rb') as fid:\n",
    "    data = pickle.load(fid)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.732738Z",
     "start_time": "2020-08-21T22:47:14.727717Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data[0][0][0].shape, data[0][0][1].shape, data[0][0][2].shape, data[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.837538Z",
     "start_time": "2020-08-21T22:47:14.734887Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainData, testData = createMantaFlowTrainTest(dataDirec,simLen,testSplit,seed)\n",
    "print((len(trainData),len(testData)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.898327Z",
     "start_time": "2020-08-21T22:47:14.838923Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "d = glob(os.path.join(dataDirec,'*.npz'))\n",
    "d = sorted(d)\n",
    "simLen = 200\n",
    "numSims = len(d)//simLen\n",
    "numTestSamples = int(np.round(testSplit*numSims))\n",
    "np.random.seed(seed)\n",
    "perm = np.random.permutation(numSims)\n",
    "testSims = perm[:numTestSamples]\n",
    "trainSims = perm[numTestSamples:]\n",
    "testSims, trainSims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.903700Z",
     "start_time": "2020-08-21T22:47:14.899903Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "testDataset = [data[i] for i in testSims]\n",
    "trainDataset = [data[i] for i in trainSims]\n",
    "len(testDataset), len(trainDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.964226Z",
     "start_time": "2020-08-21T22:47:14.905207Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = []\n",
    "E = []\n",
    "for X,y in trainDataset:\n",
    "    z = X[2]\n",
    "    z = z.reshape(1,len(z))\n",
    "    y = np.concatenate([z,y])\n",
    "    D.append(y.max(axis=0))\n",
    "    E.append(y.min(axis=0))\n",
    "D = np.array(D)\n",
    "E = np.array(E)\n",
    "ymx = D.max(axis=0)\n",
    "ymn = E.min(axis=0)\n",
    "ymx, ymn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.968529Z",
     "start_time": "2020-08-21T22:47:14.965532Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def transformY(y,ymx,ymn):\n",
    "    return (ymx - y)/(ymx - ymn)\n",
    "\n",
    "def inverseTransformY(y,ymx,ymn):\n",
    "    x = ymx - y*(ymx - ymn)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.974555Z",
     "start_time": "2020-08-21T22:47:14.969820Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = transformY(y,ymx,ymn)\n",
    "a.max(), a.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.980595Z",
     "start_time": "2020-08-21T22:47:14.976171Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "b = inverseTransformY(a,ymx,ymn)\n",
    "np.abs(y - b).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.990189Z",
     "start_time": "2020-08-21T22:47:14.982116Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class LatentSVD30(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "                 \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx, return_norm_stats=False):\n",
    "        X, y  = self.data[idx]\n",
    "        p = X[1]\n",
    "        z = X[2]\n",
    "\n",
    "        if self.transform:\n",
    "            y = torch.tensor(y)\n",
    "            norm_stats = y.norm(dim=1).unsqueeze(1)\n",
    "            y = y / norm_stats\n",
    "            \n",
    "        D = []\n",
    "        for pp in p[1:]:\n",
    "            zz = np.concatenate([pp,z])#.reshape(1,515)\n",
    "            D.append(zz)\n",
    "        X = np.array(D)\n",
    "\n",
    "        X = torch.tensor(X[:, :3])\n",
    "        inits = torch.cat([torch.tensor(z).unsqueeze(0), y[:-1]])\n",
    "        X = torch.cat([X, inits], dim=1)\n",
    "        \n",
    "        y = y[31:]\n",
    "        \n",
    "        if return_norm_stats:\n",
    "            return X, y, norm_stats\n",
    "        else:\n",
    "            return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:14.998019Z",
     "start_time": "2020-08-21T22:47:14.991847Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if transform:\n",
    "    testDataset = LatentSVD30(testDataset, transform=transformY)\n",
    "    trainDataset = LatentSVD30(trainDataset, transform=transformY)\n",
    "else:\n",
    "    testDataset = LatentSVD30(testDataset)\n",
    "    trainDataset = LatentSVD30(trainDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Making sure the transform works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:15.012029Z",
     "start_time": "2020-08-21T22:47:14.999645Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(dataset=trainDataset, batch_size=1, shuffle=True, drop_last=True)\n",
    "testDataLoader = DataLoader(dataset=testDataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:15.023235Z",
     "start_time": "2020-08-21T22:47:15.014051Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X,y = next(iter(trainDataLoader))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:15.511269Z",
     "start_time": "2020-08-21T22:47:15.024784Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y[0].squeeze().T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:15.517492Z",
     "start_time": "2020-08-21T22:47:15.512893Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y.max(), y.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:15.521702Z",
     "start_time": "2020-08-21T22:47:15.518917Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:15.527376Z",
     "start_time": "2020-08-21T22:47:15.522937Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X[:,:,3:].max(),X[:,:,3:].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:15.531542Z",
     "start_time": "2020-08-21T22:47:15.528764Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(dataset=trainDataset, batch_size=bz, shuffle=True, drop_last=True)\n",
    "testDataLoader = DataLoader(dataset=testDataset, batch_size=bz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:15.536471Z",
     "start_time": "2020-08-21T22:47:15.533113Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(trainDataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:15.541389Z",
     "start_time": "2020-08-21T22:47:15.539332Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trim_idx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:15.546096Z",
     "start_time": "2020-08-21T22:47:15.542997Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inputSize = X.shape[2]\n",
    "hiddenSize = y.shape[2]\n",
    "numLayers = 1\n",
    "bidirectional = False\n",
    "batch_first = True\n",
    "#model = nn.LSTM(inputSize, hiddenSize, numLayers, batch_first=True, bidirectional=bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:15.642074Z",
     "start_time": "2020-08-21T22:47:15.547574Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Lin30Fast(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lin30Fast,self).__init__()\n",
    "        \n",
    "        self.flattener = nn.Flatten()\n",
    "        self.my30Lins = nn.Linear(30*515, 30*128)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.final_fc = nn.Linear(30*128, 512)\n",
    "\n",
    "    def one_forward(self, x):\n",
    "        \n",
    "        x = self.flattener(x)\n",
    "        \n",
    "        feats = self.my30Lins(x)\n",
    "            \n",
    "        preds = self.final_fc(feats)\n",
    "        \n",
    "        return preds\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 1, 199, 515\n",
    "        cur_x = x[:, :30]\n",
    "        \n",
    "        all_preds = []\n",
    "        for i in range(1, 199-30):\n",
    "            \n",
    "            if (i-1) == trim_idx:\n",
    "                break\n",
    "            \n",
    "            preds = self.one_forward(cur_x)\n",
    "            all_preds.append(preds)\n",
    "            \n",
    "            cur_x = cur_x[:, 1:30]\n",
    "            abc = x[:, 30+i, :3]\n",
    "            pred_x = torch.cat([abc, preds], dim=1)\n",
    "            \n",
    "            cur_x = torch.cat([cur_x, pred_x.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        return torch.stack(all_preds, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:15.654697Z",
     "start_time": "2020-08-21T22:47:15.643663Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Lin30(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lin30,self).__init__()\n",
    "        \n",
    "        self.my30Lins = nn.ModuleList()\n",
    "        for i in range(30):\n",
    "            mySeq = nn.Sequential()\n",
    "            mySeq.add_module('lin_{0}'.format(i), nn.Linear(515, 128))\n",
    "            mySeq.add_module('tanh_{0}'.format(i), nn.Tanh())\n",
    "            self.my30Lins.append(mySeq)\n",
    "            \n",
    "        self.flattener = nn.Flatten()\n",
    "        self.final_fc = nn.Linear(30*128, 512)\n",
    "\n",
    "    def one_forward(self, x):\n",
    "        \n",
    "        feats = []\n",
    "        for i in range(len(self.my30Lins)):\n",
    "            feats.append( self.my30Lins[i](x[:, i]) )\n",
    "            \n",
    "        feats = torch.stack(feats, dim=1)\n",
    "        \n",
    "        feats = self.flattener(feats)\n",
    "        \n",
    "        preds = self.final_fc(feats)\n",
    "        \n",
    "        return preds\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 1, 199, 515\n",
    "        cur_x = x[:, :30]\n",
    "        \n",
    "        all_preds = []\n",
    "        for i in range(1, 199-30):\n",
    "            \n",
    "            if (i-1) == trim_idx:\n",
    "                break\n",
    "            \n",
    "            preds = self.one_forward(cur_x)\n",
    "            all_preds.append(preds)\n",
    "            \n",
    "            cur_x = cur_x[:, 1:30]\n",
    "            abc = x[:, 30+i, :3]\n",
    "            pred_x = torch.cat([abc, preds], dim=1)\n",
    "            \n",
    "            cur_x = torch.cat([cur_x, pred_x.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        return torch.stack(all_preds, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:15.704712Z",
     "start_time": "2020-08-21T22:47:15.657565Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Lin30().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:15.709617Z",
     "start_time": "2020-08-21T22:47:15.706132Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "printNumModelParams(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:15.724202Z",
     "start_time": "2020-08-21T22:47:15.711263Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output = model(X.to(device))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:15.728222Z",
     "start_time": "2020-08-21T22:47:15.725531Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if len(gpu_ids.split(',')) > 1:\n",
    "    model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Trimmed Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:39.544497Z",
     "start_time": "2020-08-21T22:47:39.538038Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mse_loss_func = nn.MSELoss()\n",
    "trim_idx = 2\n",
    "def trimmed_MSE(output, y):\n",
    "    return mse_loss_func(output[:, :trim_idx], y[:, :trim_idx])\n",
    "\n",
    "L = trimmed_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:39.811542Z",
     "start_time": "2020-08-21T22:47:39.786543Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    loss = L(output,y.to(device))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Set LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:43.685828Z",
     "start_time": "2020-08-21T22:47:43.682131Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# if findLRs and (len(gpu_ids.split(','))==1): # doesn't work for multigpu???\n",
    "#     opt = create_opt(1e-7,model)\n",
    "#     find_lr(model,opt,L,device,trainDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:43.748235Z",
     "start_time": "2020-08-21T22:47:43.740942Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_lr = .001\n",
    "versionName = versionName + '_lr{}'.format(str(max_lr))\n",
    "\n",
    "versionName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:44.880753Z",
     "start_time": "2020-08-21T22:47:44.869096Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def trainEpoch(myDataLoader, tensorboard_writer, model, opt, loss,\n",
    "               metric, lr_scheduler, tensorboard_rate, device,\n",
    "               tensorboard_recorder_step, total_steps):\n",
    "    running_loss = 0.0\n",
    "    running_rmse = 0.0\n",
    "    total_loss = 0.0\n",
    "    for i, sampleBatch in enumerate(myDataLoader, start=1):\n",
    "\n",
    "        # --- Main Training ---\n",
    "        \n",
    "        # gpu\n",
    "        X,y = sampleBatch[0],sampleBatch[1]\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        y_hat = model(X)\n",
    "        combined_loss = loss(y_hat,y)\n",
    "        combined_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # loss\n",
    "        batch_loss = combined_loss.item()\n",
    "        running_loss += batch_loss\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        # --- Metrics Recording ---\n",
    "\n",
    "        # metrics\n",
    "#         r = metric(y_hat, y)\n",
    "#         running_rmse += r\n",
    "\n",
    "        # record lr change\n",
    "        total_steps += 1\n",
    "        tensorboard_writer.add_scalar(tag=\"LR\", scalar_value=opt.param_groups[0]['lr'], global_step=total_steps)\n",
    "\n",
    "        # tensorboard writes\n",
    "        if (i % tensorboard_rate == 0):\n",
    "            tensorboard_recorder_step += 1\n",
    "            avg_running_loss = running_loss/tensorboard_rate\n",
    "            avg_running_rmse = running_rmse/tensorboard_rate\n",
    "            tensorboard_writer.add_scalar(tag=\"Loss\", scalar_value=avg_running_loss, global_step=tensorboard_recorder_step)\n",
    "            tensorboard_writer.add_scalar(tag=metric.__name__, scalar_value=avg_running_rmse, global_step=tensorboard_recorder_step)\n",
    "            # reset running_loss for the next set of batches. (tensorboard_rate number of batches)\n",
    "            running_loss = 0.0\n",
    "            running_rmse = 0.0\n",
    "\n",
    "    return total_loss/len(myDataLoader), tensorboard_recorder_step, total_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:44.888865Z",
     "start_time": "2020-08-21T22:47:44.882619Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def validEpoch(myDataLoader, tensorboard_writer, model, loss, metric,\n",
    "               device, tensorboard_recorder_step):\n",
    "    running_loss = 0.0\n",
    "    running_rmse = 0.0\n",
    "    for i, sampleBatch in enumerate(myDataLoader, start=1):\n",
    "\n",
    "        # --- Metrics Recording ---\n",
    "\n",
    "        # gpu\n",
    "        X,y = sampleBatch[0],sampleBatch[1]\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        #perc = len(X)/len(myDataLoader.dataset)\n",
    "        perc = 1./len(myDataLoader.dataset)\n",
    "        # forward, no gradient calculations\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(X)\n",
    "\n",
    "        # loss\n",
    "        combined_loss = loss(y_hat,y)\n",
    "        \n",
    "        running_loss += perc*(combined_loss.item())\n",
    "\n",
    "        # metrics\n",
    "#         r = metric(y_hat, y)\n",
    "#         running_rmse += perc*r\n",
    "\n",
    "    avg_running_loss = running_loss\n",
    "    avg_running_rmse = running_rmse\n",
    "    tensorboard_writer.add_scalar(tag=\"Loss\", scalar_value=avg_running_loss, global_step=tensorboard_recorder_step)\n",
    "    tensorboard_writer.add_scalar(tag=metric.__name__, scalar_value=avg_running_rmse, global_step=tensorboard_recorder_step)\n",
    "\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:45.116100Z",
     "start_time": "2020-08-21T22:47:45.110237Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(cps)\n",
    "except:\n",
    "    print(\"checkpoints directory already exists :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:45.896294Z",
     "start_time": "2020-08-21T22:47:45.832060Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create a summary writer.\n",
    "train_writer = SummaryWriter(os.path.join(tensorboard_direc, versionName,'train'))\n",
    "test_writer = SummaryWriter(os.path.join(tensorboard_direc, versionName,'valid'))\n",
    "tensorboard_recorder_step = 0\n",
    "total_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:48.947627Z",
     "start_time": "2020-08-21T22:47:48.933538Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fit_up_to_trim_idx():\n",
    "    \n",
    "    global tensorboard_recorder_step\n",
    "    global total_steps\n",
    "    \n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,patience=patience)\n",
    "\n",
    "    writeMessage('---------- Started Training ----------', versionName)\n",
    "    bestLoss = np.infty\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs+1)):  # loop over the dataset multiple times\n",
    "\n",
    "        writeMessage(\"--- Epoch {0}/{1} ---\".format(epoch, epochs), versionName)\n",
    "\n",
    "        model.train()\n",
    "        trainLoss, tensorboard_recorder_step, total_steps = trainEpoch(trainDataLoader, \n",
    "                                                                       train_writer, model, opt, L,\n",
    "                                                                       rmse, lr_scheduler, \n",
    "                                                                       tensorboard_rate, device,\n",
    "                                                                       tensorboard_recorder_step, total_steps)\n",
    "\n",
    "        writeMessage(\"trainLoss: {:.4e}\".format(trainLoss),versionName)\n",
    "        writeMessage(\"LR: {:.4e}\".format(opt.param_groups[0]['lr']),versionName)\n",
    "    #     if trainLoss < bestLoss:\n",
    "    #         bestLoss = trainLoss\n",
    "    #         writeMessage(\"Better trainLoss: {:.4e}, Saving models...\".format(bestLoss),versionName)\n",
    "    #         torch.save(model.state_dict(), os.path.join(cps,versionName))\n",
    "\n",
    "        model.eval()\n",
    "        valLoss = validEpoch(testDataLoader, test_writer, model, L, rmse, device, tensorboard_recorder_step)\n",
    "        writeMessage(\"valLoss: {:.4e}\".format(valLoss),versionName)\n",
    "\n",
    "        #checkpoint progress\n",
    "        if valLoss < bestLoss:\n",
    "            bestLoss = valLoss\n",
    "            writeMessage(\"Better valLoss: {:.4e}, Saving models...\".format(bestLoss),versionName)\n",
    "            torch.save(model.state_dict(), os.path.join(cps,versionName))\n",
    "\n",
    "        lr_scheduler.step(trainLoss)\n",
    "\n",
    "        if opt.param_groups[0]['lr'] < 5e-8:\n",
    "            break\n",
    "    writeMessage('---------- Finished Training ----------', versionName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:47:58.421508Z",
     "start_time": "2020-08-21T22:47:58.411820Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# {trim_idx : epochs_to_train_for}\n",
    "\n",
    "trim_epoch_dict = {}\n",
    "for i in range(2, 173, 5):\n",
    "    trim_epoch_dict[i] = 10\n",
    "#     if i in range(0, 50):\n",
    "#         trim_epoch_dict[i] = 100\n",
    "#     elif i in range(50, 100):\n",
    "#         trim_epoch_dict[i] = 150\n",
    "#     elif i in range(100, 150):\n",
    "#         trim_epoch_dict[i] = 200\n",
    "#     else:\n",
    "#         trim_epoch_dict[i] = 250\n",
    "trim_epoch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:18:25.039069Z",
     "start_time": "2020-08-21T22:18:25.029377Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "train_progress_widget = ipywidgets.Text('')\n",
    "train_progress_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:38:23.360862Z",
     "start_time": "2020-08-21T22:18:25.040480Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, value in trim_epoch_dict.items():\n",
    "    trim_idx = key\n",
    "    epochs = value\n",
    "    \n",
    "    train_progress_widget.value = 'Currently training trim_idx: {0}'.format(trim_idx)\n",
    "    fit_up_to_trim_idx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Compare: Generated vs. Simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:06:10.132667Z",
     "start_time": "2020-08-21T22:06:10.102763Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(cps,versionName)))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:42:19.497877Z",
     "start_time": "2020-08-21T22:42:19.489299Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:42:19.666211Z",
     "start_time": "2020-08-21T22:42:19.647864Z"
    },
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "X, y, norm_stats = testDataset.__getitem__(0, True)\n",
    "X = torch.tensor(X).unsqueeze(0)\n",
    "y = y.to(device)\n",
    "norm_stats = norm_stats.to(device)\n",
    "X.shape, y.shape, norm_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:42:19.809793Z",
     "start_time": "2020-08-21T22:42:19.806451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "norm_stats = norm_stats[31:]\n",
    "norm_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:42:19.960122Z",
     "start_time": "2020-08-21T22:42:19.956378Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trim_idx=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:42:20.739012Z",
     "start_time": "2020-08-21T22:42:20.129747Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_out = model(X.to(device))\n",
    "batch_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:42:20.870626Z",
     "start_time": "2020-08-21T22:42:20.740437Z"
    },
    "hidden": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_out = batch_out.squeeze()\n",
    "y = y.squeeze()\n",
    "err = []\n",
    "for i in range(y.shape[0]):\n",
    "    err.append(torch.norm(y[i] - batch_out[i]))\n",
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:42:21.002847Z",
     "start_time": "2020-08-21T22:42:20.872255Z"
    },
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "batch_out = batch_out.squeeze()*norm_stats\n",
    "y = y.squeeze()*norm_stats\n",
    "err = []\n",
    "for i in range(y.shape[0]):\n",
    "    err.append(torch.norm(y[i] - batch_out[i]))\n",
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:42:21.378827Z",
     "start_time": "2020-08-21T22:42:21.374124Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d = batch_out - y\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:42:21.958613Z",
     "start_time": "2020-08-21T22:42:21.801187Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:44:31.319949Z",
     "start_time": "2020-08-21T22:44:12.037305Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = np.arange(512)\n",
    "for dd,yy in zip(batch_out,y):\n",
    "    plt.plot(t, dd.detach().cpu().numpy(),t, yy.detach().cpu().numpy(),'--')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T20:04:17.010364Z",
     "start_time": "2020-08-21T20:04:17.005409Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = y.detach().cpu().numpy()\n",
    "tmp.max(), tmp.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T20:04:18.589303Z",
     "start_time": "2020-08-21T20:04:18.584116Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bb = batch_out.detach().cpu().numpy()\n",
    "bb.max(), bb.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.889Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(tmp.max(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.890Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(d[-2].detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.892Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X = y.detach().cpu().numpy()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.893Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2,verbose=3).fit_transform(X)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.894Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_embedded[:,0],X_embedded[:,1],s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.895Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = batch_out.detach().cpu().numpy()\n",
    "X_embedded = TSNE(n_components=2,verbose=3).fit_transform(X)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.896Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_embedded[:,0],X_embedded[:,1],s=2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
