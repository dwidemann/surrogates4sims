{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T19:27:30.809168Z",
     "start_time": "2020-11-12T19:27:30.805463Z"
    }
   },
   "outputs": [],
   "source": [
    "# default_exp models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T19:27:32.433517Z",
     "start_time": "2020-11-12T19:27:31.339963Z"
    }
   },
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from surrogates4sims.utils import printNumModelParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Fluids Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T19:33:49.229433Z",
     "start_time": "2020-11-12T19:33:49.094224Z"
    }
   },
   "outputs": [],
   "source": [
    "# EXPORT \n",
    "class convBlock(nn.Module):\n",
    "    def __init__(self,num_conv=4,in_channels=128,filters=128,act=nn.LeakyReLU(),downSample=True,\n",
    "                     norm=nn.BatchNorm2d):\n",
    "        super(convBlock,self).__init__()\n",
    "        self.num_conv = num_conv\n",
    "        self.in_channels = in_channels\n",
    "        self.act = act\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(num_conv):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Conv2d(in_channels, filters, kernel_size=3, stride=1,padding=1))\n",
    "                layers.append(norm(filters))\n",
    "                layers.append(act)\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(filters, filters, kernel_size=3, stride=1,padding=1))\n",
    "                layers.append(norm(filters))\n",
    "                layers.append(act)\n",
    "        self.convs = nn.Sequential(*layers)\n",
    "        #self.downSampleLayer = nn.Conv2d(in_channels+filters, in_channels+filters,kernel_size=3,stride=2,padding=1)\n",
    "        self.downSampleLayer = nn.Conv2d(filters, filters,kernel_size=3,stride=2,padding=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        x0 = x\n",
    "        x = self.convs(x)\n",
    "        #x = torch.cat([x,x0],axis=1)\n",
    "        x = self.downSampleLayer(x)\n",
    "        x = torch.cat([x,F.interpolate(x0,scale_factor=.5)],axis=1)\n",
    "        return x\n",
    "    \n",
    "class convTransBlock(nn.Module):\n",
    "    def __init__(self, num_conv=4, in_channels=128, filters=128, act=nn.LeakyReLU(),\n",
    "                 skip_connection=False, stack=False, norm=nn.BatchNorm2d):\n",
    "        super(convTransBlock,self).__init__()\n",
    "        self.filters = filters\n",
    "        self.num_conv = num_conv\n",
    "        self.in_channels = in_channels\n",
    "        self.act = act\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "        self.upsample = torch.nn.modules.Upsample(scale_factor=2)\n",
    "        layers = []\n",
    "        for i in range(num_conv-1):\n",
    "            if i == 0:\n",
    "                layers.append(nn.ConvTranspose2d(in_channels,out_channels=filters, kernel_size=3, stride=1,padding=1))\n",
    "                layers.append(norm(filters))\n",
    "                layers.append(act)\n",
    "            else:\n",
    "                layers.append(nn.ConvTranspose2d(filters,out_channels=filters, kernel_size=3, stride=1,padding=1))\n",
    "                layers.append(norm(filters))\n",
    "                layers.append(act)                \n",
    "        layers.append(nn.ConvTranspose2d(filters, out_channels=filters, kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1))\n",
    "        layers.append(act)\n",
    "        self.seq = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x0 = x\n",
    "        x = self.seq(x)\n",
    "        if self.skip_connection:\n",
    "            x += self.upsample(x0)\n",
    "        if self.stack:\n",
    "            x = torch.cat([x, self.upsample(x0)], axis=1)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,z, filters, output_shape,\n",
    "                 num_conv=4, conv_k=3, last_k=3, repeat=0, \n",
    "                 skip_connection=False, act=nn.LeakyReLU(),stack=False, norm=nn.BatchNorm2d, sigmoid_out=False):\n",
    "        super(Generator,self).__init__()\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(torch.max(output_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        x0_shape = [filters] + [int(i//np.power(2, repeat_num-1)) for i in output_shape[1:]]\n",
    "        print(x0_shape)\n",
    "        self.x0_shape = x0_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.filters = filters\n",
    "        self.num_conv = num_conv\n",
    "        num_output = int(np.prod(x0_shape))\n",
    "        self.num_output = num_output\n",
    "        self.linear = nn.Linear(z.shape[1], num_output)\n",
    "        \n",
    "        convTransBlockLayers = []\n",
    "        ch = filters\n",
    "        for i in range(repeat_num-1):\n",
    "            #print(ch)\n",
    "            convTransBlockLayers.append(convTransBlock(num_conv, ch, filters, act, skip_connection, stack,\n",
    "                                                      norm=norm))\n",
    "            if stack:\n",
    "                ch += filters\n",
    "\n",
    "        self.convTransBlockLayers = nn.Sequential(*convTransBlockLayers)\n",
    "        if ch > filters:\n",
    "            n = ch\n",
    "        else:\n",
    "            n = filters\n",
    "        if sigmoid_out:\n",
    "            self.lastConv = nn.Sequential(nn.Conv2d(n,int(output_shape[0]),kernel_size=3, stride=1,padding=1),\n",
    "                                          nn.Sigmoid())\n",
    "        else:\n",
    "            self.lastConv = nn.Conv2d(n,int(output_shape[0]),kernel_size=3, stride=1,padding=1)\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,self.x0_shape[0],self.x0_shape[1],self.x0_shape[2])\n",
    "        x = self.convTransBlockLayers(x)\n",
    "        x = self.lastConv(x)\n",
    "        return x\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, x, filters, z_num, num_conv=4, conv_k=3, repeat=0, act=nn.LeakyReLU(), norm=nn.BatchNorm2d):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        x_shape = x.shape[1:]\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.act = act\n",
    "        self.conv1 = nn.Conv2d(x_shape[0], filters, kernel_size=conv_k, stride=1,padding=1)\n",
    "        \n",
    "        ch = filters\n",
    "        convLayers = []\n",
    "        for idx in range(0,repeat_num):\n",
    "            convLayers.append(convBlock(num_conv, ch, filters, act=nn.LeakyReLU(), downSample=True, norm=norm))\n",
    "            ch += filters\n",
    "            \n",
    "        self.convs = nn.Sequential(*convLayers)\n",
    "        h = [i//2**repeat_num for i in x_shape[1:]]\n",
    "        self.nLinearInput = (ch)*np.prod(h)\n",
    "        self.linear = nn.Linear(self.nLinearInput,z_num)\n",
    "                             \n",
    "    def forward(self,x):\n",
    "        x = self.act(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.convs(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class Encoder_LK(nn.Module):\n",
    "    def __init__(self, x, filters, z_num, repeat=0, act=nn.LeakyReLU()):\n",
    "        super(Encoder_LK,self).__init__()\n",
    "        \n",
    "        x_shape = x.shape[1:]\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.act = act\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(x_shape[0])\n",
    "        self.conv1 = nn.Conv2d(x_shape[0], filters, kernel_size=13,stride=4,padding=6)\n",
    "        self.bn1 = nn.BatchNorm2d(filters)\n",
    "        self.conv2 = nn.Conv2d(filters, 4*filters, kernel_size=13,stride=4,padding=6)\n",
    "        self.bn2 = nn.BatchNorm2d(4*filters)\n",
    "        self.z_num = z_num\n",
    "        self.nLinearInput = 4*filters*8*6\n",
    "        self.linear = nn.Linear(self.nLinearInput,z_num)\n",
    "                             \n",
    "    def forward(self,x):\n",
    "        x = self.bn0(x)\n",
    "        x = self.act(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class Decoder_LK(nn.Module):\n",
    "    def __init__(self, x, filters, z_num, repeat=0, act=nn.LeakyReLU()):\n",
    "        super(Decoder_LK,self).__init__()\n",
    "        \n",
    "        x_shape = x.shape[1:]\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.act = act\n",
    "        self.bn0 = nn.BatchNorm2d(filters)\n",
    "        self.conv1 = nn.ConvTranspose2d(filters,filters,kernel_size=25,stride=4,padding=11,output_padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(filters,4*filters,kernel_size=25,stride=4,padding=11,output_padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(4*filters)\n",
    "        self.conv3 = nn.Conv2d(4*filters,x_shape[0], kernel_size=3,stride=1,padding=1)\n",
    "        self.z_num = z_num\n",
    "        self.nLinearOutput = filters*8*6\n",
    "        self.linear = nn.Linear(z_num,self.nLinearOutput)\n",
    "                             \n",
    "    def forward(self,x):\n",
    "        x = self.act(self.linear(x))\n",
    "        x = x.view(x.size(0),self.filters,8,6)\n",
    "        x = self.bn0(x)\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.bn0(x)\n",
    "        #print(x.shape)\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "    \n",
    "class AE_LK(nn.Module):\n",
    "    def __init__(self, encoder_LK, decoder_LK, return_z=True):\n",
    "        super(AE_LK,self).__init__()\n",
    "        self.encoder = encoder_LK\n",
    "        self.generator = decoder_LK\n",
    "        self.return_z = return_z\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        x = self.generator(z)\n",
    "        if self.return_z:\n",
    "            return x, z\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "class AE_no_P(nn.Module):\n",
    "    def __init__(self, encoder,generator):\n",
    "        super(AE_no_P,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        #x = torch.cat([x,p],axis=1)\n",
    "        x = self.generator(x)\n",
    "        return x\n",
    "    \n",
    "class AE_xhat_z(nn.Module):\n",
    "    def __init__(self, encoder,generator):\n",
    "        super(AE_xhat_z,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        x = self.generator(z)\n",
    "        return x, z\n",
    "    \n",
    "class AE_xhat_zV2(nn.Module):\n",
    "    def __init__(self, X, filters=32, latentDim=16, num_conv=2, repeat=0, \n",
    "                 skip_connection=False, stack=False, conv_k=3, last_k=3, act=nn.LeakyReLU, \n",
    "                 return_z=True, stream=True, device='cpu', norm = nn.BatchNorm2d, sigmoid_out=False):\n",
    "        super(AE_xhat_zV2,self).__init__()\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.latentDim = latentDim\n",
    "        self.num_conv = num_conv\n",
    "        self.repeat = repeat\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "        self.act = act\n",
    "        self.norm = norm\n",
    "        self.conv_k = 3\n",
    "        self.last_k = 3\n",
    "        self.device = device\n",
    "        self.return_z = return_z\n",
    "        self.stream = stream\n",
    "        \n",
    "        self.encoder = Encoder(X,filters,latentDim,num_conv=num_conv,norm=norm).to(device)\n",
    "        \n",
    "        z = self.encoder(X)\n",
    "        \n",
    "        if stream:\n",
    "            self.output_shape = torch.tensor(X[0][1:].shape)\n",
    "        else:\n",
    "            self.output_shape = torch.tensor(X[0].shape)\n",
    "\n",
    "        self.generator = Generator(z, filters, self.output_shape,\n",
    "                                   num_conv, conv_k, last_k, repeat, skip_connection, act=act,\n",
    "                                   stack=stack, norm=norm, sigmoid_out=sigmoid_out).to(device)\n",
    "        \n",
    "    def forward(self, x, p_x):\n",
    "        z = self.encoder(x)\n",
    "        z[:, -p_x.size(1):] = p_x\n",
    "        x = self.generator(z)\n",
    "        if self.return_z:\n",
    "            return x, z\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "class ConvDeconv(nn.Module):\n",
    "    def __init__(self, X, filters=32, latentDim=16, num_conv=2, repeat=1, \n",
    "                 skip_connection=False, stack=False, conv_k=3, last_k=3,\n",
    "                 act=nn.LeakyReLU(), return_z=True, stream=False, device='cpu'):\n",
    "        super(ConvDeconv,self).__init__()\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.latentDim = latentDim\n",
    "        self.num_conv = num_conv\n",
    "        self.repeat = repeat\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "        self.act = act\n",
    "        self.conv_k = 3\n",
    "        self.last_k = 3\n",
    "        self.device = device\n",
    "        self.return_z = return_z\n",
    "        self.stream = stream\n",
    "        \n",
    "        x_shape = X.shape\n",
    "        convLayers = [nn.BatchNorm2d(x_shape[1]),nn.Conv2d(x_shape[1],filters, kernel_size=conv_k,stride=2,padding=1)]\n",
    "        ch = filters\n",
    "        for idx in range(0,repeat):\n",
    "            convLayers.append(self.act)\n",
    "            convLayers.append(nn.BatchNorm2d(ch))\n",
    "            convLayers.append(nn.Conv2d(ch, ch+filters, kernel_size=conv_k,stride=2,padding=1))\n",
    "            ch += filters\n",
    "            \n",
    "            \n",
    "        self.encoder = nn.Sequential(*convLayers).to(device)\n",
    "        \n",
    "        z = self.encoder(X)\n",
    "        \n",
    "        if stream:\n",
    "            self.output_shape = torch.tensor(X[0][1:].shape)\n",
    "        else:\n",
    "            self.output_shape = torch.tensor(X[0].shape)\n",
    "        \n",
    "        #print(self.output_shape)\n",
    "        deconvLayers = []\n",
    "        for idx in range(0,repeat):\n",
    "            deconvLayers.append(nn.BatchNorm2d(ch))\n",
    "            deconvLayers.append(nn.ConvTranspose2d(ch,ch-filters, \n",
    "                                                   kernel_size=conv_k,stride=2,\n",
    "                                                   padding=1, output_padding=1))\n",
    "            deconvLayers.append(self.act)\n",
    "            ch -= filters\n",
    "        \n",
    "        deconvLayers.append(nn.BatchNorm2d(ch))\n",
    "        deconvLayers.append(nn.ConvTranspose2d(ch,int(self.output_shape[0]), \n",
    "                                               kernel_size=conv_k,stride=2,\n",
    "                                               padding=1, output_padding=1))  \n",
    "        \n",
    "        self.generator = nn.Sequential(*deconvLayers).to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        x = self.generator(z)\n",
    "        if self.return_z:\n",
    "            return x, z\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = [x.shape[0], *self.shape]\n",
    "        return x.view(s) \n",
    "    \n",
    "class ConvDeconvFactor2(nn.Module):\n",
    "    def __init__(self, X, filters=32, latentDim=16, num_conv=2, repeat=1, \n",
    "                 skip_connection=False, stack=False, conv_k=3, last_k=3,\n",
    "                 act=nn.LeakyReLU(), return_z=True, stream=False, device='cpu',\n",
    "                 use_sigmoid_output_layer=False, norm = nn.BatchNorm2d):\n",
    "        super(ConvDeconvFactor2,self).__init__()\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.latentDim = latentDim\n",
    "        self.num_conv = num_conv\n",
    "        self.repeat = repeat\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "        self.act = act\n",
    "        self.conv_k = 3\n",
    "        self.last_k = 3\n",
    "        self.device = device\n",
    "        self.return_z = return_z\n",
    "        self.stream = stream\n",
    "        self.use_sigmoid_output_layer = use_sigmoid_output_layer\n",
    "        \n",
    "        x_shape = X.shape\n",
    "        convLayers = [norm(x_shape[1]),nn.Conv2d(x_shape[1],filters, kernel_size=conv_k,stride=2,padding=1)]\n",
    "        ch = filters\n",
    "        for idx in range(0,repeat):\n",
    "            convLayers.append(self.act)\n",
    "            convLayers.append(norm(ch))\n",
    "            convLayers.append(nn.Conv2d(ch, 2*ch, kernel_size=conv_k,stride=2,padding=1))\n",
    "            ch = 2*ch\n",
    "        \n",
    "        self.ch = ch\n",
    "        self.sz = int(X.shape[2]//(2**(repeat+1)))\n",
    "        convLayers.append(Reshape(-1))\n",
    "        convLayers.append(nn.Linear(self.ch*self.sz*self.sz,self.latentDim))\n",
    "        self.encoder = nn.Sequential(*convLayers).to(device)\n",
    "        \n",
    "        z = self.encoder(X)\n",
    "    \n",
    "        if stream:\n",
    "            self.output_shape = torch.tensor(X[0][1:].shape)\n",
    "        else:\n",
    "            self.output_shape = torch.tensor(X[0].shape)\n",
    "        \n",
    "        #print(self.output_shape)\n",
    "        deconvLayers = [nn.Linear(self.latentDim,self.ch*self.sz*self.sz),\n",
    "                        Reshape(self.ch,self.sz,self.sz)]\n",
    "        for idx in range(0,repeat):\n",
    "            deconvLayers.append(norm(ch))\n",
    "            deconvLayers.append(nn.ConvTranspose2d(ch,ch//2, \n",
    "                                                   kernel_size=conv_k,stride=2,\n",
    "                                                   padding=1, output_padding=1))\n",
    "            deconvLayers.append(self.act)\n",
    "            ch = ch//2\n",
    "        \n",
    "        deconvLayers.append(norm(ch))\n",
    "        deconvLayers.append(nn.ConvTranspose2d(ch,int(self.output_shape[0]), \n",
    "                                               kernel_size=conv_k,stride=2,\n",
    "                                               padding=1, output_padding=1))  \n",
    "        \n",
    "        if use_sigmoid_output_layer:\n",
    "            deconvLayers.append(nn.Sigmoid())\n",
    "            \n",
    "        self.generator = nn.Sequential(*deconvLayers).to(device)\n",
    "        # input data must be square\n",
    "        \n",
    "        \n",
    "    def forward(self, x, p_x):\n",
    "        z = self.encoder(x)\n",
    "        z[:, -p_x.size(1):] = p_x\n",
    "        x = self.generator(z)\n",
    "        if self.return_z:\n",
    "            return x, z\n",
    "        else:\n",
    "            return x        \n",
    "        \n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, X, hiddenLayerSizes = [1024], activation=nn.ELU()):\n",
    "        super(MLP,self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.inputSize = X.shape[1:]\n",
    "        self.modules = []\n",
    "        self.modules.append(nn.Linear(np.prod(self.inputSize),hiddenLayerSizes[0]))\n",
    "        self.modules.append(self.activation)\n",
    "        for idx,sz in enumerate(hiddenLayerSizes[:-1]):\n",
    "            self.modules.append(nn.Linear(hiddenLayerSizes[idx],hiddenLayerSizes[idx+1]))\n",
    "            self.modules.append(self.activation)\n",
    "                               \n",
    "        self.modules.append(nn.Linear(hiddenLayerSizes[-1],np.prod(self.inputSize)))\n",
    "        self.layers = nn.Sequential(*self.modules)\n",
    "                                \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = self.layers(x)\n",
    "        x = x.view(x.shape[0],self.inputSize[0],self.inputSize[1],self.inputSize[2])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T19:33:56.203743Z",
     "start_time": "2020-11-12T19:33:56.199088Z"
    }
   },
   "outputs": [],
   "source": [
    "bz = 8\n",
    "latentDim = 16\n",
    "filters = 128\n",
    "num_conv = 4\n",
    "num_Gconv= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T19:33:58.046002Z",
     "start_time": "2020-11-12T19:33:58.033640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 128, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn([bz, 1, 128, 128]) #, torch.Size([32, 3]))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T19:33:59.622499Z",
     "start_time": "2020-11-12T19:33:59.615547Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_x = torch.ones((bz,2))\n",
    "p_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T19:36:23.949799Z",
     "start_time": "2020-11-12T19:36:22.748862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvDeconvFactor2(\n",
      "  (act): LeakyReLU(negative_slope=0.01)\n",
      "  (encoder): Sequential(\n",
      "    (0): Identity()\n",
      "    (1): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Identity()\n",
      "    (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Identity()\n",
      "    (7): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): Reshape()\n",
      "    (9): Linear(in_features=131072, out_features=512, bias=True)\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=131072, bias=True)\n",
      "    (1): Reshape()\n",
      "    (2): Identity()\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (4): LeakyReLU(negative_slope=0.01)\n",
      "    (5): Identity()\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Identity()\n",
      "    (9): ConvTranspose2d(128, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "LeakyReLU(negative_slope=0.01)\n",
      "Sequential(\n",
      "  (0): Identity()\n",
      "  (1): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (2): LeakyReLU(negative_slope=0.01)\n",
      "  (3): Identity()\n",
      "  (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (5): LeakyReLU(negative_slope=0.01)\n",
      "  (6): Identity()\n",
      "  (7): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (8): Reshape()\n",
      "  (9): Linear(in_features=131072, out_features=512, bias=True)\n",
      ")\n",
      "Identity()\n",
      "Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "Identity()\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "Identity()\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "Reshape()\n",
      "Linear(in_features=131072, out_features=512, bias=True)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=512, out_features=131072, bias=True)\n",
      "  (1): Reshape()\n",
      "  (2): Identity()\n",
      "  (3): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (4): LeakyReLU(negative_slope=0.01)\n",
      "  (5): Identity()\n",
      "  (6): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (7): LeakyReLU(negative_slope=0.01)\n",
      "  (8): Identity()\n",
      "  (9): ConvTranspose2d(128, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      ")\n",
      "Linear(in_features=512, out_features=131072, bias=True)\n",
      "Reshape()\n",
      "Identity()\n",
      "ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "Identity()\n",
      "ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "Identity()\n",
      "ConvTranspose2d(128, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "model = ConvDeconvFactor2(X, filters=128, latentDim=512, num_conv=2, repeat=2, norm=nn.Identity)\n",
    "for mod in model.modules():\n",
    "    print(mod)\n",
    "    assert type(mod)!=nn.BatchNorm2d, type(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvDeconvFactor2(X, filters=128, latentDim=512, num_conv=2, repeat=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ch, model.sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 128, 128]), torch.Size([8, 512]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xhat, z = model(X,p_x)\n",
    "Xhat.shape, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printNumModelParams(model)\n",
    "printNumModelParams(model.encoder)\n",
    "printNumModelParams(model.generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for repeat in range(0,6):\n",
    "    model = ConvDeconvFactor2(X, filters=128, latentDim=128, num_conv=2, repeat=repeat)\n",
    "    print(repeat)\n",
    "    printNumModelParams(model)\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.rand([bz,3])\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = convBlock(num_conv=num_conv,in_channels=X.shape[1],filters=filters)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = C(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTB = convTransBlock(num_conv=num_conv, filters=128, act=nn.LeakyReLU(), skip_connection=False, stack=True)\n",
    "XX = torch.randn([bz, 128,8,6]) #, torch.Size([32, 3]))\n",
    "XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = CTB(XX)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = Encoder(X,filters,latentDim,num_conv=num_conv)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printNumModelParams(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = E(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shape = torch.tensor(X.shape[1:])\n",
    "output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(bz, latentDim + p.shape[1])\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(z, 128, output_shape,\n",
    "                 num_conv=2, conv_k=3, last_k=3, repeat=0, \n",
    "                 skip_connection=False, act=nn.LeakyReLU(),stack=True)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.output_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printNumModelParams(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = G(z)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "class GeneratorOld(nn.Module):\n",
    "    def __init__(self,z, filters, output_shape,\n",
    "                 num_conv=4, conv_k=3, last_k=3, repeat=0, \n",
    "                 skip_connection=False, act=nn.LeakyReLU()):\n",
    "        super(GeneratorOld,self).__init__()\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(torch.max(output_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        x0_shape = [filters] + [i//np.power(2, repeat_num-1) for i in output_shape[1:]]\n",
    "        self.x0_shape = x0_shape\n",
    "        num_output = int(np.prod(x0_shape))\n",
    "        self.linear = nn.Linear(z.shape[1], num_output)\n",
    "        convTransBlockLayers = []\n",
    "        for i in range(repeat_num-1):\n",
    "            convTransBlockLayers.append(convTransBlock(num_conv,filters,act,skip_connection))\n",
    "        self.convTransBlockLayers = nn.Sequential(*convTransBlockLayers)\n",
    "        self.lastConv = nn.Conv2d(filters,int(output_shape[0]),kernel_size=3, stride=1,padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,self.x0_shape[0],self.x0_shape[1],self.x0_shape[2])\n",
    "        x = self.convTransBlockLayers(x)\n",
    "        #x = self.lastConv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nbdev.export import notebook2script\n",
    "# notebook2script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following two cells show that sigmoid is working\n",
    "\n",
    "#### the output goes from having a max/min of .02/.01 to .5/.49, which is about the value of the sigmoid function near zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat = 0\n",
    "skip_connection = False\n",
    "stack = False\n",
    "createStreamFcn = False\n",
    "model = AE_xhat_zV2(X, filters, latentDim, num_conv, repeat, \n",
    "                 skip_connection, stack, conv_k=3, last_k=3, \n",
    "                 act=nn.LeakyReLU(), return_z=True, stream=createStreamFcn, device='cpu',\n",
    "                norm=nn.Identity, sigmoid_out = False)\n",
    "X_out, z_out = model(X,p_x)\n",
    "X_out.shape, z_out.shape, X_out.max(), X_out.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat = 0\n",
    "skip_connection = False\n",
    "stack = False\n",
    "createStreamFcn = False\n",
    "model = AE_xhat_zV2(X, filters, latentDim, num_conv, repeat, \n",
    "                 skip_connection, stack, conv_k=3, last_k=3, \n",
    "                 act=nn.LeakyReLU(), return_z=True, stream=createStreamFcn, device='cpu',\n",
    "                norm=nn.Identity, sigmoid_out = True)\n",
    "X_out, z_out = model(X,p_x)\n",
    "X_out.shape, X_out.max(), X_out.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat = 0\n",
    "skip_connection = False\n",
    "stack = False\n",
    "createStreamFcn = False\n",
    "model = AE_xhat_zV2(X, filters, latentDim, num_conv, repeat, \n",
    "                 skip_connection, stack, conv_k=3, last_k=3, \n",
    "                 act=nn.LeakyReLU(), return_z=True, stream=createStreamFcn, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X,p_x)[1][:,-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = E(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhat,p = model(X)\n",
    "Xhat.shape,p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "loss_func = torch.nn.MSELoss()\n",
    "loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mseVal = loss_func(X,Xhat)\n",
    "mseVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build AE_no_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = E(X)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(z, 128, output_shape,\n",
    "                 num_conv=2, conv_k=3, last_k=3, repeat=0, \n",
    "                 skip_connection=False, act=nn.LeakyReLU(),stack=True)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AE_no_P(E,G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build AE_xhat_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = AE_xhat_z(E,G)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = nn.ConvTranspose1d(1,1,kernel_size=13,stride=4,padding=6,output_padding=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nn.Conv2d(2,2,kernel_size=13,stride=4,padding=6)\n",
    "out = c(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = c(out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = c(out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shape = X.shape[1:]\n",
    "repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2\n",
    "repeat_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = Encoder_LK(X,16,16)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = E(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Decoder_LK(X,16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat = D(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = AE_LK(E,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat,z = M(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat.shape,z.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func(X,xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
