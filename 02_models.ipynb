{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T17:29:17.973121Z",
     "start_time": "2020-09-24T17:29:17.969220Z"
    }
   },
   "outputs": [],
   "source": [
    "# default_exp models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T17:29:19.537337Z",
     "start_time": "2020-09-24T17:29:18.693613Z"
    }
   },
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from surrogates4sims.utils import printNumModelParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Fluids Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T17:29:45.911500Z",
     "start_time": "2020-09-24T17:29:45.845822Z"
    }
   },
   "outputs": [],
   "source": [
    "# EXPORT \n",
    "class convBlock(nn.Module):\n",
    "    def __init__(self,num_conv=4,in_channels=128,filters=128,act=nn.LeakyReLU(),downSample=True):\n",
    "        super(convBlock,self).__init__()\n",
    "        self.num_conv = num_conv\n",
    "        self.in_channels = in_channels\n",
    "        self.act = act\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(num_conv):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Conv2d(in_channels, filters, kernel_size=3, stride=1,padding=1))\n",
    "                layers.append(nn.BatchNorm2d(filters))\n",
    "                layers.append(act)\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(filters, filters, kernel_size=3, stride=1,padding=1))\n",
    "                layers.append(nn.BatchNorm2d(filters))\n",
    "                layers.append(act)\n",
    "        self.convs = nn.Sequential(*layers)\n",
    "        #self.downSampleLayer = nn.Conv2d(in_channels+filters, in_channels+filters,kernel_size=3,stride=2,padding=1)\n",
    "        self.downSampleLayer = nn.Conv2d(filters, filters,kernel_size=3,stride=2,padding=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        x0 = x\n",
    "        x = self.convs(x)\n",
    "        #x = torch.cat([x,x0],axis=1)\n",
    "        x = self.downSampleLayer(x)\n",
    "        x = torch.cat([x,F.interpolate(x0,scale_factor=.5)],axis=1)\n",
    "        return x\n",
    "    \n",
    "class convTransBlock(nn.Module):\n",
    "    def __init__(self, num_conv=4, in_channels=128, filters=128, act=nn.LeakyReLU(), skip_connection=False, stack=False):\n",
    "        super(convTransBlock,self).__init__()\n",
    "        self.filters = filters\n",
    "        self.num_conv = num_conv\n",
    "        self.in_channels = in_channels\n",
    "        self.act = act\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "        self.upsample = torch.nn.modules.Upsample(scale_factor=2)\n",
    "        layers = []\n",
    "        for i in range(num_conv-1):\n",
    "            if i == 0:\n",
    "                layers.append(nn.ConvTranspose2d(in_channels,out_channels=filters, kernel_size=3, stride=1,padding=1))\n",
    "                layers.append(nn.BatchNorm2d(filters))\n",
    "                layers.append(act)\n",
    "            else:\n",
    "                layers.append(nn.ConvTranspose2d(filters,out_channels=filters, kernel_size=3, stride=1,padding=1))\n",
    "                layers.append(nn.BatchNorm2d(filters))\n",
    "                layers.append(act)                \n",
    "        layers.append(nn.ConvTranspose2d(filters, out_channels=filters, kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1))\n",
    "        layers.append(act)\n",
    "        self.seq = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x0 = x\n",
    "        x = self.seq(x)\n",
    "        if self.skip_connection:\n",
    "            x += self.upsample(x0)\n",
    "        if self.stack:\n",
    "            x = torch.cat([x, self.upsample(x0)], axis=1)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,z, filters, output_shape,\n",
    "                 num_conv=4, conv_k=3, last_k=3, repeat=0, \n",
    "                 skip_connection=False, act=nn.LeakyReLU(),stack=False):\n",
    "        super(Generator,self).__init__()\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(torch.max(output_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        x0_shape = [filters] + [i//np.power(2, repeat_num-1) for i in output_shape[1:]]\n",
    "        self.x0_shape = x0_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.filters = filters\n",
    "        self.num_conv = num_conv\n",
    "        num_output = int(np.prod(x0_shape))\n",
    "        self.num_output = num_output\n",
    "        self.linear = nn.Linear(z.shape[1], num_output)\n",
    "        \n",
    "        convTransBlockLayers = []\n",
    "        ch = filters\n",
    "        for i in range(repeat_num-1):\n",
    "            #print(ch)\n",
    "            convTransBlockLayers.append(convTransBlock(num_conv, ch, filters, act, skip_connection, stack))\n",
    "            if stack:\n",
    "                ch += filters\n",
    "\n",
    "        self.convTransBlockLayers = nn.Sequential(*convTransBlockLayers)\n",
    "        if ch > filters:\n",
    "            n = ch\n",
    "        else:\n",
    "            n = filters\n",
    "        self.lastConv = nn.Conv2d(n,int(output_shape[0]),kernel_size=3, stride=1,padding=1)\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,self.x0_shape[0],self.x0_shape[1],self.x0_shape[2])\n",
    "        x = self.convTransBlockLayers(x)\n",
    "        x = self.lastConv(x)\n",
    "        return x\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, x, filters, z_num, num_conv=4, conv_k=3, repeat=0, act=nn.LeakyReLU()):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        x_shape = x.shape[1:]\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.act = act\n",
    "        self.conv1 = nn.Conv2d(x_shape[0], filters, kernel_size=conv_k, stride=1,padding=1)\n",
    "        \n",
    "        ch = filters\n",
    "        convLayers = []\n",
    "        for idx in range(0,repeat_num):\n",
    "            convLayers.append(convBlock(num_conv, ch, filters, act=nn.LeakyReLU(), downSample=True))\n",
    "            ch += filters\n",
    "            \n",
    "        self.convs = nn.Sequential(*convLayers)\n",
    "        h = [i//2**repeat_num for i in x_shape[1:]]\n",
    "        self.nLinearInput = (ch)*np.prod(h)\n",
    "        self.linear = nn.Linear(self.nLinearInput,z_num)\n",
    "                             \n",
    "    def forward(self,x):\n",
    "        x = self.act(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.convs(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class Encoder_LK(nn.Module):\n",
    "    def __init__(self, x, filters, z_num, repeat=0, act=nn.LeakyReLU()):\n",
    "        super(Encoder_LK,self).__init__()\n",
    "        \n",
    "        x_shape = x.shape[1:]\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.act = act\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(x_shape[0])\n",
    "        self.conv1 = nn.Conv2d(x_shape[0], filters, kernel_size=13,stride=4,padding=6)\n",
    "        self.bn1 = nn.BatchNorm2d(filters)\n",
    "        self.conv2 = nn.Conv2d(filters, 4*filters, kernel_size=13,stride=4,padding=6)\n",
    "        self.bn2 = nn.BatchNorm2d(4*filters)\n",
    "        self.z_num = z_num\n",
    "        self.nLinearInput = 4*filters*8*6\n",
    "        self.linear = nn.Linear(self.nLinearInput,z_num)\n",
    "                             \n",
    "    def forward(self,x):\n",
    "        x = self.bn0(x)\n",
    "        x = self.act(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class Decoder_LK(nn.Module):\n",
    "    def __init__(self, x, filters, z_num, repeat=0, act=nn.LeakyReLU()):\n",
    "        super(Decoder_LK,self).__init__()\n",
    "        \n",
    "        x_shape = x.shape[1:]\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.act = act\n",
    "        self.bn0 = nn.BatchNorm2d(filters)\n",
    "        self.conv1 = nn.ConvTranspose2d(filters,filters,kernel_size=25,stride=4,padding=11,output_padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(filters,4*filters,kernel_size=25,stride=4,padding=11,output_padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(4*filters)\n",
    "        self.conv3 = nn.Conv2d(4*filters,x_shape[0], kernel_size=3,stride=1,padding=1)\n",
    "        self.z_num = z_num\n",
    "        self.nLinearOutput = filters*8*6\n",
    "        self.linear = nn.Linear(z_num,self.nLinearOutput)\n",
    "                             \n",
    "    def forward(self,x):\n",
    "        x = self.act(self.linear(x))\n",
    "        x = x.view(x.size(0),self.filters,8,6)\n",
    "        x = self.bn0(x)\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.bn0(x)\n",
    "        #print(x.shape)\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "    \n",
    "class AE_LK(nn.Module):\n",
    "    def __init__(self, encoder_LK, decoder_LK, return_z=True):\n",
    "        super(AE_LK,self).__init__()\n",
    "        self.encoder = encoder_LK\n",
    "        self.generator = decoder_LK\n",
    "        self.return_z = return_z\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        x = self.generator(z)\n",
    "        if self.return_z:\n",
    "            return x, z\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "class AE_no_P(nn.Module):\n",
    "    def __init__(self, encoder,generator):\n",
    "        super(AE_no_P,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        #x = torch.cat([x,p],axis=1)\n",
    "        x = self.generator(x)\n",
    "        return x\n",
    "    \n",
    "class AE_xhat_z(nn.Module):\n",
    "    def __init__(self, encoder,generator):\n",
    "        super(AE_xhat_z,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        x = self.generator(z)\n",
    "        return x, z\n",
    "    \n",
    "class AE_xhat_zV2(nn.Module):\n",
    "    def __init__(self, X, filters=32, latentDim=16, num_conv=2, repeat=0, \n",
    "                 skip_connection=False, stack=False, conv_k=3, last_k=3,\n",
    "                 act=nn.LeakyReLU, return_z=True, stream=True, device='cpu'):\n",
    "        super(AE_xhat_zV2,self).__init__()\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.latentDim = latentDim\n",
    "        self.num_conv = num_conv\n",
    "        self.repeat = repeat\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "        self.act = act\n",
    "        self.conv_k = 3\n",
    "        self.last_k = 3\n",
    "        self.device = device\n",
    "        self.return_z = return_z\n",
    "        self.stream = stream\n",
    "        \n",
    "        self.encoder = Encoder(X,filters,latentDim,num_conv=num_conv).to(device)\n",
    "        \n",
    "        z = self.encoder(X)\n",
    "        \n",
    "        if stream:\n",
    "            self.output_shape = torch.tensor(X[0][1:].shape)\n",
    "        else:\n",
    "            self.output_shape = torch.tensor(X[0].shape)\n",
    "\n",
    "        self.generator = Generator(z, filters, self.output_shape,\n",
    "                                   num_conv, conv_k, last_k, repeat, \n",
    "                                   skip_connection, act=act, stack=stack).to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        x = self.generator(z)\n",
    "        if self.return_z:\n",
    "            return x, z\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "class ConvDeconv(nn.Module):\n",
    "    def __init__(self, X, filters=32, latentDim=16, num_conv=2, repeat=1, \n",
    "                 skip_connection=False, stack=False, conv_k=3, last_k=3,\n",
    "                 act=nn.LeakyReLU(), return_z=True, stream=False, device='cpu'):\n",
    "        super(ConvDeconv,self).__init__()\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.latentDim = latentDim\n",
    "        self.num_conv = num_conv\n",
    "        self.repeat = repeat\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "        self.act = act\n",
    "        self.conv_k = 3\n",
    "        self.last_k = 3\n",
    "        self.device = device\n",
    "        self.return_z = return_z\n",
    "        self.stream = stream\n",
    "        \n",
    "        x_shape = X.shape\n",
    "        convLayers = [nn.BatchNorm2d(x_shape[1]),nn.Conv2d(x_shape[1],filters, kernel_size=conv_k,stride=2,padding=1)]\n",
    "        ch = filters\n",
    "        for idx in range(0,repeat):\n",
    "            convLayers.append(self.act)\n",
    "            convLayers.append(nn.BatchNorm2d(ch))\n",
    "            convLayers.append(nn.Conv2d(ch, ch+filters, kernel_size=conv_k,stride=2,padding=1))\n",
    "            ch += filters\n",
    "            \n",
    "            \n",
    "        self.encoder = nn.Sequential(*convLayers).to(device)\n",
    "        \n",
    "        z = self.encoder(X)\n",
    "        \n",
    "        if stream:\n",
    "            self.output_shape = torch.tensor(X[0][1:].shape)\n",
    "        else:\n",
    "            self.output_shape = torch.tensor(X[0].shape)\n",
    "        \n",
    "        #print(self.output_shape)\n",
    "        deconvLayers = []\n",
    "        for idx in range(0,repeat):\n",
    "            deconvLayers.append(nn.BatchNorm2d(ch))\n",
    "            deconvLayers.append(nn.ConvTranspose2d(ch,ch-filters, \n",
    "                                                   kernel_size=conv_k,stride=2,\n",
    "                                                   padding=1, output_padding=1))\n",
    "            deconvLayers.append(self.act)\n",
    "            ch -= filters\n",
    "        \n",
    "        deconvLayers.append(nn.BatchNorm2d(ch))\n",
    "        deconvLayers.append(nn.ConvTranspose2d(ch,int(self.output_shape[0]), \n",
    "                                               kernel_size=conv_k,stride=2,\n",
    "                                               padding=1, output_padding=1))  \n",
    "        \n",
    "        self.generator = nn.Sequential(*deconvLayers).to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        x = self.generator(z)\n",
    "        if self.return_z:\n",
    "            return x, z\n",
    "        else:\n",
    "            return x\n",
    " \n",
    "\n",
    "class ConvDeconvFactor2(nn.Module):\n",
    "    def __init__(self, X, filters=32, latentDim=16, num_conv=2, repeat=1, \n",
    "                 skip_connection=False, stack=False, conv_k=3, last_k=3,\n",
    "                 act=nn.LeakyReLU(), return_z=True, stream=False, device='cpu'):\n",
    "        super(ConvDeconvFactor2,self).__init__()\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.latentDim = latentDim\n",
    "        self.num_conv = num_conv\n",
    "        self.repeat = repeat\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "        self.act = act\n",
    "        self.conv_k = 3\n",
    "        self.last_k = 3\n",
    "        self.device = device\n",
    "        self.return_z = return_z\n",
    "        self.stream = stream\n",
    "        \n",
    "        x_shape = X.shape\n",
    "        convLayers = [nn.BatchNorm2d(x_shape[1]),nn.Conv2d(x_shape[1],filters, kernel_size=conv_k,stride=2,padding=1)]\n",
    "        ch = filters\n",
    "        for idx in range(0,repeat):\n",
    "            convLayers.append(self.act)\n",
    "            convLayers.append(nn.BatchNorm2d(ch))\n",
    "            convLayers.append(nn.Conv2d(ch, 2*ch, kernel_size=conv_k,stride=2,padding=1))\n",
    "            ch = 2*ch\n",
    "            \n",
    "            \n",
    "        self.encoder = nn.Sequential(*convLayers).to(device)\n",
    "        \n",
    "        z = self.encoder(X)\n",
    "        \n",
    "        if stream:\n",
    "            self.output_shape = torch.tensor(X[0][1:].shape)\n",
    "        else:\n",
    "            self.output_shape = torch.tensor(X[0].shape)\n",
    "        \n",
    "        #print(self.output_shape)\n",
    "        deconvLayers = []\n",
    "        for idx in range(0,repeat):\n",
    "            deconvLayers.append(nn.BatchNorm2d(ch))\n",
    "            deconvLayers.append(nn.ConvTranspose2d(ch,ch//2, \n",
    "                                                   kernel_size=conv_k,stride=2,\n",
    "                                                   padding=1, output_padding=1))\n",
    "            deconvLayers.append(self.act)\n",
    "            ch = ch//2\n",
    "        \n",
    "        deconvLayers.append(nn.BatchNorm2d(ch))\n",
    "        deconvLayers.append(nn.ConvTranspose2d(ch,int(self.output_shape[0]), \n",
    "                                               kernel_size=conv_k,stride=2,\n",
    "                                               padding=1, output_padding=1))  \n",
    "        \n",
    "        self.generator = nn.Sequential(*deconvLayers).to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        x = self.generator(z)\n",
    "        if self.return_z:\n",
    "            return x, z\n",
    "        else:\n",
    "            return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bz = 8\n",
    "latentDim = 16\n",
    "filters = 128\n",
    "num_conv = 4\n",
    "num_Gconv= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 128, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn([bz, 2, 128, 96]) #, torch.Size([32, 3]))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2, 128,  96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shape = torch.tensor(X[0].shape)\n",
    "output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvDeconvFactor2(X, filters=128, latentDim=512, num_conv=2, repeat=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): Conv2d(2, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (2): LeakyReLU(negative_slope=0.01)\n",
       "  (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (5): LeakyReLU(negative_slope=0.01)\n",
       "  (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (8): LeakyReLU(negative_slope=0.01)\n",
       "  (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (11): LeakyReLU(negative_slope=0.01)\n",
       "  (12): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): ConvTranspose2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (2): LeakyReLU(negative_slope=0.01)\n",
       "  (3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (5): LeakyReLU(negative_slope=0.01)\n",
       "  (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (8): LeakyReLU(negative_slope=0.01)\n",
       "  (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (11): LeakyReLU(negative_slope=0.01)\n",
       "  (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): ConvTranspose2d(128, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 2, 128, 96]), torch.Size([8, 2048, 4, 3]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xhat, z = model(X)\n",
    "Xhat.shape, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2, 128,  96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shape = torch.tensor(X[0].shape)\n",
    "output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.rand([bz,3])\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convBlock(\n",
       "  (act): LeakyReLU(negative_slope=0.01)\n",
       "  (convs): Sequential(\n",
       "    (0): Conv2d(2, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.01)\n",
       "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = convBlock(num_conv=num_conv,in_channels=X.shape[1],filters=filters)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 130, 64, 48])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = C(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128, 8, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CTB = convTransBlock(num_conv=num_conv, filters=128, act=nn.LeakyReLU(), skip_connection=False, stack=True)\n",
    "XX = torch.randn([bz, 128,8,6]) #, torch.Size([32, 3]))\n",
    "XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 16, 12])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = CTB(XX)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (act): LeakyReLU(negative_slope=0.01)\n",
       "  (conv1): Conv2d(2, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (convs): Sequential(\n",
       "    (0): convBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): LeakyReLU(negative_slope=0.01)\n",
       "        (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (1): convBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): LeakyReLU(negative_slope=0.01)\n",
       "        (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (2): convBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): LeakyReLU(negative_slope=0.01)\n",
       "        (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (3): convBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): LeakyReLU(negative_slope=0.01)\n",
       "        (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (4): convBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(640, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): LeakyReLU(negative_slope=0.01)\n",
       "        (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=9216, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = Encoder(X,filters,latentDim,num_conv=num_conv)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 layers require gradients (unfrozen) out of 94 layers\n",
      "5,319,184 parameters require gradients (unfrozen) out of 5,319,184 parameters\n"
     ]
    }
   ],
   "source": [
    "printNumModelParams(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = E(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2, 128,  96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shape = torch.tensor(X.shape[1:])\n",
    "output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 19])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.randn(bz, latentDim + p.shape[1])\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (linear): Linear(in_features=19, out_features=6144, bias=True)\n",
       "  (convTransBlockLayers): Sequential(\n",
       "    (0): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (1): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (2): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (3): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lastConv): Conv2d(640, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = Generator(z, 128, output_shape,\n",
    "                 num_conv=2, conv_k=3, last_k=3, repeat=0, \n",
    "                 skip_connection=False, act=nn.LeakyReLU(),stack=True)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 layers require gradients (unfrozen) out of 28 layers\n",
      "2,200,834 parameters require gradients (unfrozen) out of 2,200,834 parameters\n"
     ]
    }
   ],
   "source": [
    "printNumModelParams(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 128, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = G(z)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T17:30:11.526778Z",
     "start_time": "2020-09-24T17:30:11.513979Z"
    }
   },
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "class GeneratorOld(nn.Module):\n",
    "    def __init__(self,z, filters, output_shape,\n",
    "                 num_conv=4, conv_k=3, last_k=3, repeat=0, \n",
    "                 skip_connection=False, act=nn.LeakyReLU()):\n",
    "        super(GeneratorOld,self).__init__()\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(torch.max(output_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        x0_shape = [filters] + [i//np.power(2, repeat_num-1) for i in output_shape[1:]]\n",
    "        self.x0_shape = x0_shape\n",
    "        num_output = int(np.prod(x0_shape))\n",
    "        self.linear = nn.Linear(z.shape[1], num_output)\n",
    "        convTransBlockLayers = []\n",
    "        for i in range(repeat_num-1):\n",
    "            convTransBlockLayers.append(convTransBlock(num_conv,filters,act,skip_connection))\n",
    "        self.convTransBlockLayers = nn.Sequential(*convTransBlockLayers)\n",
    "        self.lastConv = nn.Conv2d(filters,int(output_shape[0]),kernel_size=3, stride=1,padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,self.x0_shape[0],self.x0_shape[1],self.x0_shape[2])\n",
    "        x = self.convTransBlockLayers(x)\n",
    "        #x = self.lastConv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T17:33:17.199326Z",
     "start_time": "2020-09-24T17:33:13.920119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_mantaflow_dataset.ipynb.\n",
      "Converted 01_pnnl_dataset.ipynb.\n",
      "Converted 02_models.ipynb.\n",
      "Converted 03_train.ipynb.\n",
      "Converted 04_utils.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      " \n",
      "Warning: Exporting to \"None.py\" but this module is not part of this build\n",
      "Converted 05_experiment-Copy1.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      " \n",
      "Warning: Exporting to \"None.py\" but this module is not part of this build\n",
      "Converted 05_experiment.ipynb.\n",
      "Converted 05_experiment_SVD.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      " \n",
      "This cell doesn't have an export destination and was ignored:\n",
      " \n",
      "Warning: Exporting to \"None.py\" but this module is not part of this build\n",
      "Warning: Exporting to \"None.py\" but this module is not part of this build\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ec1767d147e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnbdev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotebook2script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnotebook2script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nbdev/export.py\u001b[0m in \u001b[0;36mnotebook2script\u001b[0;34m(fname, silent, to_dict)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mto_dict\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_mod_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_notebook2script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madd_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nbdev/export.py\u001b[0m in \u001b[0;36m_notebook2script\u001b[0;34m(fname, modules, silent, to_dict)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_from_future_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mto_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_add2all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"'{f}'\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'.'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' +$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMULTILINE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nbdev/export.py\u001b[0m in \u001b[0;36m_add2all\u001b[0;34m(fname, names, line_width)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mtw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_indent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsequent_indent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbreak_long_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mre_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_re__all__def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mre_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0mtext_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{text[start:end-1]}{'' if text[end-2]=='[' else ', '}{', '.join(names)}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_all\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'start'"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = Encoder(X,filters,latentDim,num_conv=num_conv)\n",
    "G = Generator(z, num_Gconv, output_shape,\n",
    "                 num_conv=num_conv, conv_k=3, last_k=3, repeat=0, \n",
    "                 skip_connection=False, act=nn.LeakyReLU())\n",
    "\n",
    "model = AE_xhat_z(E,G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = E(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 128, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [8 x 16], m2: [19 x 6144] at /opt/conda/conda-bld/pytorch_1579040055865/work/aten/src/TH/generic/THTensorMath.cpp:136",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-1705b61c720f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mXhat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torch2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3b3bf9c43691>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3b3bf9c43691>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx0_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx0_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx0_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvTransBlockLayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch2/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch2/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [8 x 16], m2: [19 x 6144] at /opt/conda/conda-bld/pytorch_1579040055865/work/aten/src/TH/generic/THTensorMath.cpp:136"
     ]
    }
   ],
   "source": [
    "Xhat,z = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss Function\n",
    "loss_func = torch.nn.MSELoss()\n",
    "loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.3635, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mseVal = loss_func(X,Xhat)\n",
    "mseVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build AE_no_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = E(X)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (linear): Linear(in_features=16, out_features=6144, bias=True)\n",
       "  (convTransBlockLayers): Sequential(\n",
       "    (0): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (1): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (2): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (3): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lastConv): Conv2d(640, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = Generator(z, 128, output_shape,\n",
    "                 num_conv=2, conv_k=3, last_k=3, repeat=0, \n",
    "                 skip_connection=False, act=nn.LeakyReLU(),stack=True)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AE_no_P(E,G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 128, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build AE_xhat_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AE_xhat_z(\n",
       "  (encoder): Encoder(\n",
       "    (act): LeakyReLU(negative_slope=0.01)\n",
       "    (conv1): Conv2d(2, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (convs): Sequential(\n",
       "      (0): convBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.01)\n",
       "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): LeakyReLU(negative_slope=0.01)\n",
       "          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (11): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (1): convBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.01)\n",
       "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): LeakyReLU(negative_slope=0.01)\n",
       "          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (11): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (2): convBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.01)\n",
       "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): LeakyReLU(negative_slope=0.01)\n",
       "          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (11): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (3): convBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.01)\n",
       "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): LeakyReLU(negative_slope=0.01)\n",
       "          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (11): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (4): convBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(640, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.01)\n",
       "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): LeakyReLU(negative_slope=0.01)\n",
       "          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (11): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=9216, out_features=16, bias=True)\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (linear): Linear(in_features=16, out_features=6144, bias=True)\n",
       "    (convTransBlockLayers): Sequential(\n",
       "      (0): convTransBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (seq): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (4): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (1): convTransBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (seq): Sequential(\n",
       "          (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (4): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (2): convTransBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (seq): Sequential(\n",
       "          (0): ConvTranspose2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (4): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (3): convTransBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (seq): Sequential(\n",
       "          (0): ConvTranspose2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (4): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lastConv): Conv2d(640, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = AE_xhat_z(E,G)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = nn.ConvTranspose1d(1,1,kernel_size=13,stride=4,padding=6,output_padding=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 32, 24])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = nn.Conv2d(2,2,kernel_size=13,stride=4,padding=6)\n",
    "out = c(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 8, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = c(out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 2, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = c(out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_shape = X.shape[1:]\n",
    "repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2\n",
    "repeat_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder_LK(\n",
       "  (act): LeakyReLU(negative_slope=0.01)\n",
       "  (bn0): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv1): Conv2d(2, 16, kernel_size=(13, 13), stride=(4, 4), padding=(6, 6))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 64, kernel_size=(13, 13), stride=(4, 4), padding=(6, 6))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=3072, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = Encoder_LK(X,16,16)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = E(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Decoder_LK(X,16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat = D(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 128, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = AE_LK(E,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat,z = M(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 2, 128, 96]), torch.Size([8, 16]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xhat.shape,z.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3620, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(X,xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
