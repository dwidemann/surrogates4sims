{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from surrogates4sims.utils import printNumModelParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Fluids Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT \n",
    "class convBlock(nn.Module):\n",
    "    def __init__(self,num_conv=4,in_channels=128,filters=128,act=nn.LeakyReLU(),downSample=True,\n",
    "                     norm=nn.BatchNorm2d):\n",
    "        super(convBlock,self).__init__()\n",
    "        self.num_conv = num_conv\n",
    "        self.in_channels = in_channels\n",
    "        self.act = act\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(num_conv):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Conv2d(in_channels, filters, kernel_size=3, stride=1,padding=1))\n",
    "                layers.append(norm(filters))\n",
    "                layers.append(act)\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(filters, filters, kernel_size=3, stride=1,padding=1))\n",
    "                layers.append(norm(filters))\n",
    "                layers.append(act)\n",
    "        self.convs = nn.Sequential(*layers)\n",
    "        #self.downSampleLayer = nn.Conv2d(in_channels+filters, in_channels+filters,kernel_size=3,stride=2,padding=1)\n",
    "        self.downSampleLayer = nn.Conv2d(filters, filters,kernel_size=3,stride=2,padding=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        x0 = x\n",
    "        x = self.convs(x)\n",
    "        #x = torch.cat([x,x0],axis=1)\n",
    "        x = self.downSampleLayer(x)\n",
    "        x = torch.cat([x,F.interpolate(x0,scale_factor=.5)],axis=1)\n",
    "        return x\n",
    "    \n",
    "class convTransBlock(nn.Module):\n",
    "    def __init__(self, num_conv=4, in_channels=128, filters=128, act=nn.LeakyReLU(),\n",
    "                 skip_connection=False, stack=False, norm=nn.BatchNorm2d):\n",
    "        super(convTransBlock,self).__init__()\n",
    "        self.filters = filters\n",
    "        self.num_conv = num_conv\n",
    "        self.in_channels = in_channels\n",
    "        self.act = act\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "        self.upsample = torch.nn.modules.Upsample(scale_factor=2)\n",
    "        layers = []\n",
    "        for i in range(num_conv-1):\n",
    "            if i == 0:\n",
    "                layers.append(nn.ConvTranspose2d(in_channels,out_channels=filters, kernel_size=3, stride=1,padding=1))\n",
    "                layers.append(norm(filters))\n",
    "                layers.append(act)\n",
    "            else:\n",
    "                layers.append(nn.ConvTranspose2d(filters,out_channels=filters, kernel_size=3, stride=1,padding=1))\n",
    "                layers.append(norm(filters))\n",
    "                layers.append(act)                \n",
    "        layers.append(nn.ConvTranspose2d(filters, out_channels=filters, kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1))\n",
    "        layers.append(act)\n",
    "        self.seq = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x0 = x\n",
    "        x = self.seq(x)\n",
    "        if self.skip_connection:\n",
    "            x += self.upsample(x0)\n",
    "        if self.stack:\n",
    "            x = torch.cat([x, self.upsample(x0)], axis=1)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,z, filters, output_shape,\n",
    "                 num_conv=4, conv_k=3, last_k=3, repeat=0, \n",
    "                 skip_connection=False, act=nn.LeakyReLU(),stack=False, norm=nn.BatchNorm2d, sigmoid_out=False):\n",
    "        super(Generator,self).__init__()\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(torch.max(output_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        x0_shape = [filters] + [int(i//np.power(2, repeat_num-1)) for i in output_shape[1:]]\n",
    "        print(x0_shape)\n",
    "        self.x0_shape = x0_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.filters = filters\n",
    "        self.num_conv = num_conv\n",
    "        num_output = int(np.prod(x0_shape))\n",
    "        self.num_output = num_output\n",
    "        self.linear = nn.Linear(z.shape[1], num_output)\n",
    "        \n",
    "        convTransBlockLayers = []\n",
    "        ch = filters\n",
    "        for i in range(repeat_num-1):\n",
    "            #print(ch)\n",
    "            convTransBlockLayers.append(convTransBlock(num_conv, ch, filters, act, skip_connection, stack,\n",
    "                                                      norm=norm))\n",
    "            if stack:\n",
    "                ch += filters\n",
    "\n",
    "        self.convTransBlockLayers = nn.Sequential(*convTransBlockLayers)\n",
    "        if ch > filters:\n",
    "            n = ch\n",
    "        else:\n",
    "            n = filters\n",
    "        if sigmoid_out:\n",
    "            self.lastConv = nn.Sequential(nn.Conv2d(n,int(output_shape[0]),kernel_size=3, stride=1,padding=1),\n",
    "                                          nn.Sigmoid())\n",
    "        else:\n",
    "            self.lastConv = nn.Conv2d(n,int(output_shape[0]),kernel_size=3, stride=1,padding=1)\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,self.x0_shape[0],self.x0_shape[1],self.x0_shape[2])\n",
    "        x = self.convTransBlockLayers(x)\n",
    "        x = self.lastConv(x)\n",
    "        return x\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, x, filters, z_num, num_conv=4, conv_k=3, repeat=0, act=nn.LeakyReLU(), norm=nn.BatchNorm2d):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        x_shape = x.shape[1:]\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.act = act\n",
    "        self.conv1 = nn.Conv2d(x_shape[0], filters, kernel_size=conv_k, stride=1,padding=1)\n",
    "        \n",
    "        ch = filters\n",
    "        convLayers = []\n",
    "        for idx in range(0,repeat_num):\n",
    "            convLayers.append(convBlock(num_conv, ch, filters, act=nn.LeakyReLU(), downSample=True, norm=norm))\n",
    "            ch += filters\n",
    "            \n",
    "        self.convs = nn.Sequential(*convLayers)\n",
    "        h = [i//2**repeat_num for i in x_shape[1:]]\n",
    "        self.nLinearInput = (ch)*np.prod(h)\n",
    "        self.linear = nn.Linear(self.nLinearInput,z_num)\n",
    "                             \n",
    "    def forward(self,x):\n",
    "        x = self.act(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.convs(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class Encoder_LK(nn.Module):\n",
    "    def __init__(self, x, filters, z_num, repeat=0, act=nn.LeakyReLU()):\n",
    "        super(Encoder_LK,self).__init__()\n",
    "        \n",
    "        x_shape = x.shape[1:]\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.act = act\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(x_shape[0])\n",
    "        self.conv1 = nn.Conv2d(x_shape[0], filters, kernel_size=13,stride=4,padding=6)\n",
    "        self.bn1 = nn.BatchNorm2d(filters)\n",
    "        self.conv2 = nn.Conv2d(filters, 4*filters, kernel_size=13,stride=4,padding=6)\n",
    "        self.bn2 = nn.BatchNorm2d(4*filters)\n",
    "        self.z_num = z_num\n",
    "        self.nLinearInput = 4*filters*8*6\n",
    "        self.linear = nn.Linear(self.nLinearInput,z_num)\n",
    "                             \n",
    "    def forward(self,x):\n",
    "        x = self.bn0(x)\n",
    "        x = self.act(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class Decoder_LK(nn.Module):\n",
    "    def __init__(self, x, filters, z_num, repeat=0, act=nn.LeakyReLU()):\n",
    "        super(Decoder_LK,self).__init__()\n",
    "        \n",
    "        x_shape = x.shape[1:]\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.act = act\n",
    "        self.bn0 = nn.BatchNorm2d(filters)\n",
    "        self.conv1 = nn.ConvTranspose2d(filters,filters,kernel_size=25,stride=4,padding=11,output_padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(filters,4*filters,kernel_size=25,stride=4,padding=11,output_padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(4*filters)\n",
    "        self.conv3 = nn.Conv2d(4*filters,x_shape[0], kernel_size=3,stride=1,padding=1)\n",
    "        self.z_num = z_num\n",
    "        self.nLinearOutput = filters*8*6\n",
    "        self.linear = nn.Linear(z_num,self.nLinearOutput)\n",
    "                             \n",
    "    def forward(self,x):\n",
    "        x = self.act(self.linear(x))\n",
    "        x = x.view(x.size(0),self.filters,8,6)\n",
    "        x = self.bn0(x)\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.bn0(x)\n",
    "        #print(x.shape)\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "    \n",
    "class AE_LK(nn.Module):\n",
    "    def __init__(self, encoder_LK, decoder_LK, return_z=True):\n",
    "        super(AE_LK,self).__init__()\n",
    "        self.encoder = encoder_LK\n",
    "        self.generator = decoder_LK\n",
    "        self.return_z = return_z\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        x = self.generator(z)\n",
    "        if self.return_z:\n",
    "            return x, z\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "class AE_no_P(nn.Module):\n",
    "    def __init__(self, encoder,generator):\n",
    "        super(AE_no_P,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        #x = torch.cat([x,p],axis=1)\n",
    "        x = self.generator(x)\n",
    "        return x\n",
    "    \n",
    "class AE_xhat_z(nn.Module):\n",
    "    def __init__(self, encoder,generator):\n",
    "        super(AE_xhat_z,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        x = self.generator(z)\n",
    "        return x, z\n",
    "    \n",
    "class AE_xhat_zV2(nn.Module):\n",
    "    def __init__(self, X, filters=32, latentDim=16, num_conv=2, repeat=0, \n",
    "                 skip_connection=False, stack=False, conv_k=3, last_k=3, act=nn.LeakyReLU, \n",
    "                 return_z=True, stream=True, device='cpu', norm = nn.BatchNorm2d, sigmoid_out=False):\n",
    "        super(AE_xhat_zV2,self).__init__()\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.latentDim = latentDim\n",
    "        self.num_conv = num_conv\n",
    "        self.repeat = repeat\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "        self.act = act\n",
    "        self.norm = norm\n",
    "        self.conv_k = 3\n",
    "        self.last_k = 3\n",
    "        self.device = device\n",
    "        self.return_z = return_z\n",
    "        self.stream = stream\n",
    "        \n",
    "        self.encoder = Encoder(X,filters,latentDim,num_conv=num_conv,norm=norm).to(device)\n",
    "        \n",
    "        z = self.encoder(X)\n",
    "        \n",
    "        if stream:\n",
    "            self.output_shape = torch.tensor(X[0][1:].shape)\n",
    "        else:\n",
    "            self.output_shape = torch.tensor(X[0].shape)\n",
    "\n",
    "        self.generator = Generator(z, filters, self.output_shape,\n",
    "                                   num_conv, conv_k, last_k, repeat, skip_connection, act=act,\n",
    "                                   stack=stack, norm=norm, sigmoid_out=sigmoid_out).to(device)\n",
    "        \n",
    "    def forward(self, x, p_x):\n",
    "        z = self.encoder(x)\n",
    "        z[:, -p_x.size(1):] = p_x\n",
    "        x = self.generator(z)\n",
    "        if self.return_z:\n",
    "            return x, z\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "class ConvDeconv(nn.Module):\n",
    "    def __init__(self, X, filters=32, latentDim=16, num_conv=2, repeat=1, \n",
    "                 skip_connection=False, stack=False, conv_k=3, last_k=3,\n",
    "                 act=nn.LeakyReLU(), return_z=True, stream=False, device='cpu'):\n",
    "        super(ConvDeconv,self).__init__()\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.latentDim = latentDim\n",
    "        self.num_conv = num_conv\n",
    "        self.repeat = repeat\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "        self.act = act\n",
    "        self.conv_k = 3\n",
    "        self.last_k = 3\n",
    "        self.device = device\n",
    "        self.return_z = return_z\n",
    "        self.stream = stream\n",
    "        \n",
    "        x_shape = X.shape\n",
    "        convLayers = [nn.BatchNorm2d(x_shape[1]),nn.Conv2d(x_shape[1],filters, kernel_size=conv_k,stride=2,padding=1)]\n",
    "        ch = filters\n",
    "        for idx in range(0,repeat):\n",
    "            convLayers.append(self.act)\n",
    "            convLayers.append(nn.BatchNorm2d(ch))\n",
    "            convLayers.append(nn.Conv2d(ch, ch+filters, kernel_size=conv_k,stride=2,padding=1))\n",
    "            ch += filters\n",
    "            \n",
    "            \n",
    "        self.encoder = nn.Sequential(*convLayers).to(device)\n",
    "        \n",
    "        z = self.encoder(X)\n",
    "        \n",
    "        if stream:\n",
    "            self.output_shape = torch.tensor(X[0][1:].shape)\n",
    "        else:\n",
    "            self.output_shape = torch.tensor(X[0].shape)\n",
    "        \n",
    "        #print(self.output_shape)\n",
    "        deconvLayers = []\n",
    "        for idx in range(0,repeat):\n",
    "            deconvLayers.append(nn.BatchNorm2d(ch))\n",
    "            deconvLayers.append(nn.ConvTranspose2d(ch,ch-filters, \n",
    "                                                   kernel_size=conv_k,stride=2,\n",
    "                                                   padding=1, output_padding=1))\n",
    "            deconvLayers.append(self.act)\n",
    "            ch -= filters\n",
    "        \n",
    "        deconvLayers.append(nn.BatchNorm2d(ch))\n",
    "        deconvLayers.append(nn.ConvTranspose2d(ch,int(self.output_shape[0]), \n",
    "                                               kernel_size=conv_k,stride=2,\n",
    "                                               padding=1, output_padding=1))  \n",
    "        \n",
    "        self.generator = nn.Sequential(*deconvLayers).to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        x = self.generator(z)\n",
    "        if self.return_z:\n",
    "            return x, z\n",
    "        else:\n",
    "            return x\n",
    " \n",
    "class ConvDeconvFactor2(nn.Module):\n",
    "    def __init__(self, X, filters=32, latentDim=16, num_conv=2, repeat=1, \n",
    "                 skip_connection=False, stack=False, conv_k=3, last_k=3,\n",
    "                 act=nn.LeakyReLU(), return_z=True, stream=False, device='cpu',\n",
    "                 use_sigmoid_output_layer=False):\n",
    "        super(ConvDeconvFactor2,self).__init__()\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.latentDim = latentDim\n",
    "        self.num_conv = num_conv\n",
    "        self.repeat = repeat\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "        self.act = act\n",
    "        self.conv_k = 3\n",
    "        self.last_k = 3\n",
    "        self.device = device\n",
    "        self.return_z = return_z\n",
    "        self.stream = stream\n",
    "        self.use_sigmoid_output_layer = use_sigmoid_output_layer\n",
    "        \n",
    "        x_shape = X.shape\n",
    "        convLayers = [nn.BatchNorm2d(x_shape[1]),nn.Conv2d(x_shape[1],filters, kernel_size=conv_k,stride=2,padding=1)]\n",
    "        ch = filters\n",
    "        for idx in range(0,repeat):\n",
    "            convLayers.append(self.act)\n",
    "            convLayers.append(nn.BatchNorm2d(ch))\n",
    "            convLayers.append(nn.Conv2d(ch, 2*ch, kernel_size=conv_k,stride=2,padding=1))\n",
    "            ch = 2*ch\n",
    "        \n",
    "        self.ch = ch\n",
    "        self.sz = int(X.shape[2]//(2**(repeat+1)))\n",
    "        self.encoder = nn.Sequential(*convLayers).to(device)\n",
    "        \n",
    "        z = self.encoder(X)\n",
    "    \n",
    "        if stream:\n",
    "            self.output_shape = torch.tensor(X[0][1:].shape)\n",
    "        else:\n",
    "            self.output_shape = torch.tensor(X[0].shape)\n",
    "        \n",
    "        #print(self.output_shape)\n",
    "        deconvLayers = []\n",
    "        for idx in range(0,repeat):\n",
    "            deconvLayers.append(nn.BatchNorm2d(ch))\n",
    "            deconvLayers.append(nn.ConvTranspose2d(ch,ch//2, \n",
    "                                                   kernel_size=conv_k,stride=2,\n",
    "                                                   padding=1, output_padding=1))\n",
    "            deconvLayers.append(self.act)\n",
    "            ch = ch//2\n",
    "        \n",
    "        deconvLayers.append(nn.BatchNorm2d(ch))\n",
    "        deconvLayers.append(nn.ConvTranspose2d(ch,int(self.output_shape[0]), \n",
    "                                               kernel_size=conv_k,stride=2,\n",
    "                                               padding=1, output_padding=1))  \n",
    "        \n",
    "        if use_sigmoid_output_layer:\n",
    "            deconvLayers.append(nn.Sigmoid())\n",
    "            \n",
    "        self.generator = nn.Sequential(*deconvLayers).to(device)\n",
    "        # input data must be square\n",
    "        self.downLinear = nn.Linear(self.ch*self.sz*self.sz,self.latentDim)\n",
    "        self.upLinear = nn.Linear(self.latentDim,self.ch*self.sz*self.sz)\n",
    "        \n",
    "    def forward(self, x, p_x):\n",
    "        z = self.encoder(x)\n",
    "        z = z.view(x.size(0),-1)\n",
    "        z = self.downLinear(z)\n",
    "        z[:, -p_x.size(1):] = p_x\n",
    "        x = self.upLinear(z)\n",
    "        x = x.view(x.size(0),self.ch,self.sz,self.sz)\n",
    "        x = self.generator(x)\n",
    "        if self.return_z:\n",
    "            return x, z\n",
    "        else:\n",
    "            return x        \n",
    "        \n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, X, hiddenLayerSizes = [1024], activation=nn.ELU()):\n",
    "        super(MLP,self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.inputSize = X.shape[1:]\n",
    "        self.modules = []\n",
    "        self.modules.append(nn.Linear(np.prod(self.inputSize),hiddenLayerSizes[0]))\n",
    "        self.modules.append(self.activation)\n",
    "        for idx,sz in enumerate(hiddenLayerSizes[:-1]):\n",
    "            self.modules.append(nn.Linear(hiddenLayerSizes[idx],hiddenLayerSizes[idx+1]))\n",
    "            self.modules.append(self.activation)\n",
    "                               \n",
    "        self.modules.append(nn.Linear(hiddenLayerSizes[-1],np.prod(self.inputSize)))\n",
    "        self.layers = nn.Sequential(*self.modules)\n",
    "                                \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = self.layers(x)\n",
    "        x = x.view(x.shape[0],self.inputSize[0],self.inputSize[1],self.inputSize[2])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bz = 8\n",
    "latentDim = 16\n",
    "filters = 128\n",
    "num_conv = 4\n",
    "num_Gconv= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 128, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn([bz, 1, 128, 128]) #, torch.Size([32, 3]))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_x = torch.ones((bz,2))\n",
    "p_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvDeconvFactor2(X, filters=128, latentDim=512, num_conv=2, repeat=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvDeconvFactor2(\n",
       "  (act): LeakyReLU(negative_slope=0.01)\n",
       "  (encoder): Sequential(\n",
       "    (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (generator): Sequential(\n",
       "    (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ConvTranspose2d(128, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  )\n",
       "  (downLinear): Linear(in_features=131072, out_features=512, bias=True)\n",
       "  (upLinear): Linear(in_features=512, out_features=131072, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 16)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ch, model.sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (2): LeakyReLU(negative_slope=0.01)\n",
       "  (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (5): LeakyReLU(negative_slope=0.01)\n",
       "  (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (2): LeakyReLU(negative_slope=0.01)\n",
       "  (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (5): LeakyReLU(negative_slope=0.01)\n",
       "  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): ConvTranspose2d(128, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512, 16, 16])\n",
      "torch.Size([8, 131072])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512, 16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 128, 128]), torch.Size([8, 512]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xhat, z = model(X,p_x)\n",
    "Xhat.shape, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 layers require gradients (unfrozen) out of 28 layers\n",
      "137,304,579 parameters require gradients (unfrozen) out of 137,304,579 parameters\n",
      "12 layers require gradients (unfrozen) out of 12 layers\n",
      "1,477,378 parameters require gradients (unfrozen) out of 1,477,378 parameters\n",
      "12 layers require gradients (unfrozen) out of 12 layers\n",
      "1,477,889 parameters require gradients (unfrozen) out of 1,477,889 parameters\n"
     ]
    }
   ],
   "source": [
    "printNumModelParams(model)\n",
    "printNumModelParams(model.encoder)\n",
    "printNumModelParams(model.generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "12 layers require gradients (unfrozen) out of 12 layers\n",
      "134,744,835 parameters require gradients (unfrozen) out of 134,744,835 parameters\n",
      "--------------------------------------------------------------------------------\n",
      "1\n",
      "20 layers require gradients (unfrozen) out of 20 layers\n",
      "67,964,803 parameters require gradients (unfrozen) out of 67,964,803 parameters\n",
      "--------------------------------------------------------------------------------\n",
      "2\n",
      "28 layers require gradients (unfrozen) out of 28 layers\n",
      "36,640,899 parameters require gradients (unfrozen) out of 36,640,899 parameters\n",
      "--------------------------------------------------------------------------------\n",
      "3\n",
      "36 layers require gradients (unfrozen) out of 36 layers\n",
      "29,239,939 parameters require gradients (unfrozen) out of 29,239,939 parameters\n",
      "--------------------------------------------------------------------------------\n",
      "4\n",
      "44 layers require gradients (unfrozen) out of 44 layers\n",
      "58,576,515 parameters require gradients (unfrozen) out of 58,576,515 parameters\n",
      "--------------------------------------------------------------------------------\n",
      "5\n",
      "52 layers require gradients (unfrozen) out of 52 layers\n",
      "205,379,203 parameters require gradients (unfrozen) out of 205,379,203 parameters\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for repeat in range(0,6):\n",
    "    model = ConvDeconvFactor2(X, filters=128, latentDim=128, num_conv=2, repeat=repeat)\n",
    "    print(repeat)\n",
    "    printNumModelParams(model)\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.rand([bz,3])\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convBlock(\n",
       "  (act): LeakyReLU(negative_slope=0.01)\n",
       "  (convs): Sequential(\n",
       "    (0): Conv2d(2, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.01)\n",
       "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = convBlock(num_conv=num_conv,in_channels=X.shape[1],filters=filters)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/widemann1/anaconda3/envs/torch2/lib/python3.7/site-packages/torch/nn/functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 130, 64, 48])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = C(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128, 8, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CTB = convTransBlock(num_conv=num_conv, filters=128, act=nn.LeakyReLU(), skip_connection=False, stack=True)\n",
    "XX = torch.randn([bz, 128,8,6]) #, torch.Size([32, 3]))\n",
    "XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 16, 12])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = CTB(XX)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (act): LeakyReLU(negative_slope=0.01)\n",
       "  (conv1): Conv2d(2, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (convs): Sequential(\n",
       "    (0): convBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): LeakyReLU(negative_slope=0.01)\n",
       "        (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (1): convBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): LeakyReLU(negative_slope=0.01)\n",
       "        (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (2): convBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): LeakyReLU(negative_slope=0.01)\n",
       "        (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (3): convBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): LeakyReLU(negative_slope=0.01)\n",
       "        (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (4): convBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(640, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): LeakyReLU(negative_slope=0.01)\n",
       "        (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=9216, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = Encoder(X,filters,latentDim,num_conv=num_conv)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 layers require gradients (unfrozen) out of 94 layers\n",
      "5,319,184 parameters require gradients (unfrozen) out of 5,319,184 parameters\n"
     ]
    }
   ],
   "source": [
    "printNumModelParams(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = E(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2, 128,  96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shape = torch.tensor(X.shape[1:])\n",
    "output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 19])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.randn(bz, latentDim + p.shape[1])\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 8, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (linear): Linear(in_features=19, out_features=6144, bias=True)\n",
       "  (convTransBlockLayers): Sequential(\n",
       "    (0): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (1): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (2): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (3): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lastConv): Conv2d(640, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = Generator(z, 128, output_shape,\n",
    "                 num_conv=2, conv_k=3, last_k=3, repeat=0, \n",
    "                 skip_connection=False, act=nn.LeakyReLU(),stack=True)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.output_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 layers require gradients (unfrozen) out of 28 layers\n",
      "2,200,834 parameters require gradients (unfrozen) out of 2,200,834 parameters\n"
     ]
    }
   ],
   "source": [
    "printNumModelParams(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 128, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = G(z)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "class GeneratorOld(nn.Module):\n",
    "    def __init__(self,z, filters, output_shape,\n",
    "                 num_conv=4, conv_k=3, last_k=3, repeat=0, \n",
    "                 skip_connection=False, act=nn.LeakyReLU()):\n",
    "        super(GeneratorOld,self).__init__()\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(torch.max(output_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        x0_shape = [filters] + [i//np.power(2, repeat_num-1) for i in output_shape[1:]]\n",
    "        self.x0_shape = x0_shape\n",
    "        num_output = int(np.prod(x0_shape))\n",
    "        self.linear = nn.Linear(z.shape[1], num_output)\n",
    "        convTransBlockLayers = []\n",
    "        for i in range(repeat_num-1):\n",
    "            convTransBlockLayers.append(convTransBlock(num_conv,filters,act,skip_connection))\n",
    "        self.convTransBlockLayers = nn.Sequential(*convTransBlockLayers)\n",
    "        self.lastConv = nn.Conv2d(filters,int(output_shape[0]),kernel_size=3, stride=1,padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,self.x0_shape[0],self.x0_shape[1],self.x0_shape[2])\n",
    "        x = self.convTransBlockLayers(x)\n",
    "        #x = self.lastConv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nbdev.export import notebook2script\n",
    "# notebook2script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following two cells show that sigmoid is working\n",
    "\n",
    "#### the output goes from having a max/min of .02/.01 to .5/.49, which is about the value of the sigmoid function near zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 8, 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 128, 128]),\n",
       " torch.Size([8, 16]),\n",
       " tensor(0.0019, grad_fn=<MaxBackward1>),\n",
       " tensor(-0.0056, grad_fn=<MinBackward1>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat = 0\n",
    "skip_connection = False\n",
    "stack = False\n",
    "createStreamFcn = False\n",
    "model = AE_xhat_zV2(X, filters, latentDim, num_conv, repeat, \n",
    "                 skip_connection, stack, conv_k=3, last_k=3, \n",
    "                 act=nn.LeakyReLU(), return_z=True, stream=createStreamFcn, device='cpu',\n",
    "                norm=nn.Identity, sigmoid_out = False)\n",
    "X_out, z_out = model(X,p_x)\n",
    "X_out.shape, z_out.shape, X_out.max(), X_out.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 8, 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 128, 128]),\n",
       " tensor(0.4997, grad_fn=<MaxBackward1>),\n",
       " tensor(0.4963, grad_fn=<MinBackward1>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat = 0\n",
    "skip_connection = False\n",
    "stack = False\n",
    "createStreamFcn = False\n",
    "model = AE_xhat_zV2(X, filters, latentDim, num_conv, repeat, \n",
    "                 skip_connection, stack, conv_k=3, last_k=3, \n",
    "                 act=nn.LeakyReLU(), return_z=True, stream=createStreamFcn, device='cpu',\n",
    "                norm=nn.Identity, sigmoid_out = True)\n",
    "X_out, z_out = model(X,p_x)\n",
    "X_out.shape, X_out.max(), X_out.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "repeat = 0\n",
    "skip_connection = False\n",
    "stack = False\n",
    "createStreamFcn = False\n",
    "model = AE_xhat_zV2(X, filters, latentDim, num_conv, repeat, \n",
    "                 skip_connection, stack, conv_k=3, last_k=3, \n",
    "                 act=nn.LeakyReLU(), return_z=True, stream=createStreamFcn, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X,p_x)[1][:,-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = E(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 128, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 2, 128, 96]), torch.Size([8, 16]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xhat,p = model(X)\n",
    "Xhat.shape,p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss Function\n",
    "loss_func = torch.nn.MSELoss()\n",
    "loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0010, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mseVal = loss_func(X,Xhat)\n",
    "mseVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build AE_no_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = E(X)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 8, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (linear): Linear(in_features=16, out_features=6144, bias=True)\n",
       "  (convTransBlockLayers): Sequential(\n",
       "    (0): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (1): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (2): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (3): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lastConv): Conv2d(640, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = Generator(z, 128, output_shape,\n",
    "                 num_conv=2, conv_k=3, last_k=3, repeat=0, \n",
    "                 skip_connection=False, act=nn.LeakyReLU(),stack=True)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AE_no_P(E,G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 128, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build AE_xhat_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AE_xhat_z(\n",
       "  (encoder): Encoder(\n",
       "    (act): LeakyReLU(negative_slope=0.01)\n",
       "    (conv1): Conv2d(2, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (convs): Sequential(\n",
       "      (0): convBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.01)\n",
       "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): LeakyReLU(negative_slope=0.01)\n",
       "          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (11): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (1): convBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.01)\n",
       "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): LeakyReLU(negative_slope=0.01)\n",
       "          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (11): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (2): convBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.01)\n",
       "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): LeakyReLU(negative_slope=0.01)\n",
       "          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (11): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (3): convBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.01)\n",
       "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): LeakyReLU(negative_slope=0.01)\n",
       "          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (11): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (4): convBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(640, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.01)\n",
       "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): LeakyReLU(negative_slope=0.01)\n",
       "          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (11): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=9216, out_features=16, bias=True)\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (linear): Linear(in_features=16, out_features=6144, bias=True)\n",
       "    (convTransBlockLayers): Sequential(\n",
       "      (0): convTransBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (seq): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (4): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (1): convTransBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (seq): Sequential(\n",
       "          (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (4): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (2): convTransBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (seq): Sequential(\n",
       "          (0): ConvTranspose2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (4): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (3): convTransBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (seq): Sequential(\n",
       "          (0): ConvTranspose2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (4): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lastConv): Conv2d(640, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = AE_xhat_z(E,G)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = nn.ConvTranspose1d(1,1,kernel_size=13,stride=4,padding=6,output_padding=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 32, 24])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = nn.Conv2d(2,2,kernel_size=13,stride=4,padding=6)\n",
    "out = c(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 8, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = c(out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 2, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = c(out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_shape = X.shape[1:]\n",
    "repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2\n",
    "repeat_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder_LK(\n",
       "  (act): LeakyReLU(negative_slope=0.01)\n",
       "  (bn0): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv1): Conv2d(2, 16, kernel_size=(13, 13), stride=(4, 4), padding=(6, 6))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 64, kernel_size=(13, 13), stride=(4, 4), padding=(6, 6))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=3072, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = Encoder_LK(X,16,16)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = E(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Decoder_LK(X,16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat = D(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 128, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = AE_LK(E,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat,z = M(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 2, 128, 96]), torch.Size([8, 16]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xhat.shape,z.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3521, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(X,xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
