{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T21:56:02.150389Z",
     "start_time": "2020-11-06T21:56:02.147485Z"
    }
   },
   "outputs": [],
   "source": [
    "# default_exp models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T21:56:03.575555Z",
     "start_time": "2020-11-06T21:56:02.512666Z"
    }
   },
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from surrogates4sims.utils import printNumModelParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Fluids Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T21:56:03.645117Z",
     "start_time": "2020-11-06T21:56:03.577713Z"
    }
   },
   "outputs": [],
   "source": [
    "# EXPORT \n",
    "class convBlock(nn.Module):\n",
    "    def __init__(self,num_conv=4,in_channels=128,filters=128,act=nn.LeakyReLU(),downSample=True,\n",
    "                     norm=nn.BatchNorm2d):\n",
    "        super(convBlock,self).__init__()\n",
    "        self.num_conv = num_conv\n",
    "        self.in_channels = in_channels\n",
    "        self.act = act\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(num_conv):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Conv2d(in_channels, filters, kernel_size=3, stride=1,padding=1))\n",
    "                layers.append(norm(filters))\n",
    "                layers.append(act)\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(filters, filters, kernel_size=3, stride=1,padding=1))\n",
    "                layers.append(norm(filters))\n",
    "                layers.append(act)\n",
    "        self.convs = nn.Sequential(*layers)\n",
    "        #self.downSampleLayer = nn.Conv2d(in_channels+filters, in_channels+filters,kernel_size=3,stride=2,padding=1)\n",
    "        self.downSampleLayer = nn.Conv2d(filters, filters,kernel_size=3,stride=2,padding=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        x0 = x\n",
    "        x = self.convs(x)\n",
    "        #x = torch.cat([x,x0],axis=1)\n",
    "        x = self.downSampleLayer(x)\n",
    "        x = torch.cat([x,F.interpolate(x0,scale_factor=.5)],axis=1)\n",
    "        return x\n",
    "    \n",
    "class convTransBlock(nn.Module):\n",
    "    def __init__(self, num_conv=4, in_channels=128, filters=128, act=nn.LeakyReLU(),\n",
    "                 skip_connection=False, stack=False, norm=nn.BatchNorm2d):\n",
    "        super(convTransBlock,self).__init__()\n",
    "        self.filters = filters\n",
    "        self.num_conv = num_conv\n",
    "        self.in_channels = in_channels\n",
    "        self.act = act\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "        self.upsample = torch.nn.modules.Upsample(scale_factor=2)\n",
    "        layers = []\n",
    "        for i in range(num_conv-1):\n",
    "            if i == 0:\n",
    "                layers.append(nn.ConvTranspose2d(in_channels,out_channels=filters, kernel_size=3, stride=1,padding=1))\n",
    "                layers.append(norm(filters))\n",
    "                layers.append(act)\n",
    "            else:\n",
    "                layers.append(nn.ConvTranspose2d(filters,out_channels=filters, kernel_size=3, stride=1,padding=1))\n",
    "                layers.append(norm(filters))\n",
    "                layers.append(act)                \n",
    "        layers.append(nn.ConvTranspose2d(filters, out_channels=filters, kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1))\n",
    "        layers.append(act)\n",
    "        self.seq = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x0 = x\n",
    "        x = self.seq(x)\n",
    "        if self.skip_connection:\n",
    "            x += self.upsample(x0)\n",
    "        if self.stack:\n",
    "            x = torch.cat([x, self.upsample(x0)], axis=1)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,z, filters, output_shape,\n",
    "                 num_conv=4, conv_k=3, last_k=3, repeat=0, \n",
    "                 skip_connection=False, act=nn.LeakyReLU(),stack=False, norm=nn.BatchNorm2d, sigmoid_out=False):\n",
    "        super(Generator,self).__init__()\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(torch.max(output_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        x0_shape = [filters] + [int(i//np.power(2, repeat_num-1)) for i in output_shape[1:]]\n",
    "        print(x0_shape)\n",
    "        self.x0_shape = x0_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.filters = filters\n",
    "        self.num_conv = num_conv\n",
    "        num_output = int(np.prod(x0_shape))\n",
    "        self.num_output = num_output\n",
    "        self.linear = nn.Linear(z.shape[1], num_output)\n",
    "        \n",
    "        convTransBlockLayers = []\n",
    "        ch = filters\n",
    "        for i in range(repeat_num-1):\n",
    "            #print(ch)\n",
    "            convTransBlockLayers.append(convTransBlock(num_conv, ch, filters, act, skip_connection, stack,\n",
    "                                                      norm=norm))\n",
    "            if stack:\n",
    "                ch += filters\n",
    "\n",
    "        self.convTransBlockLayers = nn.Sequential(*convTransBlockLayers)\n",
    "        if ch > filters:\n",
    "            n = ch\n",
    "        else:\n",
    "            n = filters\n",
    "        if sigmoid_out:\n",
    "            self.lastConv = nn.Sequential(nn.Conv2d(n,int(output_shape[0]),kernel_size=3, stride=1,padding=1),\n",
    "                                          nn.Sigmoid())\n",
    "        else:\n",
    "            self.lastConv = nn.Conv2d(n,int(output_shape[0]),kernel_size=3, stride=1,padding=1)\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,self.x0_shape[0],self.x0_shape[1],self.x0_shape[2])\n",
    "        x = self.convTransBlockLayers(x)\n",
    "        x = self.lastConv(x)\n",
    "        return x\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, x, filters, z_num, num_conv=4, conv_k=3, repeat=0, act=nn.LeakyReLU(), norm=nn.BatchNorm2d):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        x_shape = x.shape[1:]\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.act = act\n",
    "        self.conv1 = nn.Conv2d(x_shape[0], filters, kernel_size=conv_k, stride=1,padding=1)\n",
    "        \n",
    "        ch = filters\n",
    "        convLayers = []\n",
    "        for idx in range(0,repeat_num):\n",
    "            convLayers.append(convBlock(num_conv, ch, filters, act=nn.LeakyReLU(), downSample=True, norm=norm))\n",
    "            ch += filters\n",
    "            \n",
    "        self.convs = nn.Sequential(*convLayers)\n",
    "        h = [i//2**repeat_num for i in x_shape[1:]]\n",
    "        self.nLinearInput = (ch)*np.prod(h)\n",
    "        self.linear = nn.Linear(self.nLinearInput,z_num)\n",
    "                             \n",
    "    def forward(self,x):\n",
    "        x = self.act(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.convs(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class Encoder_LK(nn.Module):\n",
    "    def __init__(self, x, filters, z_num, repeat=0, act=nn.LeakyReLU()):\n",
    "        super(Encoder_LK,self).__init__()\n",
    "        \n",
    "        x_shape = x.shape[1:]\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.act = act\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(x_shape[0])\n",
    "        self.conv1 = nn.Conv2d(x_shape[0], filters, kernel_size=13,stride=4,padding=6)\n",
    "        self.bn1 = nn.BatchNorm2d(filters)\n",
    "        self.conv2 = nn.Conv2d(filters, 4*filters, kernel_size=13,stride=4,padding=6)\n",
    "        self.bn2 = nn.BatchNorm2d(4*filters)\n",
    "        self.z_num = z_num\n",
    "        self.nLinearInput = 4*filters*8*6\n",
    "        self.linear = nn.Linear(self.nLinearInput,z_num)\n",
    "                             \n",
    "    def forward(self,x):\n",
    "        x = self.bn0(x)\n",
    "        x = self.act(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class Decoder_LK(nn.Module):\n",
    "    def __init__(self, x, filters, z_num, repeat=0, act=nn.LeakyReLU()):\n",
    "        super(Decoder_LK,self).__init__()\n",
    "        \n",
    "        x_shape = x.shape[1:]\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.act = act\n",
    "        self.bn0 = nn.BatchNorm2d(filters)\n",
    "        self.conv1 = nn.ConvTranspose2d(filters,filters,kernel_size=25,stride=4,padding=11,output_padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(filters,4*filters,kernel_size=25,stride=4,padding=11,output_padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(4*filters)\n",
    "        self.conv3 = nn.Conv2d(4*filters,x_shape[0], kernel_size=3,stride=1,padding=1)\n",
    "        self.z_num = z_num\n",
    "        self.nLinearOutput = filters*8*6\n",
    "        self.linear = nn.Linear(z_num,self.nLinearOutput)\n",
    "                             \n",
    "    def forward(self,x):\n",
    "        x = self.act(self.linear(x))\n",
    "        x = x.view(x.size(0),self.filters,8,6)\n",
    "        x = self.bn0(x)\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.bn0(x)\n",
    "        #print(x.shape)\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "    \n",
    "class AE_LK(nn.Module):\n",
    "    def __init__(self, encoder_LK, decoder_LK, return_z=True):\n",
    "        super(AE_LK,self).__init__()\n",
    "        self.encoder = encoder_LK\n",
    "        self.generator = decoder_LK\n",
    "        self.return_z = return_z\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        x = self.generator(z)\n",
    "        if self.return_z:\n",
    "            return x, z\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "class AE_no_P(nn.Module):\n",
    "    def __init__(self, encoder,generator):\n",
    "        super(AE_no_P,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        #x = torch.cat([x,p],axis=1)\n",
    "        x = self.generator(x)\n",
    "        return x\n",
    "    \n",
    "class AE_xhat_z(nn.Module):\n",
    "    def __init__(self, encoder,generator):\n",
    "        super(AE_xhat_z,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        x = self.generator(z)\n",
    "        return x, z\n",
    "    \n",
    "class AE_xhat_zV2(nn.Module):\n",
    "    def __init__(self, X, filters=32, latentDim=16, num_conv=2, repeat=0, \n",
    "                 skip_connection=False, stack=False, conv_k=3, last_k=3, act=nn.LeakyReLU, \n",
    "                 return_z=True, stream=True, device='cpu', norm = nn.BatchNorm2d, sigmoid_out=False):\n",
    "        super(AE_xhat_zV2,self).__init__()\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.latentDim = latentDim\n",
    "        self.num_conv = num_conv\n",
    "        self.repeat = repeat\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "        self.act = act\n",
    "        self.norm = norm\n",
    "        self.conv_k = 3\n",
    "        self.last_k = 3\n",
    "        self.device = device\n",
    "        self.return_z = return_z\n",
    "        self.stream = stream\n",
    "        \n",
    "        self.encoder = Encoder(X,filters,latentDim,num_conv=num_conv,norm=norm).to(device)\n",
    "        \n",
    "        z = self.encoder(X)\n",
    "        \n",
    "        if stream:\n",
    "            self.output_shape = torch.tensor(X[0][1:].shape)\n",
    "        else:\n",
    "            self.output_shape = torch.tensor(X[0].shape)\n",
    "\n",
    "        self.generator = Generator(z, filters, self.output_shape,\n",
    "                                   num_conv, conv_k, last_k, repeat, skip_connection, act=act,\n",
    "                                   stack=stack, norm=norm, sigmoid_out=sigmoid_out).to(device)\n",
    "        \n",
    "    def forward(self, x, p_x):\n",
    "        z = self.encoder(x)\n",
    "        z[:, -p_x.size(1):] = p_x\n",
    "        x = self.generator(z)\n",
    "        if self.return_z:\n",
    "            return x, z\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "class ConvDeconv(nn.Module):\n",
    "    def __init__(self, X, filters=32, latentDim=16, num_conv=2, repeat=1, \n",
    "                 skip_connection=False, stack=False, conv_k=3, last_k=3,\n",
    "                 act=nn.LeakyReLU(), return_z=True, stream=False, device='cpu'):\n",
    "        super(ConvDeconv,self).__init__()\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.latentDim = latentDim\n",
    "        self.num_conv = num_conv\n",
    "        self.repeat = repeat\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "        self.act = act\n",
    "        self.conv_k = 3\n",
    "        self.last_k = 3\n",
    "        self.device = device\n",
    "        self.return_z = return_z\n",
    "        self.stream = stream\n",
    "        \n",
    "        x_shape = X.shape\n",
    "        convLayers = [nn.BatchNorm2d(x_shape[1]),nn.Conv2d(x_shape[1],filters, kernel_size=conv_k,stride=2,padding=1)]\n",
    "        ch = filters\n",
    "        for idx in range(0,repeat):\n",
    "            convLayers.append(self.act)\n",
    "            convLayers.append(nn.BatchNorm2d(ch))\n",
    "            convLayers.append(nn.Conv2d(ch, ch+filters, kernel_size=conv_k,stride=2,padding=1))\n",
    "            ch += filters\n",
    "            \n",
    "            \n",
    "        self.encoder = nn.Sequential(*convLayers).to(device)\n",
    "        \n",
    "        z = self.encoder(X)\n",
    "        \n",
    "        if stream:\n",
    "            self.output_shape = torch.tensor(X[0][1:].shape)\n",
    "        else:\n",
    "            self.output_shape = torch.tensor(X[0].shape)\n",
    "        \n",
    "        #print(self.output_shape)\n",
    "        deconvLayers = []\n",
    "        for idx in range(0,repeat):\n",
    "            deconvLayers.append(nn.BatchNorm2d(ch))\n",
    "            deconvLayers.append(nn.ConvTranspose2d(ch,ch-filters, \n",
    "                                                   kernel_size=conv_k,stride=2,\n",
    "                                                   padding=1, output_padding=1))\n",
    "            deconvLayers.append(self.act)\n",
    "            ch -= filters\n",
    "        \n",
    "        deconvLayers.append(nn.BatchNorm2d(ch))\n",
    "        deconvLayers.append(nn.ConvTranspose2d(ch,int(self.output_shape[0]), \n",
    "                                               kernel_size=conv_k,stride=2,\n",
    "                                               padding=1, output_padding=1))  \n",
    "        \n",
    "        self.generator = nn.Sequential(*deconvLayers).to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        x = self.generator(z)\n",
    "        if self.return_z:\n",
    "            return x, z\n",
    "        else:\n",
    "            return x\n",
    " \n",
    "\n",
    "class ConvDeconvFactor2(nn.Module):\n",
    "    def __init__(self, X, filters=32, latentDim=16, num_conv=2, repeat=1, \n",
    "                 skip_connection=False, stack=False, conv_k=3, last_k=3,\n",
    "                 act=nn.LeakyReLU(), return_z=True, stream=False, device='cpu'):\n",
    "        super(ConvDeconvFactor2,self).__init__()\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.latentDim = latentDim\n",
    "        self.num_conv = num_conv\n",
    "        self.repeat = repeat\n",
    "        self.skip_connection = skip_connection\n",
    "        self.stack = stack\n",
    "        self.act = act\n",
    "        self.conv_k = 3\n",
    "        self.last_k = 3\n",
    "        self.device = device\n",
    "        self.return_z = return_z\n",
    "        self.stream = stream\n",
    "        \n",
    "        x_shape = X.shape\n",
    "        convLayers = [nn.BatchNorm2d(x_shape[1]),nn.Conv2d(x_shape[1],filters, kernel_size=conv_k,stride=2,padding=1)]\n",
    "        ch = filters\n",
    "        for idx in range(0,repeat):\n",
    "            convLayers.append(self.act)\n",
    "            convLayers.append(nn.BatchNorm2d(ch))\n",
    "            convLayers.append(nn.Conv2d(ch, 2*ch, kernel_size=conv_k,stride=2,padding=1))\n",
    "            ch = 2*ch\n",
    "            \n",
    "            \n",
    "        self.encoder = nn.Sequential(*convLayers).to(device)\n",
    "        \n",
    "        z = self.encoder(X)\n",
    "        \n",
    "        if stream:\n",
    "            self.output_shape = torch.tensor(X[0][1:].shape)\n",
    "        else:\n",
    "            self.output_shape = torch.tensor(X[0].shape)\n",
    "        \n",
    "        #print(self.output_shape)\n",
    "        deconvLayers = []\n",
    "        for idx in range(0,repeat):\n",
    "            deconvLayers.append(nn.BatchNorm2d(ch))\n",
    "            deconvLayers.append(nn.ConvTranspose2d(ch,ch//2, \n",
    "                                                   kernel_size=conv_k,stride=2,\n",
    "                                                   padding=1, output_padding=1))\n",
    "            deconvLayers.append(self.act)\n",
    "            ch = ch//2\n",
    "        \n",
    "        deconvLayers.append(nn.BatchNorm2d(ch))\n",
    "        deconvLayers.append(nn.ConvTranspose2d(ch,int(self.output_shape[0]), \n",
    "                                               kernel_size=conv_k,stride=2,\n",
    "                                               padding=1, output_padding=1))  \n",
    "        \n",
    "        self.generator = nn.Sequential(*deconvLayers).to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        x = self.generator(z)\n",
    "        if self.return_z:\n",
    "            return x, z\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, X, hiddenLayerSizes = [1024], activation=nn.ELU()):\n",
    "        super(MLP,self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.inputSize = X.shape[1:]\n",
    "        self.modules = []\n",
    "        self.modules.append(nn.Linear(np.prod(self.inputSize),hiddenLayerSizes[0]))\n",
    "        self.modules.append(self.activation)\n",
    "        for idx,sz in enumerate(hiddenLayerSizes[:-1]):\n",
    "            self.modules.append(nn.Linear(hiddenLayerSizes[idx],hiddenLayerSizes[idx+1]))\n",
    "            self.modules.append(self.activation)\n",
    "                               \n",
    "        self.modules.append(nn.Linear(hiddenLayerSizes[-1],np.prod(self.inputSize)))\n",
    "        self.layers = nn.Sequential(*self.modules)\n",
    "                                \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = self.layers(x)\n",
    "        x = x.view(x.shape[0],self.inputSize[0],self.inputSize[1],self.inputSize[2])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T21:56:03.655183Z",
     "start_time": "2020-11-06T21:56:03.646667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 128, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(10,1,128,128)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T21:56:03.880997Z",
     "start_time": "2020-11-06T21:56:03.656749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (activation): Tanh()\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=16384, out_features=1024, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=1024, out_features=16384, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = MLP(X, hiddenLayerSizes = [1024,1024], activation=nn.Tanh())\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T21:56:04.520756Z",
     "start_time": "2020-11-06T21:56:04.490661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 128, 128])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = A(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T21:56:05.523913Z",
     "start_time": "2020-11-06T21:56:05.519966Z"
    }
   },
   "outputs": [],
   "source": [
    "bz = 8\n",
    "latentDim = 16\n",
    "filters = 128\n",
    "num_conv = 4\n",
    "num_Gconv= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T21:49:04.188980Z",
     "start_time": "2020-11-06T21:49:04.176918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 128, 96])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn([bz, 2, 128, 96]) #, torch.Size([32, 3]))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T21:49:04.865698Z",
     "start_time": "2020-11-06T21:49:04.858814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  8,   2, 128,  96])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shape = torch.tensor(X.shape)\n",
    "output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvDeconvFactor2(X, filters=128, latentDim=512, num_conv=2, repeat=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): Conv2d(2, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (2): LeakyReLU(negative_slope=0.01)\n",
       "  (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (5): LeakyReLU(negative_slope=0.01)\n",
       "  (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (8): LeakyReLU(negative_slope=0.01)\n",
       "  (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (11): LeakyReLU(negative_slope=0.01)\n",
       "  (12): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): ConvTranspose2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (2): LeakyReLU(negative_slope=0.01)\n",
       "  (3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (5): LeakyReLU(negative_slope=0.01)\n",
       "  (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (8): LeakyReLU(negative_slope=0.01)\n",
       "  (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (11): LeakyReLU(negative_slope=0.01)\n",
       "  (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): ConvTranspose2d(128, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 2, 128, 96]), torch.Size([8, 2048, 4, 3]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xhat, z = model(X)\n",
    "Xhat.shape, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2, 128,  96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shape = torch.tensor(X[0].shape)\n",
    "output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.rand([bz,3])\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convBlock(\n",
       "  (act): LeakyReLU(negative_slope=0.01)\n",
       "  (convs): Sequential(\n",
       "    (0): Conv2d(2, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.01)\n",
       "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = convBlock(num_conv=num_conv,in_channels=X.shape[1],filters=filters)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 130, 64, 48])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = C(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128, 8, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CTB = convTransBlock(num_conv=num_conv, filters=128, act=nn.LeakyReLU(), skip_connection=False, stack=True)\n",
    "XX = torch.randn([bz, 128,8,6]) #, torch.Size([32, 3]))\n",
    "XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 16, 12])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = CTB(XX)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (act): LeakyReLU(negative_slope=0.01)\n",
       "  (conv1): Conv2d(2, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (convs): Sequential(\n",
       "    (0): convBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): LeakyReLU(negative_slope=0.01)\n",
       "        (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (1): convBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): LeakyReLU(negative_slope=0.01)\n",
       "        (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (2): convBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): LeakyReLU(negative_slope=0.01)\n",
       "        (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (3): convBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): LeakyReLU(negative_slope=0.01)\n",
       "        (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (4): convBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(640, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): LeakyReLU(negative_slope=0.01)\n",
       "        (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=9216, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = Encoder(X,filters,latentDim,num_conv=num_conv)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 layers require gradients (unfrozen) out of 94 layers\n",
      "5,319,184 parameters require gradients (unfrozen) out of 5,319,184 parameters\n"
     ]
    }
   ],
   "source": [
    "printNumModelParams(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = E(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2, 128,  96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shape = torch.tensor(X.shape[1:])\n",
    "output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 19])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.randn(bz, latentDim + p.shape[1])\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 8, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (linear): Linear(in_features=19, out_features=6144, bias=True)\n",
       "  (convTransBlockLayers): Sequential(\n",
       "    (0): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (1): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (2): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (3): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lastConv): Conv2d(640, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = Generator(z, 128, output_shape,\n",
    "                 num_conv=2, conv_k=3, last_k=3, repeat=0, \n",
    "                 skip_connection=False, act=nn.LeakyReLU(),stack=True)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.output_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 layers require gradients (unfrozen) out of 28 layers\n",
      "2,200,834 parameters require gradients (unfrozen) out of 2,200,834 parameters\n"
     ]
    }
   ],
   "source": [
    "printNumModelParams(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 128, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = G(z)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "class GeneratorOld(nn.Module):\n",
    "    def __init__(self,z, filters, output_shape,\n",
    "                 num_conv=4, conv_k=3, last_k=3, repeat=0, \n",
    "                 skip_connection=False, act=nn.LeakyReLU()):\n",
    "        super(GeneratorOld,self).__init__()\n",
    "        if repeat == 0:\n",
    "            repeat_num = int(np.log2(torch.max(output_shape[1:]))) - 2\n",
    "        else:\n",
    "            repeat_num = repeat\n",
    "        x0_shape = [filters] + [i//np.power(2, repeat_num-1) for i in output_shape[1:]]\n",
    "        self.x0_shape = x0_shape\n",
    "        num_output = int(np.prod(x0_shape))\n",
    "        self.linear = nn.Linear(z.shape[1], num_output)\n",
    "        convTransBlockLayers = []\n",
    "        for i in range(repeat_num-1):\n",
    "            convTransBlockLayers.append(convTransBlock(num_conv,filters,act,skip_connection))\n",
    "        self.convTransBlockLayers = nn.Sequential(*convTransBlockLayers)\n",
    "        self.lastConv = nn.Conv2d(filters,int(output_shape[0]),kernel_size=3, stride=1,padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,self.x0_shape[0],self.x0_shape[1],self.x0_shape[2])\n",
    "        x = self.convTransBlockLayers(x)\n",
    "        #x = self.lastConv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nbdev.export import notebook2script\n",
    "# notebook2script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following two cells show that sigmoid is working\n",
    "\n",
    "#### the output goes from having a max/min of .02/.01 to .5/.49, which is about the value of the sigmoid function near zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T22:02:50.021767Z",
     "start_time": "2020-11-06T22:02:47.444603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 8, 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1, 128, 128]),\n",
       " tensor(0.0213, grad_fn=<MaxBackward1>),\n",
       " tensor(0.0107, grad_fn=<MinBackward1>))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat = 0\n",
    "skip_connection = False\n",
    "stack = False\n",
    "createStreamFcn = False\n",
    "model = AE_xhat_zV2(X, filters, latentDim, num_conv, repeat, \n",
    "                 skip_connection, stack, conv_k=3, last_k=3, \n",
    "                 act=nn.LeakyReLU(), return_z=True, stream=createStreamFcn, device='cpu',\n",
    "                norm=nn.Identity, sigmoid_out = False)\n",
    "X_out, z_out = model(X,torch.ones((10,2)))\n",
    "X_out.shape, X_out.max(), X_out.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T22:02:52.663017Z",
     "start_time": "2020-11-06T22:02:50.022888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 8, 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1, 128, 128]),\n",
       " tensor(0.5030, grad_fn=<MaxBackward1>),\n",
       " tensor(0.4991, grad_fn=<MinBackward1>))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat = 0\n",
    "skip_connection = False\n",
    "stack = False\n",
    "createStreamFcn = False\n",
    "model = AE_xhat_zV2(X, filters, latentDim, num_conv, repeat, \n",
    "                 skip_connection, stack, conv_k=3, last_k=3, \n",
    "                 act=nn.LeakyReLU(), return_z=True, stream=createStreamFcn, device='cpu',\n",
    "                norm=nn.Identity, sigmoid_out = True)\n",
    "X_out, z_out = model(X,torch.ones((10,2)))\n",
    "X_out.shape, X_out.max(), X_out.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T21:54:46.719467Z",
     "start_time": "2020-11-06T21:54:45.513009Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartoldson1/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "repeat = 0\n",
    "skip_connection = False\n",
    "stack = False\n",
    "createStreamFcn = False\n",
    "model = AE_xhat_zV2(X, filters, latentDim, num_conv, repeat, \n",
    "                 skip_connection, stack, conv_k=3, last_k=3, \n",
    "                 act=nn.LeakyReLU(), return_z=True, stream=createStreamFcn, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T21:54:51.801045Z",
     "start_time": "2020-11-06T21:54:49.927177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X,torch.ones((10,2)))[1][:,-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = E(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 128, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 2, 128, 96]), torch.Size([8, 16]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xhat,p = model(X)\n",
    "Xhat.shape,p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss Function\n",
    "loss_func = torch.nn.MSELoss()\n",
    "loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0010, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mseVal = loss_func(X,Xhat)\n",
    "mseVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build AE_no_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = E(X)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 8, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (linear): Linear(in_features=16, out_features=6144, bias=True)\n",
       "  (convTransBlockLayers): Sequential(\n",
       "    (0): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (1): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (2): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (3): convTransBlock(\n",
       "      (act): LeakyReLU(negative_slope=0.01)\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (seq): Sequential(\n",
       "        (0): ConvTranspose2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (4): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lastConv): Conv2d(640, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = Generator(z, 128, output_shape,\n",
    "                 num_conv=2, conv_k=3, last_k=3, repeat=0, \n",
    "                 skip_connection=False, act=nn.LeakyReLU(),stack=True)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AE_no_P(E,G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 128, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build AE_xhat_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AE_xhat_z(\n",
       "  (encoder): Encoder(\n",
       "    (act): LeakyReLU(negative_slope=0.01)\n",
       "    (conv1): Conv2d(2, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (convs): Sequential(\n",
       "      (0): convBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.01)\n",
       "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): LeakyReLU(negative_slope=0.01)\n",
       "          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (11): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (1): convBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.01)\n",
       "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): LeakyReLU(negative_slope=0.01)\n",
       "          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (11): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (2): convBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.01)\n",
       "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): LeakyReLU(negative_slope=0.01)\n",
       "          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (11): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (3): convBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.01)\n",
       "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): LeakyReLU(negative_slope=0.01)\n",
       "          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (11): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (4): convBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(640, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.01)\n",
       "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): LeakyReLU(negative_slope=0.01)\n",
       "          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (11): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (downSampleLayer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=9216, out_features=16, bias=True)\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (linear): Linear(in_features=16, out_features=6144, bias=True)\n",
       "    (convTransBlockLayers): Sequential(\n",
       "      (0): convTransBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (seq): Sequential(\n",
       "          (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (4): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (1): convTransBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (seq): Sequential(\n",
       "          (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (4): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (2): convTransBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (seq): Sequential(\n",
       "          (0): ConvTranspose2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (4): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (3): convTransBlock(\n",
       "        (act): LeakyReLU(negative_slope=0.01)\n",
       "        (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (seq): Sequential(\n",
       "          (0): ConvTranspose2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (4): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lastConv): Conv2d(640, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = AE_xhat_z(E,G)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = nn.ConvTranspose1d(1,1,kernel_size=13,stride=4,padding=6,output_padding=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 32, 24])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = nn.Conv2d(2,2,kernel_size=13,stride=4,padding=6)\n",
    "out = c(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 8, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = c(out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 2, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = c(out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_shape = X.shape[1:]\n",
    "repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2\n",
    "repeat_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder_LK(\n",
       "  (act): LeakyReLU(negative_slope=0.01)\n",
       "  (bn0): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv1): Conv2d(2, 16, kernel_size=(13, 13), stride=(4, 4), padding=(6, 6))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 64, kernel_size=(13, 13), stride=(4, 4), padding=(6, 6))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=3072, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = Encoder_LK(X,16,16)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = E(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Decoder_LK(X,16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat = D(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 128, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = AE_LK(E,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat,z = M(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 2, 128, 96]), torch.Size([8, 16]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xhat.shape,z.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3521, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(X,xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
