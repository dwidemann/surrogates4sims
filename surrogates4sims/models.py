# AUTOGENERATED! DO NOT EDIT! File to edit: 02_models.ipynb (unless otherwise specified).

__all__ = ['convBlock', 'convTransBlock', 'Generator', 'Encoder', 'Encoder_LK', 'Decoder_LK', 'AE_LK', 'AE_no_P',
           'AE_xhat_z', 'AE_xhat_zV2', 'ConvDeconv', 'ConvDeconvFactor2', 'MLP', 'GeneratorOld']

# Cell
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

from .utils import printNumModelParams

# Cell
class convBlock(nn.Module):
    def __init__(self,num_conv=4,in_channels=128,filters=128,act=nn.LeakyReLU(),downSample=True,
                     norm=nn.BatchNorm2d):
        super(convBlock,self).__init__()
        self.num_conv = num_conv
        self.in_channels = in_channels
        self.act = act

        layers = []
        for i in range(num_conv):
            if i == 0:
                layers.append(nn.Conv2d(in_channels, filters, kernel_size=3, stride=1,padding=1))
                layers.append(norm(filters))
                layers.append(act)
            else:
                layers.append(nn.Conv2d(filters, filters, kernel_size=3, stride=1,padding=1))
                layers.append(norm(filters))
                layers.append(act)
        self.convs = nn.Sequential(*layers)
        #self.downSampleLayer = nn.Conv2d(in_channels+filters, in_channels+filters,kernel_size=3,stride=2,padding=1)
        self.downSampleLayer = nn.Conv2d(filters, filters,kernel_size=3,stride=2,padding=1)

    def forward(self,x):
        #print(x.shape)
        x0 = x
        x = self.convs(x)
        #x = torch.cat([x,x0],axis=1)
        x = self.downSampleLayer(x)
        x = torch.cat([x,F.interpolate(x0,scale_factor=.5)],axis=1)
        return x

class convTransBlock(nn.Module):
    def __init__(self, num_conv=4, in_channels=128, filters=128, act=nn.LeakyReLU(),
                 skip_connection=False, stack=False, norm=nn.BatchNorm2d):
        super(convTransBlock,self).__init__()
        self.filters = filters
        self.num_conv = num_conv
        self.in_channels = in_channels
        self.act = act
        self.skip_connection = skip_connection
        self.stack = stack
        self.upsample = torch.nn.modules.Upsample(scale_factor=2)
        layers = []
        for i in range(num_conv-1):
            if i == 0:
                layers.append(nn.ConvTranspose2d(in_channels,out_channels=filters, kernel_size=3, stride=1,padding=1))
                layers.append(norm(filters))
                layers.append(act)
            else:
                layers.append(nn.ConvTranspose2d(filters,out_channels=filters, kernel_size=3, stride=1,padding=1))
                layers.append(norm(filters))
                layers.append(act)
        layers.append(nn.ConvTranspose2d(filters, out_channels=filters, kernel_size=3, stride=2,
                                         padding=1, output_padding=1))
        layers.append(act)
        self.seq = nn.Sequential(*layers)

    def forward(self,x):
        x0 = x
        x = self.seq(x)
        if self.skip_connection:
            x += self.upsample(x0)
        if self.stack:
            x = torch.cat([x, self.upsample(x0)], axis=1)
        #print(x.shape)
        return x

class Generator(nn.Module):
    def __init__(self,z, filters, output_shape,
                 num_conv=4, conv_k=3, last_k=3, repeat=0,
                 skip_connection=False, act=nn.LeakyReLU(),stack=False, norm=nn.BatchNorm2d):
        super(Generator,self).__init__()
        if repeat == 0:
            repeat_num = int(np.log2(torch.max(output_shape[1:]))) - 2
        else:
            repeat_num = repeat
        x0_shape = [filters] + [int(i//np.power(2, repeat_num-1)) for i in output_shape[1:]]
        print(x0_shape)
        self.x0_shape = x0_shape
        self.output_shape = output_shape
        self.filters = filters
        self.num_conv = num_conv
        num_output = int(np.prod(x0_shape))
        self.num_output = num_output
        self.linear = nn.Linear(z.shape[1], num_output)

        convTransBlockLayers = []
        ch = filters
        for i in range(repeat_num-1):
            #print(ch)
            convTransBlockLayers.append(convTransBlock(num_conv, ch, filters, act, skip_connection, stack,
                                                      norm=norm))
            if stack:
                ch += filters

        self.convTransBlockLayers = nn.Sequential(*convTransBlockLayers)
        if ch > filters:
            n = ch
        else:
            n = filters
        self.lastConv = nn.Conv2d(n,int(output_shape[0]),kernel_size=3, stride=1,padding=1)
        self.skip_connection = skip_connection
        self.stack = stack

    def forward(self, x):
        x = self.linear(x)
        x = x.view(-1,self.x0_shape[0],self.x0_shape[1],self.x0_shape[2])
        x = self.convTransBlockLayers(x)
        x = self.lastConv(x)
        return x

class Encoder(nn.Module):
    def __init__(self, x, filters, z_num, num_conv=4, conv_k=3, repeat=0, act=nn.LeakyReLU(), norm=nn.BatchNorm2d):
        super(Encoder,self).__init__()

        x_shape = x.shape[1:]
        if repeat == 0:
            repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2
        else:
            repeat_num = repeat

        self.filters = filters
        self.act = act
        self.conv1 = nn.Conv2d(x_shape[0], filters, kernel_size=conv_k, stride=1,padding=1)

        ch = filters
        convLayers = []
        for idx in range(0,repeat_num):
            convLayers.append(convBlock(num_conv, ch, filters, act=nn.LeakyReLU(), downSample=True, norm=norm))
            ch += filters

        self.convs = nn.Sequential(*convLayers)
        h = [i//2**repeat_num for i in x_shape[1:]]
        self.nLinearInput = (ch)*np.prod(h)
        self.linear = nn.Linear(self.nLinearInput,z_num)

    def forward(self,x):
        x = self.act(self.conv1(x))
        #print(x.shape)
        x = self.convs(x)
        #print(x.shape)
        x = x.view(x.size(0),-1)
        x = self.linear(x)
        return x

class Encoder_LK(nn.Module):
    def __init__(self, x, filters, z_num, repeat=0, act=nn.LeakyReLU()):
        super(Encoder_LK,self).__init__()

        x_shape = x.shape[1:]
        if repeat == 0:
            repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2
        else:
            repeat_num = repeat

        self.filters = filters
        self.act = act

        self.bn0 = nn.BatchNorm2d(x_shape[0])
        self.conv1 = nn.Conv2d(x_shape[0], filters, kernel_size=13,stride=4,padding=6)
        self.bn1 = nn.BatchNorm2d(filters)
        self.conv2 = nn.Conv2d(filters, 4*filters, kernel_size=13,stride=4,padding=6)
        self.bn2 = nn.BatchNorm2d(4*filters)
        self.z_num = z_num
        self.nLinearInput = 4*filters*8*6
        self.linear = nn.Linear(self.nLinearInput,z_num)

    def forward(self,x):
        x = self.bn0(x)
        x = self.act(self.conv1(x))
        #print(x.shape)
        x = self.bn1(x)
        x = self.act(self.conv2(x))
        x = self.bn2(x)
        #print(x.shape)
        x = x.view(x.size(0),-1)
        x = self.linear(x)
        return x

class Decoder_LK(nn.Module):
    def __init__(self, x, filters, z_num, repeat=0, act=nn.LeakyReLU()):
        super(Decoder_LK,self).__init__()

        x_shape = x.shape[1:]
        if repeat == 0:
            repeat_num = int(np.log2(np.max(x_shape[1:]))) - 2
        else:
            repeat_num = repeat

        self.filters = filters
        self.act = act
        self.bn0 = nn.BatchNorm2d(filters)
        self.conv1 = nn.ConvTranspose2d(filters,filters,kernel_size=25,stride=4,padding=11,output_padding=1)
        self.conv2 = nn.ConvTranspose2d(filters,4*filters,kernel_size=25,stride=4,padding=11,output_padding=1)
        self.bn1 = nn.BatchNorm2d(4*filters)
        self.conv3 = nn.Conv2d(4*filters,x_shape[0], kernel_size=3,stride=1,padding=1)
        self.z_num = z_num
        self.nLinearOutput = filters*8*6
        self.linear = nn.Linear(z_num,self.nLinearOutput)

    def forward(self,x):
        x = self.act(self.linear(x))
        x = x.view(x.size(0),self.filters,8,6)
        x = self.bn0(x)
        x = self.act(self.conv1(x))
        x = self.bn0(x)
        #print(x.shape)
        x = self.act(self.conv2(x))
        x = self.bn1(x)
        x = self.conv3(x)
        return x

class AE_LK(nn.Module):
    def __init__(self, encoder_LK, decoder_LK, return_z=True):
        super(AE_LK,self).__init__()
        self.encoder = encoder_LK
        self.generator = decoder_LK
        self.return_z = return_z

    def forward(self,x):
        z = self.encoder(x)
        x = self.generator(z)
        if self.return_z:
            return x, z
        else:
            return x

class AE_no_P(nn.Module):
    def __init__(self, encoder,generator):
        super(AE_no_P,self).__init__()
        self.encoder = encoder
        self.generator = generator

    def forward(self,x):
        x = self.encoder(x)
        #x = torch.cat([x,p],axis=1)
        x = self.generator(x)
        return x

class AE_xhat_z(nn.Module):
    def __init__(self, encoder,generator):
        super(AE_xhat_z,self).__init__()
        self.encoder = encoder
        self.generator = generator

    def forward(self,x):
        z = self.encoder(x)
        x = self.generator(z)
        return x, z

class AE_xhat_zV2(nn.Module):
    def __init__(self, X, filters=32, latentDim=16, num_conv=2, repeat=0,
                 skip_connection=False, stack=False, conv_k=3, last_k=3,
                 act=nn.LeakyReLU, return_z=True, stream=True, device='cpu', norm = nn.BatchNorm2d):
        super(AE_xhat_zV2,self).__init__()

        self.filters = filters
        self.latentDim = latentDim
        self.num_conv = num_conv
        self.repeat = repeat
        self.skip_connection = skip_connection
        self.stack = stack
        self.act = act
        self.norm = norm
        self.conv_k = 3
        self.last_k = 3
        self.device = device
        self.return_z = return_z
        self.stream = stream

        self.encoder = Encoder(X,filters,latentDim,num_conv=num_conv,norm=norm).to(device)

        z = self.encoder(X)

        if stream:
            self.output_shape = torch.tensor(X[0][1:].shape)
        else:
            self.output_shape = torch.tensor(X[0].shape)

        self.generator = Generator(z, filters, self.output_shape,
                                   num_conv, conv_k, last_k, repeat,
                                   skip_connection, act=act, stack=stack, norm=norm).to(device)

    def forward(self, x, p_x):
        z = self.encoder(x)
        z[:, -p_x.size(1):] = p_x
        x = self.generator(z)
        if self.return_z:
            return x, z
        else:
            return x

class ConvDeconv(nn.Module):
    def __init__(self, X, filters=32, latentDim=16, num_conv=2, repeat=1,
                 skip_connection=False, stack=False, conv_k=3, last_k=3,
                 act=nn.LeakyReLU(), return_z=True, stream=False, device='cpu'):
        super(ConvDeconv,self).__init__()

        self.filters = filters
        self.latentDim = latentDim
        self.num_conv = num_conv
        self.repeat = repeat
        self.skip_connection = skip_connection
        self.stack = stack
        self.act = act
        self.conv_k = 3
        self.last_k = 3
        self.device = device
        self.return_z = return_z
        self.stream = stream

        x_shape = X.shape
        convLayers = [nn.BatchNorm2d(x_shape[1]),nn.Conv2d(x_shape[1],filters, kernel_size=conv_k,stride=2,padding=1)]
        ch = filters
        for idx in range(0,repeat):
            convLayers.append(self.act)
            convLayers.append(nn.BatchNorm2d(ch))
            convLayers.append(nn.Conv2d(ch, ch+filters, kernel_size=conv_k,stride=2,padding=1))
            ch += filters


        self.encoder = nn.Sequential(*convLayers).to(device)

        z = self.encoder(X)

        if stream:
            self.output_shape = torch.tensor(X[0][1:].shape)
        else:
            self.output_shape = torch.tensor(X[0].shape)

        #print(self.output_shape)
        deconvLayers = []
        for idx in range(0,repeat):
            deconvLayers.append(nn.BatchNorm2d(ch))
            deconvLayers.append(nn.ConvTranspose2d(ch,ch-filters,
                                                   kernel_size=conv_k,stride=2,
                                                   padding=1, output_padding=1))
            deconvLayers.append(self.act)
            ch -= filters

        deconvLayers.append(nn.BatchNorm2d(ch))
        deconvLayers.append(nn.ConvTranspose2d(ch,int(self.output_shape[0]),
                                               kernel_size=conv_k,stride=2,
                                               padding=1, output_padding=1))

        self.generator = nn.Sequential(*deconvLayers).to(device)

    def forward(self,x):
        z = self.encoder(x)
        x = self.generator(z)
        if self.return_z:
            return x, z
        else:
            return x


class ConvDeconvFactor2(nn.Module):
    def __init__(self, X, filters=32, latentDim=16, num_conv=2, repeat=1,
                 skip_connection=False, stack=False, conv_k=3, last_k=3,
                 act=nn.LeakyReLU(), return_z=True, stream=False, device='cpu'):
        super(ConvDeconvFactor2,self).__init__()

        self.filters = filters
        self.latentDim = latentDim
        self.num_conv = num_conv
        self.repeat = repeat
        self.skip_connection = skip_connection
        self.stack = stack
        self.act = act
        self.conv_k = 3
        self.last_k = 3
        self.device = device
        self.return_z = return_z
        self.stream = stream

        x_shape = X.shape
        convLayers = [nn.BatchNorm2d(x_shape[1]),nn.Conv2d(x_shape[1],filters, kernel_size=conv_k,stride=2,padding=1)]
        ch = filters
        for idx in range(0,repeat):
            convLayers.append(self.act)
            convLayers.append(nn.BatchNorm2d(ch))
            convLayers.append(nn.Conv2d(ch, 2*ch, kernel_size=conv_k,stride=2,padding=1))
            ch = 2*ch


        self.encoder = nn.Sequential(*convLayers).to(device)

        z = self.encoder(X)

        if stream:
            self.output_shape = torch.tensor(X[0][1:].shape)
        else:
            self.output_shape = torch.tensor(X[0].shape)

        #print(self.output_shape)
        deconvLayers = []
        for idx in range(0,repeat):
            deconvLayers.append(nn.BatchNorm2d(ch))
            deconvLayers.append(nn.ConvTranspose2d(ch,ch//2,
                                                   kernel_size=conv_k,stride=2,
                                                   padding=1, output_padding=1))
            deconvLayers.append(self.act)
            ch = ch//2

        deconvLayers.append(nn.BatchNorm2d(ch))
        deconvLayers.append(nn.ConvTranspose2d(ch,int(self.output_shape[0]),
                                               kernel_size=conv_k,stride=2,
                                               padding=1, output_padding=1))

        self.generator = nn.Sequential(*deconvLayers).to(device)

    def forward(self,x):
        z = self.encoder(x)
        x = self.generator(z)
        if self.return_z:
            return x, z
        else:
            return x

class MLP(nn.Module):
    def __init__(self, X, hiddenLayerSizes = [1024], activation=nn.ELU()):
        super(MLP,self).__init__()

        self.activation = activation
        self.inputSize = X.shape[1:]
        self.modules = []
        self.modules.append(nn.Linear(np.prod(self.inputSize),hiddenLayerSizes[0]))
        self.modules.append(self.activation)
        for idx,sz in enumerate(hiddenLayerSizes[:-1]):
            self.modules.append(nn.Linear(hiddenLayerSizes[idx],hiddenLayerSizes[idx+1]))
            self.modules.append(self.activation)

        self.modules.append(nn.Linear(hiddenLayerSizes[-1],np.prod(self.inputSize)))
        self.layers = nn.Sequential(*self.modules)


    def forward(self,x):
        x = x.view(x.shape[0],-1)
        x = self.layers(x)
        x = x.view(x.shape[0],self.inputSize[0],self.inputSize[1],self.inputSize[2])
        return x

# Cell
class GeneratorOld(nn.Module):
    def __init__(self,z, filters, output_shape,
                 num_conv=4, conv_k=3, last_k=3, repeat=0,
                 skip_connection=False, act=nn.LeakyReLU()):
        super(GeneratorOld,self).__init__()
        if repeat == 0:
            repeat_num = int(np.log2(torch.max(output_shape[1:]))) - 2
        else:
            repeat_num = repeat
        x0_shape = [filters] + [i//np.power(2, repeat_num-1) for i in output_shape[1:]]
        self.x0_shape = x0_shape
        num_output = int(np.prod(x0_shape))
        self.linear = nn.Linear(z.shape[1], num_output)
        convTransBlockLayers = []
        for i in range(repeat_num-1):
            convTransBlockLayers.append(convTransBlock(num_conv,filters,act,skip_connection))
        self.convTransBlockLayers = nn.Sequential(*convTransBlockLayers)
        self.lastConv = nn.Conv2d(filters,int(output_shape[0]),kernel_size=3, stride=1,padding=1)

    def forward(self, x):
        x = self.linear(x)
        x = x.view(-1,self.x0_shape[0],self.x0_shape[1],self.x0_shape[2])
        x = self.convTransBlockLayers(x)
        #x = self.lastConv(x)
        return x