{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:25.847988Z",
     "start_time": "2020-08-21T22:43:24.382896Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "# --- Must haves ---\n",
    "import os, sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from surrogates4sims.datasets import MantaFlowDataset, getSingleSim, createMantaFlowTrainTest\n",
    "\n",
    "from surrogates4sims.utils import create_opt, create_one_cycle, find_lr, printNumModelParams, \\\n",
    "                                    rmse, writeMessage, plotSampleWprediction, plotSampleWpredictionByChannel, \\\n",
    "                                    plotSample, curl, jacobian, stream2uv, create_movie, convertSimToImage\n",
    "\n",
    "#from surrogates4sims.models import Generator, Encoder, AE_no_P, AE_xhat_z, AE_xhat_zV2\n",
    "\n",
    "from surrogates4sims.train import trainEpoch, validEpoch\n",
    "\n",
    "from surrogates4sims.svd import MantaFlowSVDDataset\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:25.860055Z",
     "start_time": "2020-08-21T22:43:25.850310Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "# model name, for tensorboard recording and checkpointing purposes.\n",
    "versionName = \"LIN_SVD_only_z_and_p_LSTM_bidirec\"\n",
    "\n",
    "# GPU Numbers to use. Comma seprate them for multi-GPUs.\n",
    "gpu_ids = \"2\"#,1,2,3\"\n",
    "versionName = versionName + '_GPUs{}'.format(gpu_ids.replace(',',''))\n",
    "# path to load model weights.\n",
    "pretrained_path = None\n",
    "\n",
    "# rate at which to record metrics. (number of batches to average over when recording metrics, e.g. \"every 5 batches\")\n",
    "tensorboard_rate = 5\n",
    "\n",
    "# number of epochs to train. This is defined here so we can use the OneCycle LR Scheduler.\n",
    "epochs = 1000\n",
    "\n",
    "# Data Directory\n",
    "dataDirec = '/data/mantaFlowSim/data/smoke_pos21_size5_f200/v'\n",
    "reverseXY = False \n",
    "\n",
    "# checkpoint directory\n",
    "cps = 'cps'\n",
    "tensorboard_direc = \"tb\"\n",
    "\n",
    "findLRs = True  \n",
    "patience = 10\n",
    "\n",
    "# hyper-params\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "testSplit = .1\n",
    "bz = 4\n",
    "numSamplesToKeep = np.infty #if not debugging\n",
    "latentDim = 512\n",
    "simLen = 200\n",
    "simVizIndex = 0 # sim in the test set to visualize\n",
    "numComponents = latentDim\n",
    "transform = True\n",
    "if DEBUG:\n",
    "    epochs = 5000\n",
    "    numSamplesToKeep = 1000\n",
    "    \n",
    "versionName = versionName + '_latentDim{}_bz{}_transform{}_epochs{}'.format(latentDim,bz,transform,epochs)\n",
    "versionName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Select Personal GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:26.045437Z",
     "start_time": "2020-08-21T22:43:25.861953Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:26.054870Z",
     "start_time": "2020-08-21T22:43:26.050084Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=gpu_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:26.095280Z",
     "start_time": "2020-08-21T22:43:26.056662Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:26.105806Z",
     "start_time": "2020-08-21T22:43:26.097181Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if device.type == 'cuda':\n",
    "    print(cuda.is_available())\n",
    "    print(cuda.device_count())\n",
    "    print(cuda.current_device())\n",
    "    print(cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:28.582646Z",
     "start_time": "2020-08-21T22:43:26.107649Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.zeros(5, device=device.type)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Datasets & Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:28.592791Z",
     "start_time": "2020-08-21T22:43:28.586595Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# A = np.load('svd_out.npz')\n",
    "# vh = A['arr_2']\n",
    "# vh.shape\n",
    "# data = []\n",
    "# numComp = 512\n",
    "# for idx in range(105):\n",
    "#     D = getSingleSim(idx)\n",
    "#     simDataset = MantaFlowDataset(D)\n",
    "#     Z = []\n",
    "#     P = []\n",
    "#     X0, p = simDataset[0]\n",
    "#     for sample in simDataset:\n",
    "#         f, p = sample\n",
    "#         coeffs = vh[:numComp]@f.flatten()\n",
    "#         Z.append(coeffs)\n",
    "#         P.append(p)\n",
    "#     y = np.array(Z)\n",
    "#     P = np.array(P)\n",
    "#     X = (X0,P,y[0])\n",
    "#     data.append((X,y[1:]))\n",
    "# with open('simLatentVectors.pkl','wb') as fid:\n",
    "#     pickle.dump(data,fid)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:28.641272Z",
     "start_time": "2020-08-21T22:43:28.597277Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('simLatentVectors.pkl','rb') as fid:\n",
    "    data = pickle.load(fid)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:28.647867Z",
     "start_time": "2020-08-21T22:43:28.643231Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data[0][0][0].shape, data[0][0][1].shape, data[0][0][2].shape, data[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:28.741732Z",
     "start_time": "2020-08-21T22:43:28.649276Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainData, testData = createMantaFlowTrainTest(dataDirec,simLen,testSplit,seed)\n",
    "print((len(trainData),len(testData)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:28.802099Z",
     "start_time": "2020-08-21T22:43:28.743129Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "d = glob(os.path.join(dataDirec,'*.npz'))\n",
    "d = sorted(d)\n",
    "simLen = 200\n",
    "numSims = len(d)//simLen\n",
    "numTestSamples = int(np.round(testSplit*numSims))\n",
    "np.random.seed(seed)\n",
    "perm = np.random.permutation(numSims)\n",
    "testSims = perm[:numTestSamples]\n",
    "trainSims = perm[numTestSamples:]\n",
    "testSims, trainSims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:28.808320Z",
     "start_time": "2020-08-21T22:43:28.803526Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "testDataset = [data[i] for i in testSims]\n",
    "trainDataset = [data[i] for i in trainSims]\n",
    "len(testDataset), len(trainDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:28.868833Z",
     "start_time": "2020-08-21T22:43:28.810109Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = []\n",
    "E = []\n",
    "for X,y in trainDataset:\n",
    "    z = X[2]\n",
    "    z = z.reshape(1,len(z))\n",
    "    y = np.concatenate([z,y])\n",
    "    D.append(y.max(axis=0))\n",
    "    E.append(y.min(axis=0))\n",
    "D = np.array(D)\n",
    "E = np.array(E)\n",
    "ymx = D.max(axis=0)\n",
    "ymn = E.min(axis=0)\n",
    "ymx, ymn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:28.873354Z",
     "start_time": "2020-08-21T22:43:28.870400Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def transformY(y,ymx,ymn):\n",
    "    return (ymx - y)/(ymx - ymn)\n",
    "\n",
    "def inverseTransformY(y,ymx,ymn):\n",
    "    x = ymx - y*(ymx - ymn)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:28.881720Z",
     "start_time": "2020-08-21T22:43:28.874716Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = transformY(y,ymx,ymn)\n",
    "a.max(), a.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:28.888177Z",
     "start_time": "2020-08-21T22:43:28.883333Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "b = inverseTransformY(a,ymx,ymn)\n",
    "np.abs(y - b).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:28.895666Z",
     "start_time": "2020-08-21T22:43:28.889563Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LatentSVD(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "                 \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx, return_norm_stats=False):\n",
    "        X, y  = self.data[idx]\n",
    "        p = X[1]\n",
    "        z = X[2]\n",
    "\n",
    "        if self.transform:\n",
    "            y = torch.tensor(y)\n",
    "            norm_stats = y.norm(dim=1).unsqueeze(1)\n",
    "            y = y / norm_stats\n",
    "            \n",
    "        D = []\n",
    "        for pp in p[1:]:\n",
    "            zz = np.concatenate([pp,z])#.reshape(1,515)\n",
    "            D.append(zz)\n",
    "        X = np.array(D)\n",
    "        \n",
    "        if return_norm_stats:\n",
    "            return X, y, norm_stats\n",
    "        else:\n",
    "            return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:28.900961Z",
     "start_time": "2020-08-21T22:43:28.897195Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if transform:\n",
    "    testDataset = LatentSVD(testDataset, transform=transformY)\n",
    "    trainDataset = LatentSVD(trainDataset, transform=transformY)\n",
    "else:\n",
    "    testDataset = LatentSVD(testDataset)\n",
    "    trainDataset = LatentSVD(trainDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Making sure the transform works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:28.908569Z",
     "start_time": "2020-08-21T22:43:28.903517Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(dataset=trainDataset, batch_size=1, shuffle=True, drop_last=True)\n",
    "testDataLoader = DataLoader(dataset=testDataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:28.936891Z",
     "start_time": "2020-08-21T22:43:28.911306Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X,y = next(iter(trainDataLoader))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:29.553873Z",
     "start_time": "2020-08-21T22:43:28.939217Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y[0].squeeze().T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:29.565426Z",
     "start_time": "2020-08-21T22:43:29.556873Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y.max(), y.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:29.569086Z",
     "start_time": "2020-08-21T22:43:29.566527Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:29.574044Z",
     "start_time": "2020-08-21T22:43:29.570221Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X[:,:,3:].max(),X[:,:,3:].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:29.577779Z",
     "start_time": "2020-08-21T22:43:29.575145Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(dataset=trainDataset, batch_size=bz, shuffle=True, drop_last=True)\n",
    "testDataLoader = DataLoader(dataset=testDataset, batch_size=bz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:29.582870Z",
     "start_time": "2020-08-21T22:43:29.580324Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inputSize = X.shape[2]\n",
    "hiddenSize = y.shape[2]\n",
    "numLayers = 1\n",
    "bidirectional = False\n",
    "batch_first = True\n",
    "#model = nn.LSTM(inputSize, hiddenSize, numLayers, batch_first=True, bidirectional=bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:29.605485Z",
     "start_time": "2020-08-21T22:43:29.585795Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,inputSize=3, hiddenSize=512, numLayers=1, batch_first=True, bidirectional=False):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(3, hiddenSize, numLayers, batch_first=batch_first, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self,x):\n",
    "        h_0 = x[:, 0, 3:].unsqueeze(0).clone()\n",
    "        c_0 = torch.zeros_like(h_0)\n",
    "        x,_ = self.lstm(x[:, :, :3], (h_0, c_0))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:29.829349Z",
     "start_time": "2020-08-21T22:43:29.607220Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = LSTM(inputSize, hiddenSize, numLayers, batch_first, bidirectional).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:29.833216Z",
     "start_time": "2020-08-21T22:43:29.830628Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "printNumModelParams(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:29.841483Z",
     "start_time": "2020-08-21T22:43:29.834452Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output = model(X.to(device))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:29.844962Z",
     "start_time": "2020-08-21T22:43:29.842667Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if len(gpu_ids.split(',')) > 1:\n",
    "    model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Orig Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:29.851413Z",
     "start_time": "2020-08-21T22:43:29.846227Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:29.855007Z",
     "start_time": "2020-08-21T22:43:29.852683Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "L = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:29.863239Z",
     "start_time": "2020-08-21T22:43:29.856427Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    loss = L(output, y.to(device))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Trimmed Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:29.872270Z",
     "start_time": "2020-08-21T22:43:29.866118Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mse_loss_func = nn.MSELoss()\n",
    "trim_idx = 2\n",
    "def trimmed_MSE(output, y):\n",
    "    return mse_loss_func(output[:, :trim_idx], y[:, :trim_idx])\n",
    "\n",
    "L = trimmed_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:29.879456Z",
     "start_time": "2020-08-21T22:43:29.874520Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    loss = L(output,y.to(device))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Set LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:29.884738Z",
     "start_time": "2020-08-21T22:43:29.881943Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# if findLRs and (len(gpu_ids.split(','))==1): # doesn't work for multigpu???\n",
    "#     opt = create_opt(1e-7,model)\n",
    "#     find_lr(model,opt,L,device,trainDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:29.891231Z",
     "start_time": "2020-08-21T22:43:29.886529Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_lr = .001\n",
    "versionName = versionName + '_lr{}'.format(str(max_lr))\n",
    "\n",
    "versionName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:31.907403Z",
     "start_time": "2020-08-21T22:43:31.892528Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def trainEpoch(myDataLoader, tensorboard_writer, model, opt, loss,\n",
    "               metric, lr_scheduler, tensorboard_rate, device,\n",
    "               tensorboard_recorder_step, total_steps):\n",
    "    running_loss = 0.0\n",
    "    running_rmse = 0.0\n",
    "    total_loss = 0.0\n",
    "    for i, sampleBatch in enumerate(myDataLoader, start=1):\n",
    "\n",
    "        # --- Main Training ---\n",
    "        \n",
    "        # gpu\n",
    "        X,y = sampleBatch[0],sampleBatch[1]\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        y_hat = model(X)\n",
    "        combined_loss = loss(y_hat,y)\n",
    "        combined_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # loss\n",
    "        batch_loss = combined_loss.item()\n",
    "        running_loss += batch_loss\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        # --- Metrics Recording ---\n",
    "\n",
    "        # metrics\n",
    "        r = metric(y_hat, y)\n",
    "        running_rmse += r\n",
    "\n",
    "        # record lr change\n",
    "        total_steps += 1\n",
    "        tensorboard_writer.add_scalar(tag=\"LR\", scalar_value=opt.param_groups[0]['lr'], global_step=total_steps)\n",
    "\n",
    "        # tensorboard writes\n",
    "        if (i % tensorboard_rate == 0):\n",
    "            tensorboard_recorder_step += 1\n",
    "            avg_running_loss = running_loss/tensorboard_rate\n",
    "            avg_running_rmse = running_rmse/tensorboard_rate\n",
    "            tensorboard_writer.add_scalar(tag=\"Loss\", scalar_value=avg_running_loss, global_step=tensorboard_recorder_step)\n",
    "            tensorboard_writer.add_scalar(tag=metric.__name__, scalar_value=avg_running_rmse, global_step=tensorboard_recorder_step)\n",
    "            # reset running_loss for the next set of batches. (tensorboard_rate number of batches)\n",
    "            running_loss = 0.0\n",
    "            running_rmse = 0.0\n",
    "\n",
    "    return total_loss/len(myDataLoader), tensorboard_recorder_step, total_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:31.943646Z",
     "start_time": "2020-08-21T22:43:31.935246Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def validEpoch(myDataLoader, tensorboard_writer, model, loss, metric,\n",
    "               device, tensorboard_recorder_step):\n",
    "    running_loss = 0.0\n",
    "    running_rmse = 0.0\n",
    "    for i, sampleBatch in enumerate(myDataLoader, start=1):\n",
    "\n",
    "        # --- Metrics Recording ---\n",
    "\n",
    "        # gpu\n",
    "        X,y = sampleBatch[0],sampleBatch[1]\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        #perc = len(X)/len(myDataLoader.dataset)\n",
    "        perc = 1./len(myDataLoader.dataset)\n",
    "        # forward, no gradient calculations\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(X)\n",
    "\n",
    "        # loss\n",
    "        combined_loss = loss(y_hat,y)\n",
    "        \n",
    "        running_loss += perc*(combined_loss.item())\n",
    "\n",
    "        # metrics\n",
    "        r = metric(y_hat, y)\n",
    "        running_rmse += perc*r\n",
    "\n",
    "    avg_running_loss = running_loss\n",
    "    avg_running_rmse = running_rmse\n",
    "    tensorboard_writer.add_scalar(tag=\"Loss\", scalar_value=avg_running_loss, global_step=tensorboard_recorder_step)\n",
    "    tensorboard_writer.add_scalar(tag=metric.__name__, scalar_value=avg_running_rmse, global_step=tensorboard_recorder_step)\n",
    "\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:31.950564Z",
     "start_time": "2020-08-21T22:43:31.946085Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(cps)\n",
    "except:\n",
    "    print(\"checkpoints directory already exists :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:31.964206Z",
     "start_time": "2020-08-21T22:43:31.953919Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create a summary writer.\n",
    "train_writer = SummaryWriter(os.path.join(tensorboard_direc, versionName,'train'))\n",
    "test_writer = SummaryWriter(os.path.join(tensorboard_direc, versionName,'valid'))\n",
    "tensorboard_recorder_step = 0\n",
    "total_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:31.978673Z",
     "start_time": "2020-08-21T22:43:31.966204Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fit_up_to_trim_idx():\n",
    "    \n",
    "    global tensorboard_recorder_step\n",
    "    global total_steps\n",
    "    \n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,patience=patience)\n",
    "\n",
    "    writeMessage('---------- Started Training ----------', versionName)\n",
    "    bestLoss = np.infty\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs+1)):  # loop over the dataset multiple times\n",
    "\n",
    "        writeMessage(\"--- Epoch {0}/{1} ---\".format(epoch, epochs), versionName)\n",
    "\n",
    "        model.train()\n",
    "        trainLoss, tensorboard_recorder_step, total_steps = trainEpoch(trainDataLoader, \n",
    "                                                                       train_writer, model, opt, L,\n",
    "                                                                       rmse, lr_scheduler, \n",
    "                                                                       tensorboard_rate, device,\n",
    "                                                                       tensorboard_recorder_step, total_steps)\n",
    "\n",
    "        writeMessage(\"trainLoss: {:.4e}\".format(trainLoss),versionName)\n",
    "        writeMessage(\"LR: {:.4e}\".format(opt.param_groups[0]['lr']),versionName)\n",
    "    #     if trainLoss < bestLoss:\n",
    "    #         bestLoss = trainLoss\n",
    "    #         writeMessage(\"Better trainLoss: {:.4e}, Saving models...\".format(bestLoss),versionName)\n",
    "    #         torch.save(model.state_dict(), os.path.join(cps,versionName))\n",
    "\n",
    "        model.eval()\n",
    "        valLoss = validEpoch(testDataLoader, test_writer, model, L, rmse, device, tensorboard_recorder_step)\n",
    "        writeMessage(\"valLoss: {:.4e}\".format(valLoss),versionName)\n",
    "\n",
    "        #checkpoint progress\n",
    "        if valLoss < bestLoss:\n",
    "            bestLoss = valLoss\n",
    "            writeMessage(\"Better valLoss: {:.4e}, Saving models...\".format(bestLoss),versionName)\n",
    "            torch.save(model.state_dict(), os.path.join(cps,versionName))\n",
    "\n",
    "        lr_scheduler.step(trainLoss)\n",
    "\n",
    "        if opt.param_groups[0]['lr'] < 5e-8:\n",
    "            break\n",
    "    writeMessage('---------- Finished Training ----------', versionName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:31.985913Z",
     "start_time": "2020-08-21T22:43:31.980288Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# {trim_idx : epochs_to_train_for}\n",
    "\n",
    "trim_epoch_dict = {}\n",
    "for i in range(2, 201, 5):\n",
    "    trim_epoch_dict[i] = 10\n",
    "#     if i in range(0, 50):\n",
    "#         trim_epoch_dict[i] = 100\n",
    "#     elif i in range(50, 100):\n",
    "#         trim_epoch_dict[i] = 150\n",
    "#     elif i in range(100, 150):\n",
    "#         trim_epoch_dict[i] = 200\n",
    "#     else:\n",
    "#         trim_epoch_dict[i] = 250\n",
    "trim_epoch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:43:31.999749Z",
     "start_time": "2020-08-21T22:43:31.987426Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "train_progress_widget = ipywidgets.Text('')\n",
    "train_progress_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:44:45.256969Z",
     "start_time": "2020-08-21T22:43:32.001307Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, value in trim_epoch_dict.items():\n",
    "    trim_idx = key\n",
    "    epochs = value\n",
    "    \n",
    "    train_progress_widget.value = 'Currently training trim_idx: {0}'.format(trim_idx)\n",
    "    fit_up_to_trim_idx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Compare: Generated vs. Simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:44:47.229683Z",
     "start_time": "2020-08-21T22:44:47.225837Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(os.path.join(cps,versionName)))\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:44:47.409126Z",
     "start_time": "2020-08-21T22:44:47.402824Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:44:47.764091Z",
     "start_time": "2020-08-21T22:44:47.749906Z"
    },
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "X, y, norm_stats = testDataset.__getitem__(0, True)\n",
    "X = torch.tensor(X).unsqueeze(0)\n",
    "y = y.to(device)\n",
    "norm_stats = norm_stats.to(device)\n",
    "X.shape, y.shape, norm_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:44:48.065705Z",
     "start_time": "2020-08-21T22:44:48.054058Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_out = model(X.to(device))\n",
    "batch_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:44:48.604173Z",
     "start_time": "2020-08-21T22:44:48.418954Z"
    },
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "batch_out = batch_out.squeeze()\n",
    "y = y.squeeze()\n",
    "err = []\n",
    "for i in range(simLen-1):\n",
    "    err.append(torch.norm(y[i] - batch_out[i]))\n",
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:44:49.095008Z",
     "start_time": "2020-08-21T22:44:48.914107Z"
    },
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "batch_out = batch_out.squeeze()*norm_stats\n",
    "y = y.squeeze()*norm_stats\n",
    "err = []\n",
    "for i in range(simLen-1):\n",
    "    err.append(torch.norm(y[i] - batch_out[i]))\n",
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:44:49.571109Z",
     "start_time": "2020-08-21T22:44:49.561347Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d = batch_out - y\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:44:49.897927Z",
     "start_time": "2020-08-21T22:44:49.738710Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T22:45:13.646382Z",
     "start_time": "2020-08-21T22:44:51.535091Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = np.arange(512)\n",
    "for dd,yy in zip(batch_out,y):\n",
    "    plt.plot(t, dd.detach().cpu().numpy(),t, yy.detach().cpu().numpy(),'--')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T19:35:34.512969Z",
     "start_time": "2020-08-21T19:35:30.859Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = y.detach().cpu().numpy()\n",
    "tmp.max(), tmp.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.888Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bb = batch_out.detach().cpu().numpy()\n",
    "bb.max(), bb.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.889Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(tmp.max(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.890Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(d[-2].detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.892Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X = y.detach().cpu().numpy()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.893Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2,verbose=3).fit_transform(X)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.894Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_embedded[:,0],X_embedded[:,1],s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.895Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = batch_out.detach().cpu().numpy()\n",
    "X_embedded = TSNE(n_components=2,verbose=3).fit_transform(X)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T16:35:21.896Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_embedded[:,0],X_embedded[:,1],s=2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
